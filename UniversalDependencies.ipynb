{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "569de287-e7cd-4c06-9c75-859442428920",
   "metadata": {},
   "source": [
    "# Working with Universal Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8bcf2f5-0b05-4a70-9a96-06346284ce58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: conllu in c:\\users\\samanvay\\appdata\\roaming\\python\\python313\\site-packages (6.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# if you don't have conllu yet, uncomment the following\n",
    "!python -m pip install conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ae05adf-0fe1-4d76-ba45-100c4ae55bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu # reading Universal Dependency files in the CONLLu format\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e103158-0fd5-4666-9713-ade344abd881",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ko_kaist-ud-train.conllu\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005796a9",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bb54b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb instances (VERB tokens): 55805\n",
      "Total obj instances: 13912\n",
      "Objects immediately before verb: 10066 (72.35%)\n",
      "Objects within 3 before verb: 12937 (92.99%)\n",
      "Objects after verb: 0 (0.00%)\n",
      "Objects before verb (any distance): 13912 (100.00%)\n",
      "Saved distance distribution to data/project_outputs\\object_verb_distance_distribution.csv\n"
     ]
    }
   ],
   "source": [
    "OUT_DIR = 'data/project_outputs'\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def parse_conllu(path):\n",
    "    \"\"\"\n",
    "    Return list of sentences; each sentence is a list of token dicts:\n",
    "    {id:int, form:str, lemma:str, upos:str, head:int or None, deprel:str}\n",
    "    Ignores multiword lines (1-2) and empty-node decimal ids (3.1).\n",
    "    \"\"\"\n",
    "    sents = []\n",
    "    tokens = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    sents.append(tokens)\n",
    "                    tokens = []\n",
    "                continue\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            cols = line.split(\"\\t\")\n",
    "            if len(cols) != 10:\n",
    "                continue\n",
    "            id_field = cols[0]\n",
    "            # skip multiword / empty nodes\n",
    "            if \"-\" in id_field or \".\" in id_field:\n",
    "                continue\n",
    "            try:\n",
    "                tid = int(id_field)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            form = cols[1]\n",
    "            lemma = cols[2]\n",
    "            upos = cols[3]\n",
    "            head = cols[6]\n",
    "            deprel = cols[7]\n",
    "            try:\n",
    "                head_int = int(head) if head != \"_\" else None\n",
    "            except ValueError:\n",
    "                head_int = None\n",
    "            tok = {\"id\": tid, \"form\": form, \"lemma\": lemma, \"upos\": upos, \"head\": head_int, \"deprel\": deprel}\n",
    "            tokens.append(tok)\n",
    "    # final\n",
    "    if tokens:\n",
    "        sents.append(tokens)\n",
    "    return sents\n",
    "\n",
    "sents = parse_conllu(\"ko_kaist-ud-train.conllu\")\n",
    "\n",
    "verb_instances = 0\n",
    "obj_instances = 0\n",
    "obj_immediate = 0\n",
    "obj_within3 = 0\n",
    "obj_to_right = 0\n",
    "obj_any_before = 0\n",
    "dist_counter = Counter()\n",
    "\n",
    "examples = defaultdict(list) \n",
    "\n",
    "for sent_idx, tokens in enumerate(sents):\n",
    "    id_to_tok = {t['id']: t for t in tokens}\n",
    "    dependents = defaultdict(list)\n",
    "    for t in tokens:\n",
    "        if t['head'] is not None and t['head'] in id_to_tok:\n",
    "            dependents[t['head']].append(t)\n",
    "    id_to_index = {t['id']: i for i, t in enumerate(tokens)}\n",
    "\n",
    "    for i, tok in enumerate(tokens):\n",
    "        if tok['upos'] == 'VERB':\n",
    "            verb_instances += 1\n",
    "            verb_id = tok['id']\n",
    "            deps = dependents.get(verb_id, [])\n",
    "            for dep in deps:\n",
    "                if dep['deprel'] == 'obj':\n",
    "                    obj_instances += 1\n",
    "                    obj_idx = id_to_index.get(dep['id'])\n",
    "                    verb_idx = i\n",
    "                    if obj_idx is None:\n",
    "                        continue\n",
    "                    diff = verb_idx - obj_idx  # positive if object is to verb's left\n",
    "                    dist_counter[diff] += 1\n",
    "                    if diff == 1:\n",
    "                        obj_immediate += 1\n",
    "                    if 1 <= diff <= 3:\n",
    "                        obj_within3 += 1\n",
    "                    if diff < 0:\n",
    "                        obj_to_right += 1\n",
    "                    if diff > 0:\n",
    "                        obj_any_before += 1\n",
    "\n",
    "                    \n",
    "                    lemma = tok['lemma'] if tok['lemma'] != '_' else tok['form']\n",
    "                    if len(examples[lemma]) < 5:\n",
    "                        sent_form = ' '.join([t['form'] for t in tokens])\n",
    "                        examples[lemma].append((dep['form'], tok['form'], sent_form))\n",
    "\n",
    "if obj_instances > 0:\n",
    "    pct_immediate = obj_immediate / obj_instances * 100\n",
    "    pct_within3 = obj_within3 / obj_instances * 100\n",
    "    pct_after = obj_to_right / obj_instances * 100\n",
    "else:\n",
    "    pct_immediate = pct_within3 = 0.0\n",
    "\n",
    "print('Verb instances (VERB tokens):', verb_instances)\n",
    "print('Total obj instances:', obj_instances)\n",
    "print(f'Objects immediately before verb: {obj_immediate} ({pct_immediate:.2f}%)')\n",
    "print(f'Objects within 3 before verb: {obj_within3} ({pct_within3:.2f}%)')\n",
    "print(f'Objects after verb: {obj_to_right} ({pct_after:.2f}%)')\n",
    "print(f'Objects before verb (any distance): {obj_any_before} ({(obj_any_before / obj_instances * 100) if obj_instances > 0 else 0.0:.2f}%)')\n",
    "\n",
    "dist_items = sorted(dist_counter.items())\n",
    "df_dist = pd.DataFrame(dist_items, columns=['verb_minus_obj_index', 'count'])\n",
    "df_dist.to_csv(os.path.join(OUT_DIR, 'object_verb_distance_distribution.csv'), index=False)\n",
    "print('Saved distance distribution to', os.path.join(OUT_DIR, 'object_verb_distance_distribution.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c453e1",
   "metadata": {},
   "source": [
    "### Task 1.1\n",
    "The generalization we can see from the above results is that an object _never_ occurs after a verb, it _must_ occur before the verb.\n",
    "\n",
    "### Task 1.2\n",
    "Our generalization generally holds true. While in our corupus, an object _never_ follows a verb, it does not always have to come immediately before a verb (it does at a 72.35% rate). We have found that the object comes within 3 tokens before the verb, though this is also not always true (it occurs 92.99% of the time). The exceptions to this rule occur when objects are farther than 3 tokens from the verb, but still are before the verb, since we could not find any case where it follows a verb (at any length)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df06b09c",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94afcdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 23010 sentences from ko_kaist-ud-train.conllu\n",
      "Top verbs (sample): ['하', '있', '되', '않', '보']\n",
      "Mid verbs (sample): ['민감', '내려오', '제창', '갈', '문지르']\n",
      "Saved negation-aware summary to data/project_outputs_task2\\verb_negation_summary.json\n",
      "Attempting to load vector model...\n",
      "Inspecting text vector file ko.tsv ...\n",
      "Constructed KeyedVectors from text file with 30185 words and dim 200.\n",
      "Computing centroids and nearest neighbors...\n",
      "Done. Outputs saved in: data/project_outputs_task2\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import KeyedVectors  # used by the loader (optional)\n",
    "\n",
    "CONLLU_PATH = \"ko_kaist-ud-train.conllu\"   #Just a reminder, all files are directly just in the same folder as this code.\n",
    "OUT_DIR = \"data/project_outputs_task2\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "VERB_UPOS = (\"VERB\", \"AUX\")   # token UPOS considered verbs\n",
    "COPULA_CANONICAL = \"이\"       # canonical token for copula (이)\n",
    "EXCLUDE_VERBS = {COPULA_CANONICAL}  # verbs to exclude from negation analyses\n",
    "\n",
    "def normalize_korean_verb(lemma_or_form):\n",
    "    \"\"\"\n",
    "        Normalize a Korean verb token (lemma or form) to a canonical representation.\n",
    "        Rules:\n",
    "        - If segmented with '+', take first morpheme unless copula '이' is present, in which case return '이'\n",
    "        - If unsegmented and ends with '다', strip terminal '다'\n",
    "        - Otherwise return as is\n",
    "    \"\"\"\n",
    "    if not lemma_or_form or lemma_or_form == \"_\":\n",
    "        return None\n",
    "\n",
    "    s = lemma_or_form.replace(\"＋\", \"+\").replace(\"‧\", \"+\").strip()\n",
    "\n",
    "    #if segmented with '+', split into morphemes\n",
    "    if \"+\" in s:\n",
    "        parts = [p for p in s.split(\"+\") if p]  # drop empty parts\n",
    "        #if any part equals the copula morpheme '이', treat as copula\n",
    "        if any(p == \"이\" for p in parts):\n",
    "            return COPULA_CANONICAL\n",
    "        return parts[0]\n",
    "\n",
    "    #if unsegmented but ends with '다', strip terminal '다' \n",
    "    if len(s) > 1 and s.endswith(\"다\"):\n",
    "        return s[:-1]\n",
    "\n",
    "    return s\n",
    "\n",
    "def aggregate_normalized_counts(counter, locations):\n",
    "    \"\"\"\n",
    "        Ensures counts/locations are collapsed to normalized keys.\n",
    "        Returns new Counter and locations dict.\n",
    "    \"\"\"\n",
    "    norm_counter = Counter()\n",
    "    norm_locations = defaultdict(list)\n",
    "    if locations:\n",
    "        for raw_key, locs in locations.items():\n",
    "            norm = normalize_korean_verb(raw_key)\n",
    "            norm_counter[norm] += len(locs)\n",
    "            norm_locations[norm].extend(locs)\n",
    "    else:\n",
    "        for raw_key, cnt in counter.items():\n",
    "            norm = normalize_korean_verb(raw_key)\n",
    "            norm_counter[norm] += cnt\n",
    "    return norm_counter, norm_locations\n",
    "\n",
    "def verb_frequencies(sents, verb_upos=VERB_UPOS):\n",
    "    \"\"\"\n",
    "        Count frequencies and record locations of verbs in sentences.\n",
    "        Returns a Counter of normalized verbs and a dict of locations.\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    locations = defaultdict(list)  # canonical_verb -> list of (sent_idx, token_idx, original_token)\n",
    "    # iterate over sentences and tokens\n",
    "    for si, tokens in enumerate(sents):\n",
    "        for i, tok in enumerate(tokens):\n",
    "            if tok[\"upos\"] in verb_upos:\n",
    "                lemma = tok[\"lemma\"] if tok[\"lemma\"] != \"_\" else tok[\"form\"]\n",
    "                norm = normalize_korean_verb(lemma)\n",
    "                counter[norm] += 1\n",
    "                locations[norm].append((si, i, tok))\n",
    "    return counter, locations\n",
    "\n",
    "def select_verbs_by_quantiles(counter, top_pct=0.20, next_pct=0.20, top_k_each=20):\n",
    "    \"\"\"\n",
    "        Select verbs by frequency quantiles.\n",
    "        Returns two lists: top verbs and next verbs.\n",
    "    \"\"\"\n",
    "    items = counter.most_common()\n",
    "    types = [v for v, _ in items]\n",
    "    n_types = len(types)\n",
    "    top_n_types = max(1, int(n_types * top_pct))\n",
    "    next_n_types = max(1, int(n_types * next_pct))\n",
    "    top_type_set = set(types[:top_n_types])\n",
    "    next_type_set = set(types[top_n_types: top_n_types + next_n_types])\n",
    "    top_candidates = [v for v, _ in items if v in top_type_set]\n",
    "    next_candidates = [v for v, _ in items if v in next_type_set]\n",
    "    return top_candidates[:top_k_each], next_candidates[:top_k_each]\n",
    "\n",
    "def extract_verb_sets(sents, verb_locations, verbs, \n",
    "                      subj_deprels=(\"nsubj\", \"nsubj:pass\", \"csubj\"), \n",
    "                      obj_deprels_prefix=(\"obj\",), \n",
    "                      modifier_deprels_prefixes=(\"advmod\", \"amod\", \"nmod\", \"obl\", \"advcl\", \"compound\")):\n",
    "    \"\"\"\n",
    "        Extract sets of subjects, objects, modifiers, and surrounding words for given verbs.\n",
    "        Returns a dict: verb -> {subject:Counter, object:Counter, modifier:Counter, before:Counter, after:Counter, occurrences:int}\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for verb in verbs:\n",
    "        subj_c = Counter()\n",
    "        obj_c = Counter()\n",
    "        mod_c = Counter()\n",
    "        before_c = Counter()\n",
    "        after_c = Counter()\n",
    "        occ = 0\n",
    "        locs = verb_locations.get(verb, [])\n",
    "        for si, vi, original_tok in locs:\n",
    "            sent = sents[si]\n",
    "            if vi < 0 or vi >= len(sent):\n",
    "                continue\n",
    "            occ += 1\n",
    "            id_to_tok = {t['id']: t for t in sent}\n",
    "            dependents = defaultdict(list)\n",
    "            for t in sent:\n",
    "                h = t['head']\n",
    "                if h is not None and h in id_to_tok:\n",
    "                    dependents[h].append(t)\n",
    "            v_deps = dependents.get(original_tok['id'], [])\n",
    "            for dep in v_deps:\n",
    "                deprel = dep['deprel']\n",
    "                if deprel in subj_deprels or deprel.startswith(\"nsubj\"):\n",
    "                    head_form = dep['lemma'] if dep['lemma'] != \"_\" else dep['form']\n",
    "                    subj_c[head_form] += 1\n",
    "                if any(deprel == p or deprel.startswith(p) for p in obj_deprels_prefix):\n",
    "                    head_form = dep['lemma'] if dep['lemma'] != \"_\" else dep['form']\n",
    "                    obj_c[head_form] += 1\n",
    "                if any(deprel == p or deprel.startswith(p) for p in modifier_deprels_prefixes):\n",
    "                    head_form = dep['lemma'] if dep['lemma'] != \"_\" else dep['form']\n",
    "                    mod_c[head_form] += 1\n",
    "            if vi - 1 >= 0:\n",
    "                before_c[sent[vi-1]['form']] += 1\n",
    "            if vi + 1 < len(sent):\n",
    "                after_c[sent[vi+1]['form']] += 1\n",
    "        results[verb] = {\n",
    "            \"subject\": subj_c,\n",
    "            \"object\": obj_c,\n",
    "            \"modifier\": mod_c,\n",
    "            \"before\": before_c,\n",
    "            \"after\": after_c,\n",
    "            \"occurrences\": occ\n",
    "        }\n",
    "    return results\n",
    "\n",
    "def load_korean_vector_model(bin_path=None, tsv_path=None, verbose=True):\n",
    "    \"\"\"\n",
    "        Load a Korean word vector model from binary or TSV format.\n",
    "        Handles Kyubyong multi-line bracketed format (index<TAB>word<TAB>[ v v v ... across lines ... ])\n",
    "        Returns a KeyedVectors instance.\n",
    "    \"\"\"\n",
    "    attempts = []\n",
    "    \n",
    "    def _log(msg):\n",
    "        if verbose:\n",
    "            print(msg)\n",
    "\n",
    "    if tsv_path and os.path.exists(tsv_path):\n",
    "        try:\n",
    "            _log(f\"Inspecting text vector file {tsv_path} ...\")\n",
    "            words = []\n",
    "            vecs = []\n",
    "            dim = None\n",
    "            with open(tsv_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as fh:\n",
    "                line_num = 0\n",
    "                collecting = False\n",
    "                current_word = None\n",
    "                current_vals = []\n",
    "                open_bracket_re = re.compile(r'^\\s*\\d+\\t([^\\t]+)\\t\\[')\n",
    "                for raw_line in fh:\n",
    "                    line_num += 1\n",
    "                    line = raw_line.rstrip(\"\\n\")\n",
    "                    if not line.strip():\n",
    "                        continue\n",
    "                    if not collecting:\n",
    "                        m = open_bracket_re.match(line)\n",
    "                        if m:\n",
    "                            current_word = m.group(1)\n",
    "                            idx = line.find('[')\n",
    "                            after = line[idx+1:].strip()\n",
    "                            if after:\n",
    "                                parts = re.split(r'\\s+', after)\n",
    "                                for p in parts:\n",
    "                                    if p == ']' or p == '':\n",
    "                                        continue\n",
    "                                    p_clean = p.strip(\"[],\")\n",
    "                                    if p_clean:\n",
    "                                        current_vals.append(p_clean)\n",
    "                            collecting = True\n",
    "                        else:\n",
    "                            parts0 = line.split(\"\\t\")\n",
    "                            if len(parts0) >= 2 and parts0[0].isdigit():\n",
    "                                current_word = parts0[1]\n",
    "                                current_vals = []\n",
    "                                collecting = True\n",
    "                                if '[' in line:\n",
    "                                    idx = line.find('[')\n",
    "                                    after = line[idx+1:].strip()\n",
    "                                    if after:\n",
    "                                        parts = re.split(r'\\s+', after)\n",
    "                                        for p in parts:\n",
    "                                            p_clean = p.strip(\"[],\")\n",
    "                                            if p_clean:\n",
    "                                                current_vals.append(p_clean)\n",
    "                            else:\n",
    "                                continue\n",
    "                    else:\n",
    "                        if ']' in line:\n",
    "                            before = line.split(']')[0].strip()\n",
    "                            if before:\n",
    "                                parts = re.split(r'\\s+', before)\n",
    "                                for p in parts:\n",
    "                                    p_clean = p.strip(\"[],\")\n",
    "                                    if p_clean:\n",
    "                                        current_vals.append(p_clean)\n",
    "                            try:\n",
    "                                vec = np.array([float(x) for x in current_vals], dtype=np.float32)\n",
    "                            except Exception as e:\n",
    "                                _log(f\"Warning: failed to convert vector for word {current_word} at line {line_num}: {e}\")\n",
    "                                current_word = None\n",
    "                                current_vals = []\n",
    "                                collecting = False\n",
    "                                continue\n",
    "                            if dim is None:\n",
    "                                dim = vec.shape[0]\n",
    "                            else:\n",
    "                                if vec.shape[0] != dim:\n",
    "                                    _log(f\"Warning: inconsistent dim for word {current_word}: {vec.shape[0]} vs {dim}; skipping\")\n",
    "                                    current_word = None\n",
    "                                    current_vals = []\n",
    "                                    collecting = False\n",
    "                                    continue\n",
    "                            words.append(current_word)\n",
    "                            vecs.append(vec)\n",
    "                            current_word = None\n",
    "                            current_vals = []\n",
    "                            collecting = False\n",
    "                        else:\n",
    "                            parts = re.split(r'\\s+', line.strip())\n",
    "                            for p in parts:\n",
    "                                p_clean = p.strip(\"[],\")\n",
    "                                if p_clean:\n",
    "                                    current_vals.append(p_clean)\n",
    "            if not words:\n",
    "                raise RuntimeError(\"Parsed zero word vectors from the TSV file.\")\n",
    "            arr = np.vstack(vecs)\n",
    "            kv = KeyedVectors(vector_size=arr.shape[1])\n",
    "            kv.add_vectors(words, arr)\n",
    "            _log(f\"Constructed KeyedVectors from text file with {len(words)} words and dim {arr.shape[1]}.\")\n",
    "            return kv\n",
    "        except Exception as e:\n",
    "            attempts.append((\"w2v_text_kyubyong\", str(e)))\n",
    "\n",
    "    msg = \"Failed to load model via TSV Kyubyong parser. Attempts:\\n\" + \"\\n\".join(f\"{k}: {v}\" for k, v in attempts)\n",
    "    raise RuntimeError(msg)\n",
    "\n",
    "COMMON_PARTICLES = {\n",
    "    \"이\",\"가\",\"을\",\"를\",\"은\",\"는\",\"에\",\"에서\",\"으로\",\"로\",\"와\",\"과\",\"에게\",\"께\",\"부터\",\"까지\",\"만\",\"도\",\"뿐\",\"처럼\",\"까지\",\"보다\"\n",
    "}\n",
    "\n",
    "def kaist_to_surface_candidates(token):\n",
    "    if token is None:\n",
    "        return []\n",
    "    t = str(token).strip()\n",
    "    if not t or t == \"_\":\n",
    "        return []\n",
    "    candidates = []\n",
    "    if \"+\" in t:\n",
    "        joined = t.replace(\"+\", \"\")\n",
    "        candidates.append(joined)\n",
    "        if joined.endswith(\"다\") and len(joined) > 1:\n",
    "            candidates.append(joined[:-1])\n",
    "        parts = [p for p in t.split(\"+\") if p]\n",
    "        if parts:\n",
    "            candidates.append(parts[0])\n",
    "            if parts[-1] in COMMON_PARTICLES and len(parts) >= 2:\n",
    "                maybe = \"\".join(parts[:-1])\n",
    "                candidates.append(maybe)\n",
    "    else:\n",
    "        candidates.append(t)\n",
    "        if len(t) > 1 and t.endswith(\"다\"):\n",
    "            candidates.append(t[:-1])\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for c in candidates:\n",
    "        if c and c not in seen:\n",
    "            seen.add(c)\n",
    "            out.append(c)\n",
    "    return out\n",
    "\n",
    "def centroid_of_words(model, words):\n",
    "    vecs = []\n",
    "    missed = []\n",
    "    for w in words:\n",
    "        if w is None:\n",
    "            continue\n",
    "        if isinstance(w, (tuple, list)):\n",
    "            keyword = w[0]\n",
    "        else:\n",
    "            keyword = str(w)\n",
    "        candidates = kaist_to_surface_candidates(keyword)\n",
    "        found_key = None\n",
    "        for cand in candidates:\n",
    "            if cand in model:\n",
    "                found_key = cand\n",
    "                break\n",
    "        if found_key is None and keyword in model:\n",
    "            found_key = keyword\n",
    "        if found_key is not None:\n",
    "            vecs.append(model[found_key])\n",
    "        else:\n",
    "            missed.append(keyword)\n",
    "    if not vecs:\n",
    "        return None, 0, missed\n",
    "    arr = np.vstack(vecs)\n",
    "    return np.mean(arr, axis=0), arr.shape[0], missed\n",
    "\n",
    "def topk_neighbors_from_centroid(model, centroid_vec, k=10):\n",
    "    if centroid_vec is None:\n",
    "        return []\n",
    "    return model.similar_by_vector(centroid_vec, topn=k)\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "# ----------------------------\n",
    "# Negation analysis helpers and extraction\n",
    "# ----------------------------\n",
    "def is_short_neg_token(tok):\n",
    "    if not tok:\n",
    "        return False\n",
    "    if (tok.get(\"lemma\") == \"안\" or tok.get(\"form\") == \"안\") and tok.get(\"upos\") == \"ADV\":\n",
    "        return True\n",
    "    if tok.get(\"form\") == \"안\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_long_neg_token(tok):\n",
    "    if not tok:\n",
    "        return False\n",
    "    lemma = tok.get(\"lemma\") or \"\"\n",
    "    form = tok.get(\"form\") or \"\"\n",
    "    upos = tok.get(\"upos\") or \"\"\n",
    "    deprel = tok.get(\"deprel\") or \"\"\n",
    "    if (\"않\" in lemma) or (form.startswith(\"않\")):\n",
    "        if upos == \"AUX\" or deprel == \"aux\" or upos == \"VERB\":\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_irregular_neg_token(tok):\n",
    "    if not tok:\n",
    "        return False\n",
    "    lemma = tok.get(\"lemma\") or \"\"\n",
    "    form = tok.get(\"form\") or \"\"\n",
    "    if lemma.startswith(\"없\") or form.startswith(\"없\"):\n",
    "        return True\n",
    "    if lemma.startswith(\"아니\") or form.startswith(\"아니\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def classify_verb_negation(sent, verb_idx, verb_tok, dependents_map):\n",
    "    verb_id = verb_tok['id']\n",
    "    deps = dependents_map.get(verb_id, [])\n",
    "    for d in deps:\n",
    "        if is_long_neg_token(d):\n",
    "            return \"long\"\n",
    "    for d in deps:\n",
    "        if is_short_neg_token(d):\n",
    "            return \"short\"\n",
    "    if verb_idx - 1 >= 0:\n",
    "        prev = sent[verb_idx - 1]\n",
    "        if is_short_neg_token(prev):\n",
    "            return \"short\"\n",
    "    if is_irregular_neg_token(verb_tok):\n",
    "        return \"irregular\"\n",
    "    return \"affirmative\"\n",
    "\n",
    "def extract_negation_sensitive_sets(sents, verb_locations, verbs):\n",
    "    NEG_TYPES = (\"long\",\"short\",\"irregular\",\"affirmative\")\n",
    "    results = {}\n",
    "    for verb in verbs:\n",
    "        buckets = {nt: {\"subject\": Counter(), \"object\": Counter(), \"modifier\": Counter(), \"before\": Counter(), \"after\": Counter(), \"occurrences\": 0} for nt in NEG_TYPES}\n",
    "        locs = verb_locations.get(verb, [])\n",
    "        for si, vi, original_tok in locs:\n",
    "            sent = sents[si]\n",
    "            if vi < 0 or vi >= len(sent):\n",
    "                continue\n",
    "            id_to_tok = {t['id']: t for t in sent}\n",
    "            dependents = defaultdict(list)\n",
    "            for t in sent:\n",
    "                h = t['head']\n",
    "                if h is not None and h in id_to_tok:\n",
    "                    dependents[h].append(t)\n",
    "            neg_type = classify_verb_negation(sent, vi, original_tok, dependents)\n",
    "            buckets[neg_type][\"occurrences\"] += 1\n",
    "            v_deps = dependents.get(original_tok['id'], [])\n",
    "            for dep in v_deps:\n",
    "                deprel = dep['deprel']\n",
    "                if deprel.startswith(\"nsubj\") or deprel in (\"nsubj\",\"csubj\",\"nsubj:pass\"):\n",
    "                    head_form = dep['lemma'] if dep['lemma'] != \"_\" else dep['form']\n",
    "                    buckets[neg_type][\"subject\"][head_form] += 1\n",
    "                if deprel.startswith(\"obj\") or deprel == \"obj\":\n",
    "                    head_form = dep['lemma'] if dep['lemma'] != \"_\" else dep['form']\n",
    "                    buckets[neg_type][\"object\"][head_form] += 1\n",
    "                if any(deprel == p or deprel.startswith(p) for p in (\"advmod\",\"amod\",\"nmod\",\"obl\",\"advcl\",\"compound\")):\n",
    "                    head_form = dep['lemma'] if dep['lemma'] != \"_\" else dep['form']\n",
    "                    buckets[neg_type][\"modifier\"][head_form] += 1\n",
    "            if vi - 1 >= 0:\n",
    "                buckets[neg_type][\"before\"][sent[vi-1]['form']] += 1\n",
    "            if vi + 1 < len(sent):\n",
    "                buckets[neg_type][\"after\"][sent[vi+1]['form']] += 1\n",
    "        results[verb] = buckets\n",
    "    return results\n",
    "\n",
    "def noun_negation_association(neg_buckets, top_n=50):\n",
    "    subj_counter_total = Counter()\n",
    "    subj_long = Counter()\n",
    "    subj_short = Counter()\n",
    "    for verb, buckets in neg_buckets.items():\n",
    "        if verb in EXCLUDE_VERBS:\n",
    "            continue\n",
    "        for nt in (\"long\",\"short\",\"irregular\",\"affirmative\"):\n",
    "            subj_ct = buckets[nt][\"subject\"]\n",
    "            for noun, cnt in subj_ct.items():\n",
    "                subj_counter_total[noun] += cnt\n",
    "                if nt == \"long\":\n",
    "                    subj_long[noun] += cnt\n",
    "                if nt == \"short\":\n",
    "                    subj_short[noun] += cnt\n",
    "    top_nouns = [n for n, _ in subj_counter_total.most_common(top_n)]\n",
    "    rows = []\n",
    "    for n in top_nouns:\n",
    "        tot = subj_counter_total[n]\n",
    "        ln = subj_long[n]\n",
    "        sn = subj_short[n]\n",
    "        long_ratio = ln / tot if tot > 0 else 0.0\n",
    "        rows.append((n, int(tot), int(ln), int(sn), float(long_ratio)))\n",
    "    return rows\n",
    "\n",
    "# ----------------------------\n",
    "# Orchestration: main pipeline adapted for negation analysis \n",
    "# ----------------------------\n",
    "def build_task2_analysis_negation(conllu_path=CONLLU_PATH, model_bin=None, model_tsv=None, k_neighbors=10, top_k_each_top20=6, top_k_each_mid=20):\n",
    "    sents = parse_conllu(conllu_path)\n",
    "    print(f\"Loaded {len(sents)} sentences from {conllu_path}\")\n",
    "    raw_counter, raw_locations = verb_frequencies(sents)\n",
    "\n",
    "    verb_counter, verb_locations = aggregate_normalized_counts(raw_counter, raw_locations)\n",
    "\n",
    "    # Remove copula from consideration so it doesn't inflate negation files/analyses\n",
    "    if COPULA_CANONICAL in verb_counter:\n",
    "        del verb_counter[COPULA_CANONICAL]\n",
    "    if COPULA_CANONICAL in verb_locations:\n",
    "        del verb_locations[COPULA_CANONICAL]\n",
    "\n",
    "    vf_df = pd.DataFrame(verb_counter.most_common(), columns=[\"verb_norm\", \"freq\"])\n",
    "    \n",
    "    vf_df.to_csv(os.path.join(OUT_DIR, \"verb_frequencies_normalized.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # select by quantiles; we will crop top list to top_k_each_top20\n",
    "    top_verbs, mid_verbs = select_verbs_by_quantiles(verb_counter, top_pct=0.20, next_pct=0.20, top_k_each=top_k_each_mid)\n",
    "    top_verbs = top_verbs[:top_k_each_top20]  # show top 6 for top-20% so to include another 5 apart from copula\n",
    "\n",
    "    # Filter out any excluded verbs (just in case)\n",
    "    top_verbs = [v for v in top_verbs if v not in EXCLUDE_VERBS]\n",
    "    mid_verbs = [v for v in mid_verbs if v not in EXCLUDE_VERBS]\n",
    "\n",
    "    print(\"Top verbs (sample):\", top_verbs)\n",
    "    print(\"Mid verbs (sample):\", mid_verbs[:top_k_each_mid])\n",
    "    # hardcoded transitives \n",
    "    hardcoded_list = ['하','찾','주','쓰','가지','깨뜨리','따','먹이','고르','지르']\n",
    "    verbs_to_analyze = list(dict.fromkeys(list(top_verbs) + list(mid_verbs[:top_k_each_mid]) + hardcoded_list))\n",
    "    # ensure we don't accidentally analyze copula\n",
    "    verbs_to_analyze = [v for v in verbs_to_analyze if v not in EXCLUDE_VERBS]\n",
    "\n",
    "    sets = extract_verb_sets(sents, verb_locations, verbs_to_analyze)\n",
    "    summary = {}\n",
    "    for v in verbs_to_analyze:\n",
    "        entry = sets.get(v, {\"subject\":Counter(),\"object\":Counter(),\"modifier\":Counter(),\"before\":Counter(),\"after\":Counter(),\"occurrences\":0})\n",
    "        summary[v] = {\n",
    "            \"occurrences\": entry[\"occurrences\"],\n",
    "            \"top_subjects\": entry[\"subject\"].most_common(50),\n",
    "            \"top_objects\": entry[\"object\"].most_common(50),\n",
    "            \"top_modifiers\": entry[\"modifier\"].most_common(50),\n",
    "            \"top_before\": entry[\"before\"].most_common(50),\n",
    "            \"top_after\": entry[\"after\"].most_common(50)\n",
    "        }\n",
    "    with open(os.path.join(OUT_DIR, \"verb_sets_summary_normalized.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # NEGATION-SENSITIVE extraction\n",
    "    neg_buckets = extract_negation_sensitive_sets(sents, verb_locations, verbs_to_analyze)\n",
    "\n",
    "    # Write negation summary JSON \n",
    "    neg_summary = {}\n",
    "    for v, buckets in neg_buckets.items():\n",
    "        if v in EXCLUDE_VERBS:\n",
    "            continue\n",
    "        neg_summary[v] = {}\n",
    "        for nt, data in buckets.items():\n",
    "            neg_summary[v][nt] = {\n",
    "                \"occurrences\": int(data[\"occurrences\"]),\n",
    "                \"top_subjects\": data[\"subject\"].most_common(50),\n",
    "                \"top_objects\": data[\"object\"].most_common(50),\n",
    "                \"top_modifiers\": data[\"modifier\"].most_common(50),\n",
    "                \"top_before\": data[\"before\"].most_common(50),\n",
    "                \"top_after\": data[\"after\"].most_common(50),\n",
    "            }\n",
    "    neg_out_path = os.path.join(OUT_DIR, \"verb_negation_summary.json\")\n",
    "    with open(neg_out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(neg_summary, f, ensure_ascii=False, indent=2)\n",
    "    print(\"Saved negation-aware summary to\", neg_out_path)\n",
    "\n",
    "    # Noun association summary (CSV)\n",
    "    noun_assoc_rows = noun_negation_association(neg_buckets, top_n=100)\n",
    "    noun_assoc_df = pd.DataFrame(noun_assoc_rows, columns=[\"noun\",\"total_subj_count\",\"long_count\",\"short_count\",\"long_ratio\"])\n",
    "    noun_assoc_df.to_csv(os.path.join(OUT_DIR, \"noun_negation_association.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # centroids & knn per negation bucket (Unsure if this is necessary)\n",
    "    model = None\n",
    "    neighbors_summary = {}\n",
    "    if (model_bin and os.path.exists(model_bin)) or (model_tsv and os.path.exists(model_tsv)):\n",
    "        print(\"Attempting to load vector model...\")\n",
    "        model = load_korean_vector_model(bin_path=model_bin, tsv_path=model_tsv)\n",
    "        print(\"Computing centroids and nearest neighbors...\")\n",
    "        for v, buckets in neg_buckets.items():\n",
    "            if v in EXCLUDE_VERBS:\n",
    "                continue\n",
    "            neighbors_summary[v] = {}\n",
    "            for nt, data in buckets.items():\n",
    "                words = [w for w, cnt in data[\"subject\"].most_common(200)]\n",
    "                centroid, n_in_vocab, missed = centroid_of_words(model, words)\n",
    "                knn = topk_neighbors_from_centroid(model, centroid, k=k_neighbors) if centroid is not None else []\n",
    "                neighbors_summary[v][nt] = {\n",
    "                    \"subject_centroid_n_in_vocab\": int(n_in_vocab),\n",
    "                    \"subject_missed_count\": len(missed),\n",
    "                    \"subject_missed_examples\": missed[:30],\n",
    "                    \"subject_knn\": [(w, float(sim)) for w, sim in knn]\n",
    "                }\n",
    "        with open(os.path.join(OUT_DIR, \"verb_negation_neighbors.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(neighbors_summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return {\n",
    "        \"sentences\": len(sents),\n",
    "        \"verb_freq_df\": vf_df,\n",
    "        \"top_verbs\": top_verbs,\n",
    "        \"mid_verbs\": mid_verbs,\n",
    "        \"verbs_to_analyze\": verbs_to_analyze,\n",
    "        \"sets\": sets,\n",
    "        \"summary\": summary,\n",
    "        \"neg_buckets\": neg_buckets,\n",
    "        \"neg_summary\": neg_summary,\n",
    "        \"noun_assoc_df\": noun_assoc_df,\n",
    "        \"neighbors_summary\": neighbors_summary if model else None\n",
    "    }\n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "res = build_task2_analysis_negation(\n",
    "    conllu_path=\"ko_kaist-ud-train.conllu\",\n",
    "    model_bin=\"ko.bin\",     \n",
    "    model_tsv=\"ko.tsv\",     \n",
    "    k_neighbors=20,\n",
    "    top_k_each_top20=6,\n",
    "    top_k_each_mid=5\n",
    ")\n",
    "\n",
    "print(\"Done. Outputs saved in:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e1be162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            class  total_all  neg_total  p_long_of_neg  p_short_of_neg\n",
      "transitive_action       4964         87       0.954023        0.045977\n",
      "      existential       3468          0            NaN             NaN\n",
      "  change_of_state       2017        128       0.289062        0.710938\n",
      "            other       1706          0            NaN             NaN\n",
      "       perception        780         10       1.000000        0.000000\n",
      "           motion          7          0            NaN             NaN\n",
      "            class  odds_ratio      p_value  cls_long  cls_short  rest_long  rest_short\n",
      "  change_of_state    0.017488 4.147050e-27        37         91         93           4\n",
      "transitive_action   40.175532 2.526940e-22        83          4         47          91\n",
      "       perception         inf 5.738008e-03        10          0        120          95\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import json, os\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "from scipy.stats import fisher_exact\n",
    "\n",
    "OUT_DIR = \"data/project_outputs_task2\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "INPUT_NEG_SUMMARY = os.path.join(OUT_DIR, \"verb_negation_summary.json\")\n",
    "OUTPUT_PREFIX = os.path.join(OUT_DIR, \"verb_negation_summary_no_copula\")\n",
    "\n",
    "HARDCODED_TRANSITIVE = ['하','찾','주','쓰','가지','깨뜨리','따','먹이','고르','지르']\n",
    "\n",
    "CLASS_SIGNATURES = {\n",
    "    \"transitive_action\": HARDCODED_TRANSITIVE,\n",
    "    \"speech\": [\"말\",\"묻\",\"질문\",\"이야기\",\"말하\",\"발화\",\"선언\",\"답변\",\"전하\",\"말씀\"],\n",
    "    \"cognition_belief\": [\"생각\",\"믿\",\"추정\",\"판단\",\"의견\",\"알\",\"인지\",\"신념\",\"추측\",\"생각하\"],\n",
    "    \"motion\": [\"가\",\"오\",\"내려\",\"도착\",\"올라\",\"움직\",\"들어\",\"나가\",\"돌아\",\"출발\",\"도망\"],\n",
    "    \"change_of_state\": [\"되\",\"변하\",\"증가\",\"감소\",\"생기\",\"개선\",\"시작\",\"멈추\",\"끝나\",\"확산\"],\n",
    "    \"existential\": [\"있\",\"없\"],\n",
    "    \"perception\": [\"보이\",\"보\",\"들리\",\"느끼\",\"관찰\"],\n",
    "}\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def write_json(obj, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def safe_int(x):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def noun_negation_association(neg_summary, top_n=100):\n",
    "    total = Counter()\n",
    "    long_c = Counter()\n",
    "    short_c = Counter()\n",
    "    for verb, buckets in neg_summary.items():\n",
    "        for nt in (\"long\", \"short\", \"affirmative\"):\n",
    "            subj_ct = buckets.get(nt, {}).get(\"top_subjects\", [])\n",
    "            for noun, cnt in subj_ct:\n",
    "                total[noun] += cnt\n",
    "                if nt == \"long\":\n",
    "                    long_c[noun] += cnt\n",
    "                if nt == \"short\":\n",
    "                    short_c[noun] += cnt\n",
    "    top_nouns = [n for n, _ in total.most_common(top_n)]\n",
    "    rows = []\n",
    "    for n in top_nouns:\n",
    "        tot = total[n]\n",
    "        ln = long_c[n]\n",
    "        sn = short_c[n]\n",
    "        lr = ln / tot if tot > 0 else 0.0\n",
    "        rows.append((n, int(tot), int(ln), int(sn), float(lr)))\n",
    "    return pd.DataFrame(rows, columns=[\"noun\",\"total_subj_count\",\"long_count\",\"short_count\",\"long_ratio\"])\n",
    "\n",
    "def remove_copula_from_summary(neg_summary, copula_key=\"이\"):\n",
    "    if copula_key in neg_summary:\n",
    "        out = dict(neg_summary)\n",
    "        del out[copula_key]\n",
    "        return out\n",
    "    return neg_summary\n",
    "\n",
    "def assign_semantic_class(verb, verb_summary):\n",
    "    v = str(verb)\n",
    "    if v in HARDCODED_TRANSITIVE:\n",
    "        return \"transitive_action\"\n",
    "    for cls, sigs in CLASS_SIGNATURES.items():\n",
    "        for s in sigs:\n",
    "            if s and s in v:\n",
    "                return cls\n",
    "    if verb_summary is not None:\n",
    "        subj_count = sum(cnt for _, cnt in verb_summary.get(\"top_subjects\", []))\n",
    "        obj_count = sum(cnt for _, cnt in verb_summary.get(\"top_objects\", []))\n",
    "        if obj_count > subj_count * 1.5 and obj_count >= 3:\n",
    "            return \"transitive_action\"\n",
    "    return \"other\"\n",
    "\n",
    "def aggregate_class_stats(neg_summary, verb_to_class):\n",
    "    class_stats = defaultdict(lambda: {\"affirmative\":0,\"long\":0,\"short\":0,\"verbs\":[]})\n",
    "    for verb, buckets in neg_summary.items():\n",
    "        cls = verb_to_class.get(verb, \"other\")\n",
    "        class_stats[cls][\"verbs\"].append(verb)\n",
    "        for key in (\"affirmative\",\"long\",\"short\"):\n",
    "            cnt = buckets.get(key, {}).get(\"occurrences\", 0)\n",
    "            class_stats[cls][key] += safe_int(cnt)\n",
    "    out = []\n",
    "    for cls, d in class_stats.items():\n",
    "        aff = d[\"affirmative\"]\n",
    "        lo = d[\"long\"]\n",
    "        sh = d[\"short\"]\n",
    "        neg_total = lo + sh\n",
    "        p_long_of_neg = (lo / neg_total) if neg_total > 0 else float(\"nan\")\n",
    "        p_short_of_neg = (sh / neg_total) if neg_total > 0 else float(\"nan\")\n",
    "        total_all = aff + neg_total\n",
    "        neg_rate_of_all = (neg_total / total_all) if total_all > 0 else 0.0\n",
    "        out.append({\n",
    "            \"class\": cls,\n",
    "            \"affirmative\": aff,\n",
    "            \"long\": lo,\n",
    "            \"short\": sh,\n",
    "            \"neg_total\": neg_total,\n",
    "            \"p_long_of_neg\": p_long_of_neg,\n",
    "            \"p_short_of_neg\": p_short_of_neg,\n",
    "            \"neg_rate_of_all\": neg_rate_of_all,\n",
    "            \"total_all\": total_all,\n",
    "            \"verbs\": d[\"verbs\"]\n",
    "        })\n",
    "    df = pd.DataFrame(out).sort_values(by=\"total_all\", ascending=False).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def fisher_test_for_class(class_df):\n",
    "    rows = []\n",
    "    counts = {r[\"class\"]:(r[\"long\"], r[\"short\"]) for _, r in class_df.iterrows()}\n",
    "    grand_long = sum([v[0] for v in counts.values()])\n",
    "    grand_short = sum([v[1] for v in counts.values()])\n",
    "    for _, r in class_df.iterrows():\n",
    "        cls = r[\"class\"]\n",
    "        cls_long = int(r[\"long\"])\n",
    "        cls_short = int(r[\"short\"])\n",
    "        rest_long = grand_long - cls_long\n",
    "        rest_short = grand_short - cls_short\n",
    "        table = [[cls_long, cls_short], [rest_long, rest_short]]\n",
    "        degenerate = (cls_long + cls_short == 0) or (rest_long + rest_short == 0)\n",
    "        if degenerate:\n",
    "            rows.append({\"class\":cls,\"cls_long\":cls_long,\"cls_short\":cls_short,\"rest_long\":rest_long,\"rest_short\":rest_short,\"test\":\"none\",\"odds_ratio\":None,\"p_value\":None})\n",
    "            continue\n",
    "        try:\n",
    "            oddsratio, p = fisher_exact(table, alternative=\"two-sided\")\n",
    "            rows.append({\"class\":cls,\"cls_long\":cls_long,\"cls_short\":cls_short,\"rest_long\":rest_long,\"rest_short\":rest_short,\"test\":\"fisher_exact\",\"odds_ratio\":float(oddsratio),\"p_value\":float(p)})\n",
    "        except Exception as e:\n",
    "            rows.append({\"class\":cls,\"cls_long\":cls_long,\"cls_short\":cls_short,\"rest_long\":rest_long,\"rest_short\":rest_short,\"test\":\"error\",\"odds_ratio\":None,\"p_value\":None})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def main(input_path=INPUT_NEG_SUMMARY):\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"Input not found: {input_path}\")\n",
    "    neg_summary_raw = load_json(input_path)\n",
    "    neg_summary = remove_copula_from_summary(neg_summary_raw, copula_key=\"이\")\n",
    "    out_neg_path = OUTPUT_PREFIX + \".json\"\n",
    "    write_json(neg_summary, out_neg_path)\n",
    "    noun_assoc_df = noun_negation_association(neg_summary, top_n=200)\n",
    "    noun_assoc_csv = os.path.join(OUT_DIR, \"noun_negation_association_no_copula.csv\")\n",
    "    noun_assoc_df.to_csv(noun_assoc_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    verb_to_class = {verb: assign_semantic_class(verb, data) for verb, data in neg_summary.items()}\n",
    "    for k in list(verb_to_class.keys()):\n",
    "        if verb_to_class[k] == \"copula\":\n",
    "            verb_to_class[k] = \"other\"\n",
    "    verb_to_class_path = os.path.join(OUT_DIR, \"verb_to_class_no_copula.json\")\n",
    "    write_json(verb_to_class, verb_to_class_path)\n",
    "    class_df = aggregate_class_stats(neg_summary, verb_to_class)\n",
    "    class_csv = os.path.join(OUT_DIR, \"semantic_class_neg_stats_no_copula.csv\")\n",
    "    class_df.to_csv(class_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    fisher_df = fisher_test_for_class(class_df)\n",
    "    fisher_csv = os.path.join(OUT_DIR, \"semantic_class_fisher_tests_no_copula.csv\")\n",
    "    fisher_df.to_csv(fisher_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    sig = fisher_df[(fisher_df[\"p_value\"].notnull()) & (fisher_df[\"p_value\"] < 0.05)].sort_values(\"p_value\")\n",
    "    print(class_df[[\"class\",\"total_all\",\"neg_total\",\"p_long_of_neg\",\"p_short_of_neg\"]].head(20).to_string(index=False))\n",
    "    if sig.empty:\n",
    "        print(\"No significant class-level differences (p < 0.05).\")\n",
    "    else:\n",
    "        print(sig[[\"class\",\"odds_ratio\",\"p_value\",\"cls_long\",\"cls_short\",\"rest_long\",\"rest_short\"]].to_string(index=False))\n",
    "    return {\"neg_summary_no_copula\": out_neg_path, \"noun_assoc_csv\": noun_assoc_csv, \"verb_to_class_json\": verb_to_class_path, \"class_stats_csv\": class_csv, \"fisher_csv\": fisher_csv}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    out = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65b41cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verb_info(verb):\n",
    "    print(f\"Verb: {verb}\")\n",
    "    entry = res['summary'][verb]\n",
    "    print(f\"  Occurrences: {entry['occurrences']}\")\n",
    "    print(f\"  Top Subjects: {entry['top_subjects'][:5]}\")\n",
    "    print(f\"  Top Objects: {entry['top_objects'][:5]}\")\n",
    "    print(f\"  Top Modifiers: {entry['top_modifiers'][:5]}\")\n",
    "    print(f\"  Top Before: {entry['top_before'][:5]}\")\n",
    "    print(f\"  Top After: {entry['top_after'][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9bbeb7",
   "metadata": {},
   "source": [
    "### Task 2.1\n",
    "#### 5 Verbs from Top 20%\n",
    "List of verbs: ['이', '하', '있', '되', '않']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c5a5881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb: 이\n",
      "  Occurrences: 10508\n",
      "  Top Subjects: [('것+이', 213), ('이것+이', 59), ('그것+이', 32), ('자체+가', 10), ('사람+이', 8)]\n",
      "  Top Objects: [('이+를', 4), ('이것+을', 3), ('작품+을', 2), ('정+을', 1), ('세계관+을', 1)]\n",
      "  Top Modifiers: [('것+이+다', 131), ('바로', 120), ('것+도', 64), ('가장', 40), ('때문+이+다', 34)]\n",
      "  Top Before: [('있는', 322), ('있을', 133), (\"'\", 126), ('할', 125), ('될', 115)]\n",
      "  Top After: [('.', 6029), ('것이다', 185), ('?', 150), ('할', 109), ('때문이다', 54)]\n",
      "\n",
      "Verb: 하\n",
      "  Occurrences: 3704\n",
      "  Top Subjects: [('내+가', 7), ('우리+가', 6), ('자기+가', 6), ('그+가', 6), ('그것+이', 3)]\n",
      "  Top Objects: [('역할+을', 45), ('일+을', 26), ('것+을', 22), ('생각+을', 13), ('말+을', 12)]\n",
      "  Top Modifiers: [('수', 269), ('것+이+다', 75), ('필요+로', 24), ('때', 23), ('것+이+ㅂ니다', 19)]\n",
      "  Top Before: [('있다고', 48), ('해야', 47), ('역할을', 43), ('있어야', 42), ('필요로', 24)]\n",
      "  Top After: [('.', 1546), ('수', 271), ('것이다', 163), ('것은', 62), (',', 58)]\n",
      "\n",
      "Verb: 있\n",
      "  Occurrences: 3468\n",
      "  Top Subjects: [('문제+가', 4), ('것+이', 4), ('관계+가', 4), ('수+가', 3), ('필요+가', 2)]\n",
      "  Top Objects: []\n",
      "  Top Modifiers: [('수', 23), ('또한', 3), ('데+에', 3), ('데', 3), ('점+에서', 2)]\n",
      "  Top Before: [('가지고', 122), ('알고', 87), ('지니고', 77), ('하고', 71), ('되어', 64)]\n",
      "  Top After: [('.', 1606), ('것이다', 122), (',', 117), ('때문이다', 35), ('것을', 34)]\n",
      "\n",
      "Verb: 되\n",
      "  Occurrences: 2017\n",
      "  Top Subjects: [('것+이', 44), ('문제+가', 19), ('도움+이', 18), ('사람+이', 17), ('대상+이', 16)]\n",
      "  Top Objects: [('아이+들+을', 1), ('화살표+를', 1), ('제공자+들+을', 1)]\n",
      "  Top Modifiers: [('안', 91), ('것+이+다', 70), ('수', 49), ('잘', 17), ('것+이+ㅂ니다', 17)]\n",
      "  Top Before: [('안', 92), ('있게', 48), ('알게', 42), ('것이', 31), ('갖게', 28)]\n",
      "  Top After: [('.', 917), ('것이다', 179), ('것은', 50), ('수', 49), ('있다', 40)]\n",
      "\n",
      "Verb: 않\n",
      "  Occurrences: 1685\n",
      "  Top Subjects: [('마음+이', 1)]\n",
      "  Top Objects: []\n",
      "  Top Modifiers: [('때+마다', 1), ('편치', 1)]\n",
      "  Top Before: [('하지', 60), ('있지', 58), ('지나지', 51), ('존재하지', 37), ('되지', 37)]\n",
      "  Top After: [('.', 525), ('수', 64), (',', 62), ('것이다', 46), ('안', 36)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for v in res['top_verbs']:\n",
    "    verb_info(v)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4dae72",
   "metadata": {},
   "source": [
    "### Task 2.2\n",
    "#### 5 Verbs from Top 20-40%\n",
    "List of verbs: ['물리치', '민감', '내려오', '제창', '갈']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc7cee6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb: 물리치\n",
      "  Occurrences: 8\n",
      "  Top Subjects: []\n",
      "  Top Objects: [('이민족+을', 1), ('도깨비+를', 1), ('기운+을', 1), ('왜구+를', 1), ('재도전+을', 1)]\n",
      "  Top Modifiers: [('수', 2), ('그걸+로', 1), ('의심+도', 1)]\n",
      "  Top Before: [('이민족을', 1), ('도깨비를', 1), ('기운을', 1), ('왜구를', 1), ('재도전을', 1)]\n",
      "  Top After: [('수', 2), ('존왕양이의', 1), ('위해', 1), ('영약으로', 1), ('결심을', 1)]\n",
      "\n",
      "Verb: 민감\n",
      "  Occurrences: 7\n",
      "  Top Subjects: [('동물+들+이', 1)]\n",
      "  Top Objects: []\n",
      "  Top Modifiers: [('더', 2), ('측면+에', 1), ('여론+에', 1), ('것+에', 1), ('특성+상', 1)]\n",
      "  Top Before: [('더', 2), ('여론에', 1), ('정책에', 1), ('것에', 1), ('덜', 1)]\n",
      "  Top After: [('.', 2), ('인사라고나', 1), ('반응을', 1), ('법', 1), ('부분의', 1)]\n",
      "\n",
      "Verb: 내려오\n",
      "  Occurrences: 7\n",
      "  Top Subjects: []\n",
      "  Top Objects: []\n",
      "  Top Modifiers: [('예+로+부터', 1), ('고장+에', 1), ('가장자리+까지', 1), ('옛+부터', 1), ('조상+도', 1)]\n",
      "  Top Before: [('전해', 2), ('가장자리까지', 1), ('옛부터', 1), ('묻고', 1), ('하늘에서', 1)]\n",
      "  Top After: [('.', 2), ('좋은', 1), ('한다', 1), ('한', 1), ('날', 1)]\n",
      "\n",
      "Verb: 제창\n",
      "  Occurrences: 7\n",
      "  Top Subjects: [('사람+이', 1), ('루스벨트+가', 1), ('그+가', 1)]\n",
      "  Top Objects: [('것+을', 1), ('국채+보상+운동+을', 1), ('뉴딜정책+을', 1), ('민주주의+를', 1)]\n",
      "  Top Modifiers: [('이후', 1), ('1930+년대', 1), ('구제책+으로서', 1), ('중반+부터', 1), ('중심+으로', 1)]\n",
      "  Top Before: [('것을', 1), ('국채보상운동을', 1), ('위해서', 1), ('사람이', 1), ('뉴딜정책을', 1)]\n",
      "  Top After: [('.', 3), ('서상돈', 1), ('용어로', 1), ('하는', 1), ('왔다', 1)]\n",
      "\n",
      "Verb: 갈\n",
      "  Occurrences: 7\n",
      "  Top Subjects: []\n",
      "  Top Objects: [('부리+를', 2), ('콩+을', 1), ('넘+어', 1), ('땅+을', 1)]\n",
      "  Top Modifiers: [('믹서+에', 2), ('강판+에', 1), ('언제', 1), ('되+ㄴ', 1), ('틈틈이', 1)]\n",
      "  Top Before: [('믹서에', 2), ('강판에', 1), ('언제', 1), ('땅을', 1), ('날카롭게', 1)]\n",
      "  Top After: [('당근을', 1), ('때', 1), ('콩물을', 1), ('하는가', 1), ('된', 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for v in res['mid_verbs']:\n",
    "    verb_info(v)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1821ca",
   "metadata": {},
   "source": [
    "2A \n",
    "\n",
    "The object set gives the most useful signal about verb meaning. The before set also provides semantic meaning though it is a lot more noisy. Referring back to the verb 되다 (twoi., “to become”) from earlier, most of the words were pretty general, but from the objects ([('children+is', 1), ('arrow+is', 1), ('providers+is', 1)]) and before ([('in', 92), ('have', 48), ('know', 42), ('thing', 31), ('have', 28)]) sets, we got the most information (which holds true for most words we found) as opposed to after ([('.', 917), ('is', 179), ('is', 50), ('can', 49), ('is', 40)]), modifiers ([('thing+is+this', 5), ('number', 3), ('came+at+da', 2), ('message+with', 1), ('girl+with', 1)]), and subjects ([('thing+this', 44), ('problem+is', 19), ('help+is', 18), ('person+this', 17)]).\n",
    "It is important to note that the object and before sets are not mutually exclusive due to the subject object verb order of Korean which means that there might be some objects in the before set too. One of the cons of the object set is that it doesn't generalize to intransitive verbs but the before set does (along with containing some objects). However the issue with the before set is its inherent noise. Since it is able to capture a lot of different types of words, you can't generalize a meaning out of what it captures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
