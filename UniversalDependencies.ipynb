{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "569de287-e7cd-4c06-9c75-859442428920",
   "metadata": {},
   "source": [
    "# Working with Universal Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8bcf2f5-0b05-4a70-9a96-06346284ce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you don't have conllu yet, uncomment the following\n",
    "# !python3 -m pip install conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7ae05adf-0fe1-4d76-ba45-100c4ae55bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu # reading Universal Dependency files in the CONLLu format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5e103158-0fd5-4666-9713-ade344abd881",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ko_kaist-ud-train.conllu\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "13236d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 23010 sentences\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def parse_conllu():\n",
    "    sentences = []\n",
    "    with open(\"ko_kaist-ud-train.conllu\", encoding='utf-8') as fh:\n",
    "        sent_lines = []\n",
    "        for raw in fh:\n",
    "            line = raw.rstrip('\\n')\n",
    "            if line.startswith('#'):\n",
    "                sent_lines.append(line)\n",
    "            elif line.strip() == '':\n",
    "                if sent_lines:\n",
    "                    sentences.append(sent_lines)\n",
    "                    sent_lines = []\n",
    "            else:\n",
    "                sent_lines.append(line)\n",
    "        if sent_lines:\n",
    "            sentences.append(sent_lines)\n",
    "\n",
    "    parsed = []\n",
    "    for s in sentences:\n",
    "        tokens = []\n",
    "        for ln in s:\n",
    "            if ln.startswith('#'):\n",
    "                continue\n",
    "            parts = ln.split('\\t')\n",
    "            if len(parts) != 10:\n",
    "                continue\n",
    "            id_, form, lemma, upos, xpos, feats, head, deprel, deps, misc = parts\n",
    "            if '-' in id_:\n",
    "                continue\n",
    "            try:\n",
    "                id_int = int(id_)\n",
    "            except Exception:\n",
    "                continue\n",
    "            token = {\n",
    "                'id': id_int,\n",
    "                'form': form,\n",
    "                'lemma': lemma,\n",
    "                'upos': upos,\n",
    "                'xpos': xpos,\n",
    "                'feats': feats,\n",
    "                'head': int(head) if head != '_' else None,\n",
    "                'deprel': deprel,\n",
    "                'deps': deps,\n",
    "                'misc': misc\n",
    "            }\n",
    "            tokens.append(token)\n",
    "        if tokens:\n",
    "            tokens.sort(key=lambda t: t['id'])\n",
    "            parsed.append(tokens)\n",
    "    return parsed\n",
    "\n",
    "sents = parse_conllu()\n",
    "print(f'Parsed {len(sents)} sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9bb54b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb instances (VERB tokens): 55805\n",
      "Total obj instances: 13912\n",
      "Objects immediately before verb: 10066 (72.35%)\n",
      "Objects within 1 tokens before verb: 10066 (72.35%)\n",
      "Saved distance distribution to data/project_outputs/object_verb_distance_distribution.csv\n",
      "Saved POS word count stats to data/project_outputs/pos_wordcount_stats.csv\n",
      "           mean  median  max  min  count\n",
      "CCONJ  1.993268     2.0    8    1  16192\n",
      "ADV    1.919793     2.0    7    1  41555\n",
      "SCONJ  2.394573     2.0    7    1  14925\n",
      "DET    1.000000     1.0    1    1   4286\n",
      "NOUN   1.887334     2.0    8    1  88820\n",
      "VERB   2.835391     3.0    7    2  55805\n",
      "ADJ    1.961284     2.0    5    1  12088\n",
      "PUNCT  1.000000     1.0    1    1  33005\n",
      "AUX    1.000000     1.0    1    1   6901\n",
      "PRON   1.982317     2.0    5    1   6786\n",
      "PROPN  1.638292     2.0    5    1  10232\n",
      "NUM    1.460066     1.0    5    1   3919\n",
      "INTJ   1.145833     1.0    3    1     48\n",
      "PART   2.970085     3.0    6    1    234\n",
      "X      1.005495     1.0    2    1    546\n",
      "ADP    1.032226     1.0    2    1    993\n",
      "SYM    1.072072     1.0    2    1    111\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "OUT_DIR = 'data/project_outputs'\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "verb_instances = 0\n",
    "obj_instances = 0\n",
    "obj_immediate = 0\n",
    "obj_within3 = 0\n",
    "dist_counter = Counter()\n",
    "\n",
    "examples = defaultdict(list) \n",
    "\n",
    "for sent_idx, tokens in enumerate(sents):\n",
    "    id_to_tok = {t['id']: t for t in tokens}\n",
    "    dependents = defaultdict(list)\n",
    "    for t in tokens:\n",
    "        if t['head'] is not None and t['head'] in id_to_tok:\n",
    "            dependents[t['head']].append(t)\n",
    "    id_to_index = {t['id']: i for i, t in enumerate(tokens)}\n",
    "\n",
    "    for i, tok in enumerate(tokens):\n",
    "        if tok['upos'] == 'VERB':\n",
    "            verb_instances += 1\n",
    "            verb_id = tok['id']\n",
    "            deps = dependents.get(verb_id, [])\n",
    "            for dep in deps:\n",
    "                if dep['deprel'] == 'obj':\n",
    "                    obj_instances += 1\n",
    "                    obj_idx = id_to_index.get(dep['id'])\n",
    "                    verb_idx = i\n",
    "                    if obj_idx is None:\n",
    "                        continue\n",
    "                    diff = verb_idx - obj_idx\n",
    "                    dist_counter[diff] += 1\n",
    "                    if diff == 1:\n",
    "                        obj_immediate += 1\n",
    "                    if 0 <= diff <= 1:\n",
    "                        obj_within3 += 1\n",
    "                    lemma = tok['lemma'] if tok['lemma'] != '_' else tok['form']\n",
    "                    if len(examples[lemma]) < 5:\n",
    "                        sent_form = ' '.join([t['form'] for t in tokens])\n",
    "                        examples[lemma].append((dep['form'], tok['form'], sent_form))\n",
    "\n",
    "if obj_instances > 0:\n",
    "    pct_immediate = obj_immediate / obj_instances * 100\n",
    "    pct_within3 = obj_within3 / obj_instances * 100\n",
    "else:\n",
    "    pct_immediate = pct_within3 = 0.0\n",
    "\n",
    "print('Verb instances (VERB tokens):', verb_instances)\n",
    "print('Total obj instances:', obj_instances)\n",
    "print(f'Objects immediately before verb: {obj_immediate} ({pct_immediate:.2f}%)')\n",
    "print(f'Objects within 1 tokens before verb: {obj_within3} ({pct_within3:.2f}%)')\n",
    "\n",
    "dist_items = sorted(dist_counter.items())\n",
    "df_dist = pd.DataFrame(dist_items, columns=['verb_minus_obj_index', 'count'])\n",
    "df_dist.to_csv(os.path.join(OUT_DIR, 'object_verb_distance_distribution.csv'), index=False)\n",
    "print('Saved distance distribution to', os.path.join(OUT_DIR, 'object_verb_distance_distribution.csv'))\n",
    "\n",
    "\n",
    "# parse words within words\n",
    "def num_words_in_word(word):\n",
    "    length = len(word.split('+'))\n",
    "    return length\n",
    "\n",
    "# analyze how many words within words by part of speech\n",
    "pos_wordcount = defaultdict(list)\n",
    "for tokens in sents:\n",
    "    for t in tokens:\n",
    "        pos = t['upos']\n",
    "        word = t['lemma']\n",
    "        count = num_words_in_word(word)\n",
    "        pos_wordcount[pos].append(count)\n",
    "import numpy as np\n",
    "pos_wordcount_stats = {}\n",
    "for pos, counts in pos_wordcount.items():\n",
    "    arr = np.array(counts)\n",
    "    pos_wordcount_stats[pos] = {\n",
    "        'mean': np.mean(arr),\n",
    "        'median': np.median(arr),\n",
    "        'max': np.max(arr),\n",
    "        'min': np.min(arr),\n",
    "        'count': len(arr)\n",
    "    }\n",
    "df_wordcount = pd.DataFrame.from_dict(pos_wordcount_stats, orient='index')\n",
    "df_wordcount.to_csv(os.path.join(OUT_DIR, 'pos_wordcount_stats.csv'))\n",
    "print('Saved POS word count stats to', os.path.join(OUT_DIR, 'pos_wordcount_stats.csv'))\n",
    "print(df_wordcount)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e07669f",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "### Task 1.1\n",
    "\n",
    "### Task 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bb6c24",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "### Top 20 Verbs\n",
    "\n",
    "### High Frequency Verbs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
