{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "569de287-e7cd-4c06-9c75-859442428920",
   "metadata": {},
   "source": [
    "# Working with Universal Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8bcf2f5-0b05-4a70-9a96-06346284ce58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: conllu in /opt/anaconda3/lib/python3.13/site-packages (6.0.0)\n"
     ]
    }
   ],
   "source": [
    "# if you don't have conllu yet, uncomment the following\n",
    "!python -m pip install conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ae05adf-0fe1-4d76-ba45-100c4ae55bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu # reading Universal Dependency files in the CONLLu format\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e103158-0fd5-4666-9713-ade344abd881",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ko_kaist-ud-train.conllu\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005796a9",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9bb54b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb instances (VERB tokens): 55805\n",
      "Total obj instances: 13912\n",
      "Objects immediately before verb: 10066 (72.35%)\n",
      "Objects within 3 before verb: 12937 (92.99%)\n",
      "Objects after verb: 0 (0.00%)\n",
      "Objects before verb (any distance): 13912 (100.00%)\n",
      "Saved distance distribution to data/project_outputs/object_verb_distance_distribution.csv\n"
     ]
    }
   ],
   "source": [
    "OUT_DIR = 'data/project_outputs'\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def parse_conllu(path):\n",
    "    \"\"\"\n",
    "    Return list of sentences; each sentence is a list of token dicts:\n",
    "    {id:int, form:str, lemma:str, upos:str, head:int or None, deprel:str}\n",
    "    Ignores multiword lines (1-2) and empty-node decimal ids (3.1).\n",
    "    \"\"\"\n",
    "    sents = []\n",
    "    tokens = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    sents.append(tokens)\n",
    "                    tokens = []\n",
    "                continue\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            cols = line.split(\"\\t\")\n",
    "            if len(cols) != 10:\n",
    "                continue\n",
    "            id_field = cols[0]\n",
    "            # skip multiword / empty nodes\n",
    "            if \"-\" in id_field or \".\" in id_field:\n",
    "                continue\n",
    "            try:\n",
    "                tid = int(id_field)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            form = cols[1]\n",
    "            lemma = cols[2]\n",
    "            upos = cols[3]\n",
    "            head = cols[6]\n",
    "            deprel = cols[7]\n",
    "            try:\n",
    "                head_int = int(head) if head != \"_\" else None\n",
    "            except ValueError:\n",
    "                head_int = None\n",
    "            tok = {\"id\": tid, \"form\": form, \"lemma\": lemma, \"upos\": upos, \"head\": head_int, \"deprel\": deprel}\n",
    "            tokens.append(tok)\n",
    "    # final\n",
    "    if tokens:\n",
    "        sents.append(tokens)\n",
    "    return sents\n",
    "\n",
    "sents = parse_conllu(\"ko_kaist-ud-train.conllu\")\n",
    "\n",
    "verb_instances = 0\n",
    "obj_instances = 0\n",
    "obj_immediate = 0\n",
    "obj_within3 = 0\n",
    "obj_to_right = 0\n",
    "obj_any_before = 0\n",
    "dist_counter = Counter()\n",
    "\n",
    "examples = defaultdict(list) \n",
    "\n",
    "for sent_idx, tokens in enumerate(sents):\n",
    "    id_to_tok = {t['id']: t for t in tokens}\n",
    "    dependents = defaultdict(list)\n",
    "    for t in tokens:\n",
    "        if t['head'] is not None and t['head'] in id_to_tok:\n",
    "            dependents[t['head']].append(t)\n",
    "    id_to_index = {t['id']: i for i, t in enumerate(tokens)}\n",
    "\n",
    "    for i, tok in enumerate(tokens):\n",
    "        if tok['upos'] == 'VERB':\n",
    "            verb_instances += 1\n",
    "            verb_id = tok['id']\n",
    "            deps = dependents.get(verb_id, [])\n",
    "            for dep in deps:\n",
    "                if dep['deprel'] == 'obj':\n",
    "                    obj_instances += 1\n",
    "                    obj_idx = id_to_index.get(dep['id'])\n",
    "                    verb_idx = i\n",
    "                    if obj_idx is None:\n",
    "                        continue\n",
    "                    diff = verb_idx - obj_idx  # positive if object is to verb's left\n",
    "                    dist_counter[diff] += 1\n",
    "                    if diff == 1:\n",
    "                        obj_immediate += 1\n",
    "                    if 1 <= diff <= 3:\n",
    "                        obj_within3 += 1\n",
    "                    if diff < 0:\n",
    "                        obj_to_right += 1\n",
    "                    if diff > 0:\n",
    "                        obj_any_before += 1\n",
    "\n",
    "                    \n",
    "                    lemma = tok['lemma'] if tok['lemma'] != '_' else tok['form']\n",
    "                    if len(examples[lemma]) < 5:\n",
    "                        sent_form = ' '.join([t['form'] for t in tokens])\n",
    "                        examples[lemma].append((dep['form'], tok['form'], sent_form))\n",
    "\n",
    "if obj_instances > 0:\n",
    "    pct_immediate = obj_immediate / obj_instances * 100\n",
    "    pct_within3 = obj_within3 / obj_instances * 100\n",
    "    pct_after = obj_to_right / obj_instances * 100\n",
    "else:\n",
    "    pct_immediate = pct_within3 = 0.0\n",
    "\n",
    "print('Verb instances (VERB tokens):', verb_instances)\n",
    "print('Total obj instances:', obj_instances)\n",
    "print(f'Objects immediately before verb: {obj_immediate} ({pct_immediate:.2f}%)')\n",
    "print(f'Objects within 3 before verb: {obj_within3} ({pct_within3:.2f}%)')\n",
    "print(f'Objects after verb: {obj_to_right} ({pct_after:.2f}%)')\n",
    "print(f'Objects before verb (any distance): {obj_any_before} ({(obj_any_before / obj_instances * 100) if obj_instances > 0 else 0.0:.2f}%)')\n",
    "\n",
    "dist_items = sorted(dist_counter.items())\n",
    "df_dist = pd.DataFrame(dist_items, columns=['verb_minus_obj_index', 'count'])\n",
    "df_dist.to_csv(os.path.join(OUT_DIR, 'object_verb_distance_distribution.csv'), index=False)\n",
    "print('Saved distance distribution to', os.path.join(OUT_DIR, 'object_verb_distance_distribution.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c453e1",
   "metadata": {},
   "source": [
    "### Task 1.1\n",
    "The generalization we can see from the above results is that an object _never_ occurs after a verb, it _must_ occur before the verb.\n",
    "\n",
    "### Task 1.2\n",
    "Our generalization generally holds true. While in our corupus, an object _never_ follows a verb, it does not always have to come immediately before a verb (it does at a 72.35% rate). We have found that the object comes within 3 tokens before the verb, though this is also not always true (it occurs 92.99% of the time). The exceptions to this rule occur when objects are farther than 3 tokens from the verb, but still are before the verb, since we could not find any case where it follows a verb (at any length)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df06b09c",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94afcdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 23010 sentences from ko_kaist-ud-train.conllu\n",
      "Top verbs (sample): ['이', '하', '있', '되', '않']\n",
      "Mid verbs (sample): ['물리치', '민감', '내려오', '제창', '갈']\n",
      "Attempting to load vector model...\n",
      "Inspecting text vector file ko.tsv ...\n",
      "No headers so parsing text file line-by-line and building KeyedVectors (this may take time on your computers).\n",
      "Constructed KeyedVectors from text file with 603232 words and dim 5.\n",
      "Computing centroids and nearest neighbors...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "CONLLU_PATH = \"ko_kaist-ud-train.conllu\"   #Just a reminder, all files are directly just in the same folder as this code.\n",
    "OUT_DIR = \"data/project_outputs_task2\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "VERB_UPOS = (\"VERB\", \"AUX\")   # token UPOS considered verbs\n",
    "COPULA_CANONICAL = \"이\"       # canonical token for copula (이)\n",
    "\n",
    "def normalize_korean_verb(lemma_or_form):\n",
    "    \"\"\"\n",
    "        Normalize a Korean verb token (lemma or form) to a canonical representation.\n",
    "        Rules:\n",
    "        - If segmented with '+', take first morpheme unless copula '이' is present, in which case return '이'\n",
    "        - If unsegmented and ends with '다', strip terminal '다'\n",
    "        - Otherwise return as is\n",
    "    \"\"\"\n",
    "    if not lemma_or_form or lemma_or_form == \"_\":\n",
    "        return None\n",
    "\n",
    "    \n",
    "    s = lemma_or_form.replace(\"＋\", \"+\").replace(\"‧\", \"+\").strip()\n",
    "\n",
    "    #if segmented with '+', split into morphemes\n",
    "    if \"+\" in s:\n",
    "        parts = [p for p in s.split(\"+\") if p]  # drop empty parts\n",
    "        #if any part equals the copula morpheme '이', treat as copula\n",
    "        if any(p == \"이\" for p in parts):\n",
    "            return COPULA_CANONICAL\n",
    "        return parts[0]\n",
    "\n",
    "    #if unsegmented but ends with '다', strip terminal '다' \n",
    "    #this is a common verb infinitive ending in Korean, which we want to normalize away\n",
    "    if len(s) > 1 and s.endswith(\"다\"):\n",
    "        return s[:-1]\n",
    "\n",
    "    return s\n",
    "\n",
    "def aggregate_normalized_counts(counter, locations):\n",
    "    \"\"\"\n",
    "        Ensures counts/locations are collapsed to normalized keys.\n",
    "        Returns new Counter and locations dict.\n",
    "    \"\"\"\n",
    "    norm_counter = Counter()\n",
    "    norm_locations = defaultdict(list)\n",
    "    if locations:\n",
    "        for raw_key, locs in locations.items():\n",
    "            norm = normalize_korean_verb(raw_key)\n",
    "            norm_counter[norm] += len(locs)\n",
    "            norm_locations[norm].extend(locs)\n",
    "    else:\n",
    "        for raw_key, cnt in counter.items():\n",
    "            norm = normalize_korean_verb(raw_key)\n",
    "            norm_counter[norm] += cnt\n",
    "    return norm_counter, norm_locations\n",
    "\n",
    "\n",
    "\n",
    "def verb_frequencies(sents, verb_upos=VERB_UPOS):\n",
    "    \"\"\"\n",
    "        Count frequencies and record locations of verbs in sentences.\n",
    "        Returns a Counter of normalized verbs and a dict of locations.\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    locations = defaultdict(list)  # canonical_verb -> list of (sent_idx, token_idx, original_token)\n",
    "    # iterate over sentences and tokens\n",
    "    for si, tokens in enumerate(sents):\n",
    "        for i, tok in enumerate(tokens):\n",
    "            if tok[\"upos\"] in verb_upos:\n",
    "                lemma = tok[\"lemma\"] if tok[\"lemma\"] != \"_\" else tok[\"form\"]\n",
    "                norm = normalize_korean_verb(lemma)\n",
    "                counter[norm] += 1\n",
    "                locations[norm].append((si, i, tok))\n",
    "    return counter, locations\n",
    "\n",
    "\n",
    "def select_verbs_by_quantiles(counter, top_pct=0.20, next_pct=0.20, top_k_each=20):\n",
    "    \"\"\"\n",
    "        Select verbs by frequency quantiles.\n",
    "        Returns two lists: top verbs and next verbs.\n",
    "    \"\"\"\n",
    "    items = counter.most_common()\n",
    "    types = [v for v, _ in items]\n",
    "    n_types = len(types)\n",
    "    top_n_types = max(1, int(n_types * top_pct))\n",
    "    next_n_types = max(1, int(n_types * next_pct))\n",
    "    top_type_set = set(types[:top_n_types])\n",
    "    next_type_set = set(types[top_n_types: top_n_types + next_n_types])\n",
    "    top_candidates = [v for v, _ in items if v in top_type_set]\n",
    "    next_candidates = [v for v, _ in items if v in next_type_set]\n",
    "    return top_candidates[:top_k_each], next_candidates[:top_k_each]\n",
    "\n",
    "\n",
    "def extract_verb_sets(sents, verb_locations, verbs, \n",
    "                      subj_deprels=(\"nsubj\", \"nsubj:pass\", \"csubj\"), \n",
    "                      obj_deprels_prefix=(\"obj\",), \n",
    "                      modifier_deprels_prefixes=(\"advmod\", \"amod\", \"nmod\", \"obl\", \"advcl\", \"compound\")):\n",
    "    \"\"\"\n",
    "        Extract sets of subjects, objects, modifiers, and surrounding words for given verbs.\n",
    "        Returns a dict: verb -> {subject:Counter, object:Counter, modifier:Counter, before:Counter, after:Counter, occurrences:int}\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for verb in verbs:\n",
    "        subj_c = Counter()\n",
    "        obj_c = Counter()\n",
    "        mod_c = Counter()\n",
    "        before_c = Counter()\n",
    "        after_c = Counter()\n",
    "        occ = 0\n",
    "        locs = verb_locations.get(verb, [])\n",
    "        for si, vi, original_tok in locs:\n",
    "            sent = sents[si]\n",
    "            if vi < 0 or vi >= len(sent):\n",
    "                continue\n",
    "            occ += 1\n",
    "            id_to_tok = {t['id']: t for t in sent}\n",
    "            dependents = defaultdict(list)\n",
    "            for t in sent:\n",
    "                h = t['head']\n",
    "                if h is not None and h in id_to_tok:\n",
    "                    dependents[h].append(t)\n",
    "            v_deps = dependents.get(original_tok['id'], [])\n",
    "            for dep in v_deps:\n",
    "                deprel = dep['deprel']\n",
    "                # NOTE: probably redundant to check startswith for nsubj since in subj_deprels already \n",
    "                if deprel in subj_deprels or deprel.startswith(\"nsubj\"):\n",
    "                    head_form = dep['lemma'] if dep['lemma'] != \"_\" else dep['form']\n",
    "                    subj_c[head_form] += 1\n",
    "                if any(deprel == p or deprel.startswith(p) for p in obj_deprels_prefix):\n",
    "                    head_form = dep['lemma'] if dep['lemma'] != \"_\" else dep['form']\n",
    "                    obj_c[head_form] += 1\n",
    "                if any(deprel == p or deprel.startswith(p) for p in modifier_deprels_prefixes):\n",
    "                    head_form = dep['lemma'] if dep['lemma'] != \"_\" else dep['form']\n",
    "                    mod_c[head_form] += 1\n",
    "            if vi - 1 >= 0:\n",
    "                before_c[sent[vi-1]['form']] += 1\n",
    "            if vi + 1 < len(sent):\n",
    "                after_c[sent[vi+1]['form']] += 1\n",
    "        results[verb] = {\n",
    "            \"subject\": subj_c,\n",
    "            \"object\": obj_c,\n",
    "            \"modifier\": mod_c,\n",
    "            \"before\": before_c,\n",
    "            \"after\": after_c,\n",
    "            \"occurrences\": occ\n",
    "        }\n",
    "    return results\n",
    "\n",
    "def load_korean_vector_model(bin_path=None, tsv_path=None, verbose=True):\n",
    "    \"\"\"\n",
    "        Load a Korean word vector model from binary or TSV format.\n",
    "        Handles Kyubyong multi-line bracketed format (index<TAB>word<TAB>[ v v v ... across lines ... ])\n",
    "        Returns a KeyedVectors instance.\n",
    "    \"\"\"\n",
    "    attempts = []\n",
    "    \n",
    "    def _log(msg):\n",
    "        if verbose:\n",
    "            print(msg)\n",
    "\n",
    "    # If a TSV (text) file is given and exists, attempt to parse.\n",
    "    if tsv_path and os.path.exists(tsv_path):\n",
    "        try:\n",
    "            _log(f\"Inspecting text vector file {tsv_path} ...\")\n",
    "            # We'll parse Kyubyong style files where each entry starts like:\n",
    "            #    0\\t하\\t[ 1.2513299  -0.79136038 ... \n",
    "            # then continues across multiple lines until a closing ']' is found.\n",
    "            words = []\n",
    "            vecs = []\n",
    "            dim = None\n",
    "            with open(tsv_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as fh:\n",
    "                line_num = 0\n",
    "                collecting = False\n",
    "                current_word = None\n",
    "                current_vals = []\n",
    "                open_bracket_re = re.compile(r'^\\s*\\d+\\t([^\\t]+)\\t\\[')\n",
    "                # Pattern when start line doesn't include '[' but still provides word? less common.\n",
    "                for raw_line in fh:\n",
    "                    line_num += 1\n",
    "                    line = raw_line.rstrip(\"\\n\")\n",
    "                    if not line.strip():\n",
    "                        continue\n",
    "                    # If we are not currently collecting, check for start of a new entry\n",
    "                    if not collecting:\n",
    "                        m = open_bracket_re.match(line)\n",
    "                        if m:\n",
    "                            # start collecting numeric tokens from this line (after '[')\n",
    "                            current_word = m.group(1)\n",
    "                            # extract everything after the '[' in this line\n",
    "                            idx = line.find('[')\n",
    "                            after = line[idx+1:].strip()\n",
    "                            if after:\n",
    "                                # split numeric tokens and keep them\n",
    "                                parts = re.split(r'\\s+', after)\n",
    "                                for p in parts:\n",
    "                                    if p == ']' or p == '':\n",
    "                                        continue\n",
    "                                    # strip any trailing commas/brackets\n",
    "                                    p_clean = p.strip(\"[],\")\n",
    "                                    if p_clean:\n",
    "                                        current_vals.append(p_clean)\n",
    "                            collecting = True\n",
    "                        else:\n",
    "                            # Some variants might put the 'index<tab>word<tab>' on its own line then '[' on next line.\n",
    "                            # handle: try to see 'index\\tword\\t' pattern without '['\n",
    "                            parts0 = line.split(\"\\t\")\n",
    "                            if len(parts0) >= 2 and parts0[0].isdigit():\n",
    "                                # next lines likely begin with '['; treat as start but wait for '[' in later lines\n",
    "                                current_word = parts0[1]\n",
    "                                current_vals = []\n",
    "                                collecting = True\n",
    "                                # try to find '[' on same line\n",
    "                                if '[' in line:\n",
    "                                    idx = line.find('[')\n",
    "                                    after = line[idx+1:].strip()\n",
    "                                    if after:\n",
    "                                        parts = re.split(r'\\s+', after)\n",
    "                                        for p in parts:\n",
    "                                            p_clean = p.strip(\"[],\")\n",
    "                                            if p_clean:\n",
    "                                                current_vals.append(p_clean)\n",
    "                                # continue to next loop where we'll collect remaining numeric lines\n",
    "                            else:\n",
    "                                # Not a start line; skip\n",
    "                                continue\n",
    "                    else:\n",
    "                        # we are currently collecting numeric tokens for current_word\n",
    "                        # check if this line contains ']' closing the vector\n",
    "                        if ']' in line:\n",
    "                            # take everything before ']' on this line\n",
    "                            before = line.split(']')[0].strip()\n",
    "                            if before:\n",
    "                                parts = re.split(r'\\s+', before)\n",
    "                                for p in parts:\n",
    "                                    p_clean = p.strip(\"[],\")\n",
    "                                    if p_clean:\n",
    "                                        current_vals.append(p_clean)\n",
    "                            # now finish this vector\n",
    "                            try:\n",
    "                                vec = np.array([float(x) for x in current_vals], dtype=np.float32)\n",
    "                            except Exception as e:\n",
    "                                # if conversion fails, skip this entry but continue parsing\n",
    "                                _log(f\"Warning: failed to convert vector for word {current_word} at line {line_num}: {e}\")\n",
    "                                current_word = None\n",
    "                                current_vals = []\n",
    "                                collecting = False\n",
    "                                continue\n",
    "                            if dim is None:\n",
    "                                dim = vec.shape[0]\n",
    "                            else:\n",
    "                                if vec.shape[0] != dim:\n",
    "                                    _log(f\"Warning: inconsistent dim for word {current_word}: {vec.shape[0]} vs {dim}; skipping\")\n",
    "                                    current_word = None\n",
    "                                    current_vals = []\n",
    "                                    collecting = False\n",
    "                                    continue\n",
    "                            words.append(current_word)\n",
    "                            vecs.append(vec)\n",
    "                            # reset for next entry\n",
    "                            current_word = None\n",
    "                            current_vals = []\n",
    "                            collecting = False\n",
    "                        else:\n",
    "                            # no closing bracket yet; line should be numeric tokens\n",
    "                            parts = re.split(r'\\s+', line.strip())\n",
    "                            for p in parts:\n",
    "                                p_clean = p.strip(\"[],\")\n",
    "                                if p_clean:\n",
    "                                    current_vals.append(p_clean)\n",
    "                # End file loop\n",
    "            if not words:\n",
    "                raise RuntimeError(\"Parsed zero word vectors from the TSV file.\")\n",
    "            arr = np.vstack(vecs)\n",
    "            kv = KeyedVectors(vector_size=arr.shape[1])\n",
    "            kv.add_vectors(words, arr)\n",
    "            _log(f\"Constructed KeyedVectors from text file with {len(words)} words and dim {arr.shape[1]}.\")\n",
    "            # a small diagnostic: show first few keys\n",
    "            # _log(\"Sample model keys: \" + \", \".join(words[:20]))\n",
    "            return kv\n",
    "        except Exception as e:\n",
    "            attempts.append((\"w2v_text_kyubyong\", str(e)))\n",
    "\n",
    "    # If no TSV path or parsing failed, raise an informative error:\n",
    "    msg = \"Failed to load model via TSV Kyubyong parser. Attempts:\\n\" + \"\\n\".join(f\"{k}: {v}\" for k, v in attempts)\n",
    "    raise RuntimeError(msg)\n",
    "\n",
    "\n",
    "# -------------- NEW: KAIST -> surface mapping + robust centroid lookup --------------\n",
    "COMMON_PARTICLES = {\n",
    "    \"이\",\"가\",\"을\",\"를\",\"은\",\"는\",\"에\",\"에서\",\"으로\",\"로\",\"와\",\"과\",\"에게\",\"께\",\"부터\",\"까지\",\"만\",\"도\",\"뿐\",\"처럼\",\"까지\",\"보다\"\n",
    "}\n",
    "\n",
    "def kaist_to_surface_candidates(token):\n",
    "    \"\"\"\n",
    "    Given a KAIST-style token like '사람+들+이' or '것+이' or '있+는',\n",
    "    return candidate surface keys to try in the model in order of preference.\n",
    "    \"\"\"\n",
    "    if token is None:\n",
    "        return []\n",
    "    t = str(token).strip()\n",
    "    if not t or t == \"_\":\n",
    "        return []\n",
    "    candidates = []\n",
    "    if \"+\" in t:\n",
    "        # primary: join all parts (remove '+'), e.g. '사람+들+이' -> '사람들이'\n",
    "        joined = t.replace(\"+\", \"\")\n",
    "        candidates.append(joined)\n",
    "        # try joined without trailing '다' if present\n",
    "        if joined.endswith(\"다\") and len(joined) > 1:\n",
    "            candidates.append(joined[:-1])\n",
    "        # also try the first part (stem)\n",
    "        parts = [p for p in t.split(\"+\") if p]\n",
    "        if parts:\n",
    "            candidates.append(parts[0])\n",
    "            # if last part is a particle, try joining all but last (e.g., '것+들+이' -> '것들')\n",
    "            if parts[-1] in COMMON_PARTICLES and len(parts) >= 2:\n",
    "                maybe = \"\".join(parts[:-1])\n",
    "                candidates.append(maybe)\n",
    "    else:\n",
    "        # no plus segmentation: try token as-is first\n",
    "        candidates.append(t)\n",
    "        # if ends with '다', also add stem\n",
    "        if len(t) > 1 and t.endswith(\"다\"):\n",
    "            candidates.append(t[:-1])\n",
    "    # deduplicate preserving order\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for c in candidates:\n",
    "        if c and c not in seen:\n",
    "            seen.add(c)\n",
    "            out.append(c)\n",
    "    return out\n",
    "\n",
    "def centroid_of_words(model, words):\n",
    "    \"\"\"\n",
    "    Map KAIST tokens to model keys and compute centroid.\n",
    "    Returns (centroid_vector or None, n_in_vocab, missed_list)\n",
    "    \"\"\"\n",
    "    vecs = []\n",
    "    missed = []\n",
    "    for w in words:\n",
    "        if w is None:\n",
    "            continue\n",
    "        # if items are tuples like (word, count), handle that\n",
    "        if isinstance(w, (tuple, list)):\n",
    "            keyword = w[0]\n",
    "        else:\n",
    "            keyword = str(w)\n",
    "        candidates = kaist_to_surface_candidates(keyword)\n",
    "        found_key = None\n",
    "        for cand in candidates:\n",
    "            if cand in model:\n",
    "                found_key = cand\n",
    "                break\n",
    "        # final fallback: try the raw keyword itself\n",
    "        if found_key is None and keyword in model:\n",
    "            found_key = keyword\n",
    "        if found_key is not None:\n",
    "            vecs.append(model[found_key])\n",
    "        else:\n",
    "            missed.append(keyword)\n",
    "    if not vecs:\n",
    "        return None, 0, missed\n",
    "    arr = np.vstack(vecs)\n",
    "    return np.mean(arr, axis=0), arr.shape[0], missed\n",
    "\n",
    "def topk_neighbors_from_centroid(model, centroid_vec, k=10):\n",
    "    if centroid_vec is None:\n",
    "        return []\n",
    "    return model.similar_by_vector(centroid_vec, topn=k)\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def build_task2_analysis(conllu_path=CONLLU_PATH, model_bin=None, model_tsv=None, k_neighbors=10, top_k_each=5):\n",
    "    sents = parse_conllu(conllu_path)\n",
    "    print(f\"Loaded {len(sents)} sentences from {conllu_path}\")\n",
    "    raw_counter, raw_locations = verb_frequencies(sents)\n",
    "\n",
    "    verb_counter, verb_locations = aggregate_normalized_counts(raw_counter, raw_locations)\n",
    "\n",
    "    vf_df = pd.DataFrame(verb_counter.most_common(), columns=[\"verb_norm\", \"freq\"])\n",
    "    \n",
    "    vf_df.to_csv(os.path.join(OUT_DIR, \"verb_frequencies_normalized.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "    top_verbs, mid_verbs = select_verbs_by_quantiles(verb_counter, top_pct=0.20, next_pct=0.20, top_k_each=top_k_each)\n",
    "    print(\"Top verbs (sample):\", top_verbs)\n",
    "    print(\"Mid verbs (sample):\", mid_verbs)\n",
    "    verbs_to_analyze = list(top_verbs) + list(mid_verbs)\n",
    "    sets = extract_verb_sets(sents, verb_locations, verbs_to_analyze)\n",
    "    summary = {}\n",
    "    for v in verbs_to_analyze:\n",
    "        entry = sets[v]\n",
    "        summary[v] = {\n",
    "            \"occurrences\": entry[\"occurrences\"],\n",
    "            \"top_subjects\": entry[\"subject\"].most_common(50),\n",
    "            \"top_objects\": entry[\"object\"].most_common(50),\n",
    "            \"top_modifiers\": entry[\"modifier\"].most_common(50),\n",
    "            \"top_before\": entry[\"before\"].most_common(50),\n",
    "            \"top_after\": entry[\"after\"].most_common(50)\n",
    "        }\n",
    "    with open(os.path.join(OUT_DIR, \"verb_sets_summary_normalized.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    model = None\n",
    "    neighbors_summary = {}\n",
    "    if (model_bin and os.path.exists(model_bin)) or (model_tsv and os.path.exists(model_tsv)):\n",
    "        print(\"Attempting to load vector model...\")\n",
    "        model = load_korean_vector_model(bin_path=model_bin, tsv_path=model_tsv)\n",
    "        print(\"Computing centroids and nearest neighbors...\")\n",
    "        for v in verbs_to_analyze:\n",
    "            neighbors_summary[v] = {}\n",
    "            for set_name in (\"top_subjects\", \"top_objects\", \"top_modifiers\", \"top_before\", \"top_after\"):\n",
    "                words = [w for w, cnt in summary[v][set_name][:200]]\n",
    "                centroid, n_in_vocab, missed = centroid_of_words(model, words)\n",
    "                knn = topk_neighbors_from_centroid(model, centroid, k=k_neighbors) if centroid is not None else []\n",
    "                neighbors_summary[v][set_name] = {\n",
    "                    \"centroid_n_in_vocab\": int(n_in_vocab),\n",
    "                    \"missed_count\": len(missed),\n",
    "                    \"missed_examples\": missed[:30],\n",
    "                    \"knn\": [(w, float(sim)) for w, sim in knn]\n",
    "                }\n",
    "        with open(os.path.join(OUT_DIR, \"verb_neighbors_summary_normalized.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(neighbors_summary, f, ensure_ascii=False, indent=2)\n",
    "    else:\n",
    "        print(\"No model files provided or found; skipping centroid/knn stage.\")\n",
    "\n",
    "    return {\n",
    "        \"sentences\": len(sents),\n",
    "        \"verb_freq_df\": vf_df,\n",
    "        \"top_verbs\": top_verbs,\n",
    "        \"mid_verbs\": mid_verbs,\n",
    "        \"sets\": sets,\n",
    "        \"summary\": summary,\n",
    "        \"neighbors_summary\": neighbors_summary if model else None\n",
    "    }\n",
    "\n",
    "\n",
    "res = build_task2_analysis(\n",
    "    conllu_path=\"ko_kaist-ud-train.conllu\",\n",
    "    model_bin=\"ko.bin\",     \n",
    "    model_tsv=\"ko.tsv\",     \n",
    "    k_neighbors=20,\n",
    "    top_k_each=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65b41cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verb_info(verb):\n",
    "    print(f\"Verb: {verb}\")\n",
    "    entry = res['summary'][verb]\n",
    "    print(f\"  Occurrences: {entry['occurrences']}\")\n",
    "    print(f\"  Top Subjects: {entry['top_subjects'][:5]}\")\n",
    "    print(f\"  Top Objects: {entry['top_objects'][:5]}\")\n",
    "    print(f\"  Top Modifiers: {entry['top_modifiers'][:5]}\")\n",
    "    print(f\"  Top Before: {entry['top_before'][:5]}\")\n",
    "    print(f\"  Top After: {entry['top_after'][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9bbeb7",
   "metadata": {},
   "source": [
    "### Task 2.1\n",
    "#### 5 Verbs from Top 20%\n",
    "List of verbs: ['이', '하', '있', '되', '않']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c5a5881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb: 이\n",
      "  Occurrences: 10508\n",
      "  Top Subjects: [('것+이', 213), ('이것+이', 59), ('그것+이', 32), ('자체+가', 10), ('사람+이', 8)]\n",
      "  Top Objects: [('이+를', 4), ('이것+을', 3), ('작품+을', 2), ('정+을', 1), ('세계관+을', 1)]\n",
      "  Top Modifiers: [('것+이+다', 131), ('바로', 120), ('것+도', 64), ('가장', 40), ('때문+이+다', 34)]\n",
      "  Top Before: [('있는', 322), ('있을', 133), (\"'\", 126), ('할', 125), ('될', 115)]\n",
      "  Top After: [('.', 6029), ('것이다', 185), ('?', 150), ('할', 109), ('때문이다', 54)]\n",
      "\n",
      "Verb: 하\n",
      "  Occurrences: 3704\n",
      "  Top Subjects: [('내+가', 7), ('우리+가', 6), ('자기+가', 6), ('그+가', 6), ('그것+이', 3)]\n",
      "  Top Objects: [('역할+을', 45), ('일+을', 26), ('것+을', 22), ('생각+을', 13), ('말+을', 12)]\n",
      "  Top Modifiers: [('수', 269), ('것+이+다', 75), ('필요+로', 24), ('때', 23), ('것+이+ㅂ니다', 19)]\n",
      "  Top Before: [('있다고', 48), ('해야', 47), ('역할을', 43), ('있어야', 42), ('필요로', 24)]\n",
      "  Top After: [('.', 1546), ('수', 271), ('것이다', 163), ('것은', 62), (',', 58)]\n",
      "\n",
      "Verb: 있\n",
      "  Occurrences: 3468\n",
      "  Top Subjects: [('문제+가', 4), ('것+이', 4), ('관계+가', 4), ('수+가', 3), ('필요+가', 2)]\n",
      "  Top Objects: []\n",
      "  Top Modifiers: [('수', 23), ('또한', 3), ('데+에', 3), ('데', 3), ('점+에서', 2)]\n",
      "  Top Before: [('가지고', 122), ('알고', 87), ('지니고', 77), ('하고', 71), ('되어', 64)]\n",
      "  Top After: [('.', 1606), ('것이다', 122), (',', 117), ('때문이다', 35), ('것을', 34)]\n",
      "\n",
      "Verb: 되\n",
      "  Occurrences: 2017\n",
      "  Top Subjects: [('것+이', 44), ('문제+가', 19), ('도움+이', 18), ('사람+이', 17), ('대상+이', 16)]\n",
      "  Top Objects: [('아이+들+을', 1), ('화살표+를', 1), ('제공자+들+을', 1)]\n",
      "  Top Modifiers: [('안', 91), ('것+이+다', 70), ('수', 49), ('잘', 17), ('것+이+ㅂ니다', 17)]\n",
      "  Top Before: [('안', 92), ('있게', 48), ('알게', 42), ('것이', 31), ('갖게', 28)]\n",
      "  Top After: [('.', 917), ('것이다', 179), ('것은', 50), ('수', 49), ('있다', 40)]\n",
      "\n",
      "Verb: 않\n",
      "  Occurrences: 1685\n",
      "  Top Subjects: [('마음+이', 1)]\n",
      "  Top Objects: []\n",
      "  Top Modifiers: [('때+마다', 1), ('편치', 1)]\n",
      "  Top Before: [('하지', 60), ('있지', 58), ('지나지', 51), ('존재하지', 37), ('되지', 37)]\n",
      "  Top After: [('.', 525), ('수', 64), (',', 62), ('것이다', 46), ('안', 36)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for v in res['top_verbs']:\n",
    "    verb_info(v)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4dae72",
   "metadata": {},
   "source": [
    "### Task 2.2\n",
    "#### 5 Verbs from Top 20-40%\n",
    "List of verbs: ['물리치', '민감', '내려오', '제창', '갈']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc7cee6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb: 물리치\n",
      "  Occurrences: 8\n",
      "  Top Subjects: []\n",
      "  Top Objects: [('이민족+을', 1), ('도깨비+를', 1), ('기운+을', 1), ('왜구+를', 1), ('재도전+을', 1)]\n",
      "  Top Modifiers: [('수', 2), ('그걸+로', 1), ('의심+도', 1)]\n",
      "  Top Before: [('이민족을', 1), ('도깨비를', 1), ('기운을', 1), ('왜구를', 1), ('재도전을', 1)]\n",
      "  Top After: [('수', 2), ('존왕양이의', 1), ('위해', 1), ('영약으로', 1), ('결심을', 1)]\n",
      "\n",
      "Verb: 민감\n",
      "  Occurrences: 7\n",
      "  Top Subjects: [('동물+들+이', 1)]\n",
      "  Top Objects: []\n",
      "  Top Modifiers: [('더', 2), ('측면+에', 1), ('여론+에', 1), ('것+에', 1), ('특성+상', 1)]\n",
      "  Top Before: [('더', 2), ('여론에', 1), ('정책에', 1), ('것에', 1), ('덜', 1)]\n",
      "  Top After: [('.', 2), ('인사라고나', 1), ('반응을', 1), ('법', 1), ('부분의', 1)]\n",
      "\n",
      "Verb: 내려오\n",
      "  Occurrences: 7\n",
      "  Top Subjects: []\n",
      "  Top Objects: []\n",
      "  Top Modifiers: [('예+로+부터', 1), ('고장+에', 1), ('가장자리+까지', 1), ('옛+부터', 1), ('조상+도', 1)]\n",
      "  Top Before: [('전해', 2), ('가장자리까지', 1), ('옛부터', 1), ('묻고', 1), ('하늘에서', 1)]\n",
      "  Top After: [('.', 2), ('좋은', 1), ('한다', 1), ('한', 1), ('날', 1)]\n",
      "\n",
      "Verb: 제창\n",
      "  Occurrences: 7\n",
      "  Top Subjects: [('사람+이', 1), ('루스벨트+가', 1), ('그+가', 1)]\n",
      "  Top Objects: [('것+을', 1), ('국채+보상+운동+을', 1), ('뉴딜정책+을', 1), ('민주주의+를', 1)]\n",
      "  Top Modifiers: [('이후', 1), ('1930+년대', 1), ('구제책+으로서', 1), ('중반+부터', 1), ('중심+으로', 1)]\n",
      "  Top Before: [('것을', 1), ('국채보상운동을', 1), ('위해서', 1), ('사람이', 1), ('뉴딜정책을', 1)]\n",
      "  Top After: [('.', 3), ('서상돈', 1), ('용어로', 1), ('하는', 1), ('왔다', 1)]\n",
      "\n",
      "Verb: 갈\n",
      "  Occurrences: 7\n",
      "  Top Subjects: []\n",
      "  Top Objects: [('부리+를', 2), ('콩+을', 1), ('넘+어', 1), ('땅+을', 1)]\n",
      "  Top Modifiers: [('믹서+에', 2), ('강판+에', 1), ('언제', 1), ('되+ㄴ', 1), ('틈틈이', 1)]\n",
      "  Top Before: [('믹서에', 2), ('강판에', 1), ('언제', 1), ('땅을', 1), ('날카롭게', 1)]\n",
      "  Top After: [('당근을', 1), ('때', 1), ('콩물을', 1), ('하는가', 1), ('된', 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for v in res['mid_verbs']:\n",
    "    verb_info(v)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "135847b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>obj_count</th>\n",
       "      <th>obj_immediate_count</th>\n",
       "      <th>pct_immediate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>하+는</td>\n",
       "      <td>122</td>\n",
       "      <td>59</td>\n",
       "      <td>48.360656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>가지+고</td>\n",
       "      <td>113</td>\n",
       "      <td>98</td>\n",
       "      <td>86.725664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>위하+ㄴ</td>\n",
       "      <td>98</td>\n",
       "      <td>90</td>\n",
       "      <td>91.836735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>하+ㄴ</td>\n",
       "      <td>89</td>\n",
       "      <td>38</td>\n",
       "      <td>42.696629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>알+ㄹ</td>\n",
       "      <td>84</td>\n",
       "      <td>49</td>\n",
       "      <td>58.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>하+었+다</td>\n",
       "      <td>82</td>\n",
       "      <td>44</td>\n",
       "      <td>53.658537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>가지+ㄴ</td>\n",
       "      <td>77</td>\n",
       "      <td>68</td>\n",
       "      <td>88.311688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>하+ㄹ</td>\n",
       "      <td>68</td>\n",
       "      <td>37</td>\n",
       "      <td>54.411765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>하+ㄴ다</td>\n",
       "      <td>61</td>\n",
       "      <td>27</td>\n",
       "      <td>44.262295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>지니+고</td>\n",
       "      <td>58</td>\n",
       "      <td>48</td>\n",
       "      <td>82.758621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>알+고</td>\n",
       "      <td>56</td>\n",
       "      <td>29</td>\n",
       "      <td>51.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>갖+고</td>\n",
       "      <td>55</td>\n",
       "      <td>45</td>\n",
       "      <td>81.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>보+ㄹ</td>\n",
       "      <td>54</td>\n",
       "      <td>29</td>\n",
       "      <td>53.703704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>하+고</td>\n",
       "      <td>52</td>\n",
       "      <td>36</td>\n",
       "      <td>69.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>갖+는</td>\n",
       "      <td>46</td>\n",
       "      <td>40</td>\n",
       "      <td>86.956522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>비롯+하+ㄴ</td>\n",
       "      <td>44</td>\n",
       "      <td>36</td>\n",
       "      <td>81.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>지니+ㄴ</td>\n",
       "      <td>43</td>\n",
       "      <td>39</td>\n",
       "      <td>90.697674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>의미+하+ㄴ다</td>\n",
       "      <td>42</td>\n",
       "      <td>35</td>\n",
       "      <td>83.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>갖+게</td>\n",
       "      <td>41</td>\n",
       "      <td>39</td>\n",
       "      <td>95.121951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>알+게</td>\n",
       "      <td>40</td>\n",
       "      <td>28</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lemma  obj_count  obj_immediate_count  pct_immediate\n",
       "93        하+는        122                   59      48.360656\n",
       "388      가지+고        113                   98      86.725664\n",
       "284      위하+ㄴ         98                   90      91.836735\n",
       "88        하+ㄴ         89                   38      42.696629\n",
       "48        알+ㄹ         84                   49      58.333333\n",
       "386     하+었+다         82                   44      53.658537\n",
       "7        가지+ㄴ         77                   68      88.311688\n",
       "693       하+ㄹ         68                   37      54.411765\n",
       "79       하+ㄴ다         61                   27      44.262295\n",
       "46       지니+고         58                   48      82.758621\n",
       "31        알+고         56                   29      51.785714\n",
       "125       갖+고         55                   45      81.818182\n",
       "158       보+ㄹ         54                   29      53.703704\n",
       "60        하+고         52                   36      69.230769\n",
       "241       갖+는         46                   40      86.956522\n",
       "9      비롯+하+ㄴ         44                   36      81.818182\n",
       "289      지니+ㄴ         43                   39      90.697674\n",
       "1461  의미+하+ㄴ다         42                   35      83.333333\n",
       "305       갖+게         41                   39      95.121951\n",
       "290       알+게         40                   28      70.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved per-verb obj stats to data/project_outputs_task2/verbs_with_obj_stats.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "verb_obj_counts = defaultdict(int)\n",
    "verb_obj_immediate = defaultdict(int)\n",
    "\n",
    "for sent_idx, tokens in enumerate(sents):\n",
    "    id_to_tok = {t['id']: t for t in tokens}\n",
    "    dependents = defaultdict(list)\n",
    "    for t in tokens:\n",
    "        if t['head'] is not None and t['head'] in id_to_tok:\n",
    "            dependents[t['head']].append(t)\n",
    "    id_to_index = {t['id']: i for i, t in enumerate(tokens)}\n",
    "\n",
    "    for i, tok in enumerate(tokens):\n",
    "        if tok['upos'] == 'VERB':\n",
    "            verb_id = tok['id']\n",
    "            lemma = tok['lemma'] if tok['lemma'] != '_' else tok['form']\n",
    "            deps = dependents.get(verb_id, [])\n",
    "            for dep in deps:\n",
    "                if dep['deprel'] == 'obj':\n",
    "                    verb_obj_counts[lemma] += 1\n",
    "                    obj_idx = id_to_index.get(dep['id'])\n",
    "                    if obj_idx is not None and i - obj_idx == 1:\n",
    "                        verb_obj_immediate[lemma] += 1\n",
    "\n",
    "rows = []\n",
    "for lemma, cnt in verb_obj_counts.items():\n",
    "    rows.append({\n",
    "        'lemma': lemma,\n",
    "        'obj_count': cnt,\n",
    "        'obj_immediate_count': verb_obj_immediate.get(lemma, 0),\n",
    "        'pct_immediate': verb_obj_immediate.get(lemma, 0) / cnt * 100 if cnt>0 else 0\n",
    "    })\n",
    "df_verb_obj = pd.DataFrame(rows).sort_values('obj_count', ascending=False)\n",
    "\n",
    "display(df_verb_obj.head(20))\n",
    "csv_path = os.path.join(OUT_DIR, 'verbs_with_obj_stats.csv')\n",
    "df_verb_obj.to_csv(csv_path, index=False)\n",
    "print('Saved per-verb obj stats to', csv_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
