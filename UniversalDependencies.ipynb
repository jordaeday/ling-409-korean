{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "569de287-e7cd-4c06-9c75-859442428920",
   "metadata": {},
   "source": [
    "# Working with Universal Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8bcf2f5-0b05-4a70-9a96-06346284ce58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: conllu in /opt/anaconda3/lib/python3.13/site-packages (6.0.0)\n"
     ]
    }
   ],
   "source": [
    "# if you don't have conllu yet, uncomment the following\n",
    "!python -m pip install conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae05adf-0fe1-4d76-ba45-100c4ae55bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu # reading Universal Dependency files in the CONLLu format\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df75cf88-7d78-4868-9b71-05a30f5bb139",
   "metadata": {},
   "source": [
    "We open the GUM corpus as a text file, and look at its first few lines. After the initial metadata, the first sentence starts with the line\n",
    "      \"# text = Aesthetic Appreciation and Spanish Art:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e103158-0fd5-4666-9713-ade344abd881",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ko_kaist-ud-train.conllu\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005796a9",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb54b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb instances (VERB tokens): 55805\n",
      "Total obj instances: 13912\n",
      "Objects immediately before verb: 10066 (72.35%)\n",
      "Objects within 3 before verb: 12937 (92.99%)\n",
      "Objects after verb: 0 (0.00%)\n",
      "Saved distance distribution to data/project_outputs/object_verb_distance_distribution.csv\n"
     ]
    }
   ],
   "source": [
    "OUT_DIR = 'data/project_outputs'\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def parse_conllu(path):\n",
    "    \"\"\"\n",
    "    Return list of sentences; each sentence is a list of token dicts:\n",
    "    {id:int, form:str, lemma:str, upos:str, head:int or None, deprel:str}\n",
    "    Ignores multiword lines (1-2) and empty-node decimal ids (3.1).\n",
    "    \"\"\"\n",
    "    sents = []\n",
    "    tokens = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    sents.append(tokens)\n",
    "                    tokens = []\n",
    "                continue\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            cols = line.split(\"\\t\")\n",
    "            if len(cols) != 10:\n",
    "                continue\n",
    "            id_field = cols[0]\n",
    "            # skip multiword / empty nodes\n",
    "            if \"-\" in id_field or \".\" in id_field:\n",
    "                continue\n",
    "            try:\n",
    "                tid = int(id_field)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            form = cols[1]\n",
    "            lemma = cols[2]\n",
    "            upos = cols[3]\n",
    "            head = cols[6]\n",
    "            deprel = cols[7]\n",
    "            try:\n",
    "                head_int = int(head) if head != \"_\" else None\n",
    "            except ValueError:\n",
    "                head_int = None\n",
    "            tok = {\"id\": tid, \"form\": form, \"lemma\": lemma, \"upos\": upos, \"head\": head_int, \"deprel\": deprel}\n",
    "            tokens.append(tok)\n",
    "    # final\n",
    "    if tokens:\n",
    "        sents.append(tokens)\n",
    "    return sents\n",
    "\n",
    "sents = parse_conllu(\"ko_kaist-ud-train.conllu\")\n",
    "\n",
    "verb_instances = 0\n",
    "obj_instances = 0\n",
    "obj_immediate = 0\n",
    "obj_within3 = 0\n",
    "obj_to_right = 0\n",
    "dist_counter = Counter()\n",
    "\n",
    "examples = defaultdict(list) \n",
    "\n",
    "for sent_idx, tokens in enumerate(sents):\n",
    "    id_to_tok = {t['id']: t for t in tokens}\n",
    "    dependents = defaultdict(list)\n",
    "    for t in tokens:\n",
    "        if t['head'] is not None and t['head'] in id_to_tok:\n",
    "            dependents[t['head']].append(t)\n",
    "    id_to_index = {t['id']: i for i, t in enumerate(tokens)}\n",
    "\n",
    "    for i, tok in enumerate(tokens):\n",
    "        if tok['upos'] == 'VERB':\n",
    "            verb_instances += 1\n",
    "            verb_id = tok['id']\n",
    "            deps = dependents.get(verb_id, [])\n",
    "            for dep in deps:\n",
    "                if dep['deprel'] == 'obj':\n",
    "                    obj_instances += 1\n",
    "                    obj_idx = id_to_index.get(dep['id'])\n",
    "                    verb_idx = i\n",
    "                    if obj_idx is None:\n",
    "                        continue\n",
    "                    diff = verb_idx - obj_idx  # positive if object is to verb's left\n",
    "                    dist_counter[diff] += 1\n",
    "                    if diff == 1:\n",
    "                        obj_immediate += 1\n",
    "                    if 1 <= diff <= 3:\n",
    "                        obj_within3 += 1\n",
    "                    if diff < 0:\n",
    "                        obj_to_right += 1\n",
    "\n",
    "                    \n",
    "                    lemma = tok['lemma'] if tok['lemma'] != '_' else tok['form']\n",
    "                    if len(examples[lemma]) < 5:\n",
    "                        sent_form = ' '.join([t['form'] for t in tokens])\n",
    "                        examples[lemma].append((dep['form'], tok['form'], sent_form))\n",
    "\n",
    "if obj_instances > 0:\n",
    "    pct_immediate = obj_immediate / obj_instances * 100\n",
    "    pct_within3 = obj_within3 / obj_instances * 100\n",
    "    pct_after = obj_to_right / obj_instances * 100\n",
    "else:\n",
    "    pct_immediate = pct_within3 = 0.0\n",
    "\n",
    "print('Verb instances (VERB tokens):', verb_instances)\n",
    "print('Total obj instances:', obj_instances)\n",
    "print(f'Objects immediately before verb: {obj_immediate} ({pct_immediate:.2f}%)')\n",
    "print(f'Objects within 3 before verb: {obj_within3} ({pct_within3:.2f}%)')\n",
    "print(f'Objects after verb: {obj_to_right} ({pct_after:.2f}%)')\n",
    "\n",
    "dist_items = sorted(dist_counter.items())\n",
    "df_dist = pd.DataFrame(dist_items, columns=['verb_minus_obj_index', 'count'])\n",
    "df_dist.to_csv(os.path.join(OUT_DIR, 'object_verb_distance_distribution.csv'), index=False)\n",
    "print('Saved distance distribution to', os.path.join(OUT_DIR, 'object_verb_distance_distribution.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c453e1",
   "metadata": {},
   "source": [
    "### Task 1.1\n",
    "The generalization we can see from the above results is that an object _never_ occurs after a verb, it _must_ occur before the verb.\n",
    "\n",
    "### Task 1.2\n",
    "Our generalization generally holds true. While in our corupus, an object _never_ follows a verb, it does not always have to come before a verb (it does at a 72.35% rate). We have found that the object comes within 3 tokens before the verb, though this is also not always true (it occurs 92.99% of the time). The exceptions to this rule probably occur when objects are farther than 3 tokens from the verb, but still are before the verb, since we could not find any case where it follows a verb (at any length)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df06b09c",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94afcdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 23010 sentences from ko_kaist-ud-train.conllu\n",
      "Top verbs (sample): ['이', '하', '있', '되', '않', '보', '이러하', '대하', '주', '알', '보이', '생각', '가지', '말하', '못하', '오', '만들', '살', '받', '이루']\n",
      "Mid verbs (sample): ['물리치', '민감', '내려오', '제창', '갈', '문지르', '벗기', '자르', '되돌리', '담그', '금하', '대신', '낫', '운동', '공개', '소멸', '상호', '이바지', '낭비', '거절']\n",
      "Attempting to load vector model...\n",
      "Inspecting text vector file ko.tsv ...\n",
      "No headers so parsing text file line-by-line and building KeyedVectors (this may take time on your computers).\n",
      "Constructed KeyedVectors from text file with 603232 words and dim 5.\n",
      "Computing centroids and nearest neighbors...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "CONLLU_PATH = \"ko_kaist-ud-train.conllu\"   #Just a reminder, all files are directly just in the same folder as this code.\n",
    "OUT_DIR = \"data/project_outputs_task2\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "VERB_UPOS = (\"VERB\", \"AUX\")   # token UPOS considered verbs\n",
    "COPULA_CANONICAL = \"이\"       # canonical token for copula (이)\n",
    "\n",
    "def normalize_korean_verb(lemma_or_form):\n",
    "\n",
    "    if not lemma_or_form or lemma_or_form == \"_\":\n",
    "        return lemma_or_form\n",
    "\n",
    "    \n",
    "    s = lemma_or_form.replace(\"＋\", \"+\").replace(\"‧\", \"+\").strip()\n",
    "\n",
    "    #if segmented with '+', split into morphemes\n",
    "    if \"+\" in s:\n",
    "        parts = [p for p in s.split(\"+\") if p]  # drop empty parts\n",
    "        #if any part equals the copula morpheme '이', treat as copula\n",
    "        if any(p == \"이\" for p in parts):\n",
    "            return COPULA_CANONICAL\n",
    "        return parts[0]\n",
    "\n",
    "    #if unsegmented but ends with '다', strip terminal '다' \n",
    "    if len(s) > 1 and s.endswith(\"다\"):\n",
    "        return s[:-1]\n",
    "\n",
    "    return s\n",
    "\n",
    "# ensures counts/locations are collapsed to normalized keys\n",
    "def aggregate_normalized_counts(counter, locations):\n",
    "\n",
    "    norm_counter = Counter()\n",
    "    norm_locations = defaultdict(list)\n",
    "    if locations:\n",
    "        for raw_key, locs in locations.items():\n",
    "            norm = normalize_korean_verb(raw_key)\n",
    "            norm_counter[norm] += len(locs)\n",
    "            norm_locations[norm].extend(locs)\n",
    "    else:\n",
    "        for raw_key, cnt in counter.items():\n",
    "            norm = normalize_korean_verb(raw_key)\n",
    "            norm_counter[norm] += cnt\n",
    "    return norm_counter, norm_locations\n",
    "\n",
    "\n",
    "\n",
    "def verb_frequencies(sents, verb_upos=VERB_UPOS):\n",
    "    counter = Counter()\n",
    "    locations = defaultdict(list)  # canonical_verb -> list of (sent_idx, token_idx, original_token)\n",
    "    for si, tokens in enumerate(sents):\n",
    "        for i, tok in enumerate(tokens):\n",
    "            if tok[\"upos\"] in verb_upos:\n",
    "                lemma = tok[\"lemma\"] if tok[\"lemma\"] != \"_\" else tok[\"form\"]\n",
    "                norm = normalize_korean_verb(lemma)\n",
    "                counter[norm] += 1\n",
    "                locations[norm].append((si, i, tok))\n",
    "    return counter, locations\n",
    "\n",
    "\n",
    "def select_verbs_by_quantiles(counter, top_pct=0.20, next_pct=0.20, top_k_each=20):\n",
    "    items = counter.most_common()\n",
    "    types = [v for v, _ in items]\n",
    "    n_types = len(types)\n",
    "    top_n_types = max(1, int(n_types * top_pct))\n",
    "    next_n_types = max(1, int(n_types * next_pct))\n",
    "    top_type_set = set(types[:top_n_types])\n",
    "    next_type_set = set(types[top_n_types: top_n_types + next_n_types])\n",
    "    top_candidates = [v for v, _ in items if v in top_type_set]\n",
    "    next_candidates = [v for v, _ in items if v in next_type_set]\n",
    "    return top_candidates[:top_k_each], next_candidates[:top_k_each]\n",
    "\n",
    "\n",
    "def extract_verb_sets(sents, verb_locations, verbs, \n",
    "                      subj_deprels=(\"nsubj\", \"nsubj:pass\", \"csubj\"), \n",
    "                      obj_deprels_prefix=(\"obj\",), \n",
    "                      modifier_deprels_prefixes=(\"advmod\", \"amod\", \"nmod\", \"obl\", \"advcl\", \"compound\")):\n",
    "    results = {}\n",
    "    for verb in verbs:\n",
    "        subj_c = Counter()\n",
    "        obj_c = Counter()\n",
    "        mod_c = Counter()\n",
    "        before_c = Counter()\n",
    "        after_c = Counter()\n",
    "        occ = 0\n",
    "        locs = verb_locations.get(verb, [])\n",
    "        for si, vi, original_tok in locs:\n",
    "            sent = sents[si]\n",
    "            if vi < 0 or vi >= len(sent):\n",
    "                continue\n",
    "            occ += 1\n",
    "            id_to_tok = {t['id']: t for t in sent}\n",
    "            dependents = defaultdict(list)\n",
    "            for t in sent:\n",
    "                h = t['head']\n",
    "                if h is not None and h in id_to_tok:\n",
    "                    dependents[h].append(t)\n",
    "            v_deps = dependents.get(original_tok['id'], [])\n",
    "            for dep in v_deps:\n",
    "                deprel = dep['deprel']\n",
    "                if deprel in subj_deprels or deprel.startswith(\"nsubj\"):\n",
    "                    head_form = dep['lemma'] if dep['lemma'] != \"_\" else dep['form']\n",
    "                    subj_c[head_form] += 1\n",
    "                if any(deprel == p or deprel.startswith(p) for p in obj_deprels_prefix):\n",
    "                    head_form = dep['lemma'] if dep['lemma'] != \"_\" else dep['form']\n",
    "                    obj_c[head_form] += 1\n",
    "                if any(deprel == p or deprel.startswith(p) for p in modifier_deprels_prefixes):\n",
    "                    head_form = dep['lemma'] if dep['lemma'] != \"_\" else dep['form']\n",
    "                    mod_c[head_form] += 1\n",
    "            if vi - 1 >= 0:\n",
    "                before_c[sent[vi-1]['form']] += 1\n",
    "            if vi + 1 < len(sent):\n",
    "                after_c[sent[vi+1]['form']] += 1\n",
    "        results[verb] = {\n",
    "            \"subject\": subj_c,\n",
    "            \"object\": obj_c,\n",
    "            \"modifier\": mod_c,\n",
    "            \"before\": before_c,\n",
    "            \"after\": after_c,\n",
    "            \"occurrences\": occ\n",
    "        }\n",
    "    return results\n",
    "\n",
    "def load_korean_vector_model(bin_path=None, tsv_path=None, verbose=True):\n",
    "\n",
    "    attempts = []\n",
    "    \n",
    "    def _log(msg):\n",
    "        if verbose:\n",
    "            print(msg)\n",
    "\n",
    "    # Tried multiple ways to load the word2vec model but it was acting super weird and the stackoverflow page got very confusing for me\n",
    "    # (https://stackoverflow.com/questions/70458726/cant-load-the-pre-trained-word2vec-of-korean-language)\n",
    "    # Now I am just manually loading the whole model, let me know if you think this is the correct way\n",
    "    if tsv_path and os.path.exists(tsv_path):\n",
    "        try:\n",
    "            _log(f\"Inspecting text vector file {tsv_path} ...\")\n",
    "            with open(tsv_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                \n",
    "                sample = []\n",
    "                max_lines = 5000  \n",
    "                for i, line in enumerate(f):\n",
    "                    if not line.strip():\n",
    "                        continue\n",
    "                    sample.append(line.rstrip('\\n'))\n",
    "                    if i >= max_lines:\n",
    "                        break\n",
    "            if not sample:\n",
    "                raise RuntimeError(\"Text vector file appears empty or unreadable.\")\n",
    "            \n",
    "            first_tokens = sample[0].strip().split()\n",
    "            header_like = False\n",
    "            if len(first_tokens) >= 2 and first_tokens[0].isdigit() and first_tokens[1].isdigit():\n",
    "                header_like = True\n",
    "\n",
    "            \n",
    "\n",
    "            _log(\"No headers so parsing text file line-by-line and building KeyedVectors (this may take time on your computers).\")\n",
    "            words = []\n",
    "            vecs = []\n",
    "            dim = None\n",
    "            with open(tsv_path, 'r', encoding='utf-8', errors='ignore') as fh:\n",
    "                for line in fh:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    parts = line.split()\n",
    "                    if len(parts) < 2:\n",
    "                        # malformed; skip\n",
    "                        continue\n",
    "                    word = parts[0]\n",
    "                    num_tokens = parts[1:]\n",
    "                    # try convert to floats\n",
    "                    try:\n",
    "                        vec = np.array([float(x) for x in num_tokens], dtype=np.float32)\n",
    "                    except Exception:\n",
    "                        # kip line with non-numeric tokens\n",
    "                        continue\n",
    "                    if dim is None:\n",
    "                        dim = vec.shape[0]\n",
    "                    else:\n",
    "                        # if vector dims mismatch, skip\n",
    "                        if vec.shape[0] != dim:\n",
    "                            continue\n",
    "                    words.append(word)\n",
    "                    vecs.append(vec)\n",
    "            if not words:\n",
    "                raise RuntimeError(\"No valid word/vector lines parsed from file.\")\n",
    "            arr = np.vstack(vecs)\n",
    "            kv = KeyedVectors(vector_size=arr.shape[1])\n",
    "            kv.add_vectors(words, arr)\n",
    "            _log(f\"Constructed KeyedVectors from text file with {len(words)} words and dim {arr.shape[1]}.\")\n",
    "            return kv\n",
    "        except Exception as e:\n",
    "            attempts.append((\"w2v_text_manual\", str(e)))\n",
    "\n",
    "\n",
    "def centroid_of_words(model, words):\n",
    "    vecs = []\n",
    "    missed = []\n",
    "    for w in words:\n",
    "        if w is None:\n",
    "            continue\n",
    "        key = w\n",
    "        # for Korean we might want to try forms and lemmas as separate keys present assumption is that keys are surface tokens\n",
    "        if key in model:\n",
    "            vecs.append(model[key])\n",
    "        else:\n",
    "            missed.append(w)\n",
    "    if not vecs:\n",
    "        return None, 0, missed\n",
    "    arr = np.vstack(vecs)\n",
    "    return np.mean(arr, axis=0), arr.shape[0], missed\n",
    "\n",
    "def topk_neighbors_from_centroid(model, centroid_vec, k=10):\n",
    "    if centroid_vec is None:\n",
    "        return []\n",
    "    return model.similar_by_vector(centroid_vec, topn=k)\n",
    "\n",
    "\n",
    "def build_task2_analysis(conllu_path=CONLLU_PATH, model_bin=None, model_tsv=None, k_neighbors=10, top_k_each=5):\n",
    "    sents = parse_conllu(conllu_path)\n",
    "    print(f\"Loaded {len(sents)} sentences from {conllu_path}\")\n",
    "    raw_counter, raw_locations = verb_frequencies(sents)\n",
    "\n",
    "    verb_counter, verb_locations = aggregate_normalized_counts(raw_counter, raw_locations)\n",
    "\n",
    "    vf_df = pd.DataFrame(verb_counter.most_common(), columns=[\"verb_norm\", \"freq\"])\n",
    "    \n",
    "    vf_df.to_csv(os.path.join(OUT_DIR, \"verb_frequencies_normalized.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "    top_verbs, mid_verbs = select_verbs_by_quantiles(verb_counter, top_pct=0.20, next_pct=0.20, top_k_each=top_k_each)\n",
    "    print(\"Top verbs (sample):\", top_verbs)\n",
    "    print(\"Mid verbs (sample):\", mid_verbs)\n",
    "    verbs_to_analyze = list(top_verbs) + list(mid_verbs)\n",
    "    sets = extract_verb_sets(sents, verb_locations, verbs_to_analyze)\n",
    "    summary = {}\n",
    "    for v in verbs_to_analyze:\n",
    "        entry = sets[v]\n",
    "        summary[v] = {\n",
    "            \"occurrences\": entry[\"occurrences\"],\n",
    "            \"top_subjects\": entry[\"subject\"].most_common(50),\n",
    "            \"top_objects\": entry[\"object\"].most_common(50),\n",
    "            \"top_modifiers\": entry[\"modifier\"].most_common(50),\n",
    "            \"top_before\": entry[\"before\"].most_common(50),\n",
    "            \"top_after\": entry[\"after\"].most_common(50)\n",
    "        }\n",
    "    with open(os.path.join(OUT_DIR, \"verb_sets_summary_normalized.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    model = None\n",
    "    neighbors_summary = {}\n",
    "    if (model_bin and os.path.exists(model_bin)) or (model_tsv and os.path.exists(model_tsv)):\n",
    "        print(\"Attempting to load vector model...\")\n",
    "        model = load_korean_vector_model(bin_path=model_bin, tsv_path=model_tsv)\n",
    "        print(\"Computing centroids and nearest neighbors...\")\n",
    "        for v in verbs_to_analyze:\n",
    "            neighbors_summary[v] = {}\n",
    "            for set_name in (\"top_subjects\", \"top_objects\", \"top_modifiers\", \"top_before\", \"top_after\"):\n",
    "                words = [w for w, cnt in summary[v][set_name][:200]]\n",
    "                centroid, n_in_vocab, missed = centroid_of_words(model, words)\n",
    "                knn = topk_neighbors_from_centroid(model, centroid, k=k_neighbors) if centroid is not None else []\n",
    "                neighbors_summary[v][set_name] = {\n",
    "                    \"centroid_n_in_vocab\": int(n_in_vocab),\n",
    "                    \"missed_count\": len(missed),\n",
    "                    \"missed_examples\": missed[:30],\n",
    "                    \"knn\": [(w, float(sim)) for w, sim in knn]\n",
    "                }\n",
    "        with open(os.path.join(OUT_DIR, \"verb_neighbors_summary_normalized.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(neighbors_summary, f, ensure_ascii=False, indent=2)\n",
    "    else:\n",
    "        print(\"No model files provided or found; skipping centroid/knn stage.\")\n",
    "\n",
    "    return {\n",
    "        \"sentences\": len(sents),\n",
    "        \"verb_freq_df\": vf_df,\n",
    "        \"top_verbs\": top_verbs,\n",
    "        \"mid_verbs\": mid_verbs,\n",
    "        \"sets\": sets,\n",
    "        \"summary\": summary,\n",
    "        \"neighbors_summary\": neighbors_summary if model else None\n",
    "    }\n",
    "\n",
    "\n",
    "res = build_task2_analysis(\n",
    "    conllu_path=\"ko_kaist-ud-train.conllu\",\n",
    "    model_bin=\"ko.bin\",     \n",
    "    model_tsv=\"ko.tsv\",     \n",
    "    k_neighbors=20,\n",
    "    top_k_each=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135847b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>obj_count</th>\n",
       "      <th>obj_immediate_count</th>\n",
       "      <th>pct_immediate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>하+는</td>\n",
       "      <td>122</td>\n",
       "      <td>59</td>\n",
       "      <td>48.360656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>가지+고</td>\n",
       "      <td>113</td>\n",
       "      <td>98</td>\n",
       "      <td>86.725664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>위하+ㄴ</td>\n",
       "      <td>98</td>\n",
       "      <td>90</td>\n",
       "      <td>91.836735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>하+ㄴ</td>\n",
       "      <td>89</td>\n",
       "      <td>38</td>\n",
       "      <td>42.696629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>알+ㄹ</td>\n",
       "      <td>84</td>\n",
       "      <td>49</td>\n",
       "      <td>58.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>하+었+다</td>\n",
       "      <td>82</td>\n",
       "      <td>44</td>\n",
       "      <td>53.658537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>가지+ㄴ</td>\n",
       "      <td>77</td>\n",
       "      <td>68</td>\n",
       "      <td>88.311688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>하+ㄹ</td>\n",
       "      <td>68</td>\n",
       "      <td>37</td>\n",
       "      <td>54.411765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>하+ㄴ다</td>\n",
       "      <td>61</td>\n",
       "      <td>27</td>\n",
       "      <td>44.262295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>지니+고</td>\n",
       "      <td>58</td>\n",
       "      <td>48</td>\n",
       "      <td>82.758621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>알+고</td>\n",
       "      <td>56</td>\n",
       "      <td>29</td>\n",
       "      <td>51.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>갖+고</td>\n",
       "      <td>55</td>\n",
       "      <td>45</td>\n",
       "      <td>81.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>보+ㄹ</td>\n",
       "      <td>54</td>\n",
       "      <td>29</td>\n",
       "      <td>53.703704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>하+고</td>\n",
       "      <td>52</td>\n",
       "      <td>36</td>\n",
       "      <td>69.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>갖+는</td>\n",
       "      <td>46</td>\n",
       "      <td>40</td>\n",
       "      <td>86.956522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>비롯+하+ㄴ</td>\n",
       "      <td>44</td>\n",
       "      <td>36</td>\n",
       "      <td>81.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>지니+ㄴ</td>\n",
       "      <td>43</td>\n",
       "      <td>39</td>\n",
       "      <td>90.697674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>의미+하+ㄴ다</td>\n",
       "      <td>42</td>\n",
       "      <td>35</td>\n",
       "      <td>83.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>갖+게</td>\n",
       "      <td>41</td>\n",
       "      <td>39</td>\n",
       "      <td>95.121951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>알+게</td>\n",
       "      <td>40</td>\n",
       "      <td>28</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lemma  obj_count  obj_immediate_count  pct_immediate\n",
       "93        하+는        122                   59      48.360656\n",
       "388      가지+고        113                   98      86.725664\n",
       "284      위하+ㄴ         98                   90      91.836735\n",
       "88        하+ㄴ         89                   38      42.696629\n",
       "48        알+ㄹ         84                   49      58.333333\n",
       "386     하+었+다         82                   44      53.658537\n",
       "7        가지+ㄴ         77                   68      88.311688\n",
       "693       하+ㄹ         68                   37      54.411765\n",
       "79       하+ㄴ다         61                   27      44.262295\n",
       "46       지니+고         58                   48      82.758621\n",
       "31        알+고         56                   29      51.785714\n",
       "125       갖+고         55                   45      81.818182\n",
       "158       보+ㄹ         54                   29      53.703704\n",
       "60        하+고         52                   36      69.230769\n",
       "241       갖+는         46                   40      86.956522\n",
       "9      비롯+하+ㄴ         44                   36      81.818182\n",
       "289      지니+ㄴ         43                   39      90.697674\n",
       "1461  의미+하+ㄴ다         42                   35      83.333333\n",
       "305       갖+게         41                   39      95.121951\n",
       "290       알+게         40                   28      70.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved per-verb obj stats to data/project_outputs_task2/verbs_with_obj_stats.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "verb_obj_counts = defaultdict(int)\n",
    "verb_obj_immediate = defaultdict(int)\n",
    "\n",
    "for sent_idx, tokens in enumerate(sents):\n",
    "    id_to_tok = {t['id']: t for t in tokens}\n",
    "    dependents = defaultdict(list)\n",
    "    for t in tokens:\n",
    "        if t['head'] is not None and t['head'] in id_to_tok:\n",
    "            dependents[t['head']].append(t)\n",
    "    id_to_index = {t['id']: i for i, t in enumerate(tokens)}\n",
    "\n",
    "    for i, tok in enumerate(tokens):\n",
    "        if tok['upos'] == 'VERB':\n",
    "            verb_id = tok['id']\n",
    "            lemma = tok['lemma'] if tok['lemma'] != '_' else tok['form']\n",
    "            deps = dependents.get(verb_id, [])\n",
    "            for dep in deps:\n",
    "                if dep['deprel'] == 'obj':\n",
    "                    verb_obj_counts[lemma] += 1\n",
    "                    obj_idx = id_to_index.get(dep['id'])\n",
    "                    if obj_idx is not None and i - obj_idx == 1:\n",
    "                        verb_obj_immediate[lemma] += 1\n",
    "\n",
    "rows = []\n",
    "for lemma, cnt in verb_obj_counts.items():\n",
    "    rows.append({\n",
    "        'lemma': lemma,\n",
    "        'obj_count': cnt,\n",
    "        'obj_immediate_count': verb_obj_immediate.get(lemma, 0),\n",
    "        'pct_immediate': verb_obj_immediate.get(lemma, 0) / cnt * 100 if cnt>0 else 0\n",
    "    })\n",
    "df_verb_obj = pd.DataFrame(rows).sort_values('obj_count', ascending=False)\n",
    "\n",
    "display(df_verb_obj.head(20))\n",
    "csv_path = os.path.join(OUT_DIR, 'verbs_with_obj_stats.csv')\n",
    "df_verb_obj.to_csv(csv_path, index=False)\n",
    "print('Saved per-verb obj stats to', csv_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
