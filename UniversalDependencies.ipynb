{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "569de287-e7cd-4c06-9c75-859442428920",
   "metadata": {},
   "source": [
    "# Working with Universal Dependencies\n",
    "\n",
    "The Universal Dependencies framework distributes annotated corpora for many languages, all using the same dependency format. In this notebook, I'll demonstrate how to access such a corpus from within Python. \n",
    "\n",
    "We will work with English GUM corpus. You can find it listed under \"English\" on https://universaldependencies.org/#language- The repository with the actual corpus is at https://github.com/UniversalDependencies/UD_English-GUM/tree/master\n",
    "\n",
    "From the github repository, please download the training portion of the corpus, and put it in the same directory as this notebook. \n",
    "\n",
    "First, we will take a look at the format in which Universal Dependencies corpora are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8bcf2f5-0b05-4a70-9a96-06346284ce58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: conllu in c:\\users\\samanvay\\appdata\\roaming\\python\\python313\\site-packages (6.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# if you don't have conllu yet, uncomment the following\n",
    "!python -m pip install conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ae05adf-0fe1-4d76-ba45-100c4ae55bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu # reading Universal Dependency files in the CONLLu format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df75cf88-7d78-4868-9b71-05a30f5bb139",
   "metadata": {},
   "source": [
    "We open the GUM corpus as a text file, and look at its first few lines. After the initial metadata, the first sentence starts with the line\n",
    "      \"# text = Aesthetic Appreciation and Spanish Art:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e103158-0fd5-4666-9713-ade344abd881",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ko_kaist-ud-train.conllu\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13236d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 23010 sentences\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def parseconllu():\n",
    "    sentences = []\n",
    "    with open(\"ko_kaist-ud-train.conllu\", encoding='utf-8') as fh:\n",
    "        sent_lines = []\n",
    "        for raw in fh:\n",
    "            line = raw.rstrip('\\n')\n",
    "            if line.startswith('#'):\n",
    "                sent_lines.append(line)\n",
    "            elif line.strip() == '':\n",
    "                if sent_lines:\n",
    "                    sentences.append(sent_lines)\n",
    "                    sent_lines = []\n",
    "            else:\n",
    "                sent_lines.append(line)\n",
    "        if sent_lines:\n",
    "            sentences.append(sent_lines)\n",
    "\n",
    "    parsed = []\n",
    "    for s in sentences:\n",
    "        tokens = []\n",
    "        for ln in s:\n",
    "            if ln.startswith('#'):\n",
    "                continue\n",
    "            parts = ln.split('\\t')\n",
    "            if len(parts) != 10:\n",
    "                continue\n",
    "            id_, form, lemma, upos, xpos, feats, head, deprel, deps, misc = parts\n",
    "            if '-' in id_:\n",
    "                continue\n",
    "            try:\n",
    "                id_int = int(id_)\n",
    "            except Exception:\n",
    "                continue\n",
    "            token = {\n",
    "                'id': id_int,\n",
    "                'form': form,\n",
    "                'lemma': lemma,\n",
    "                'upos': upos,\n",
    "                'xpos': xpos,\n",
    "                'feats': feats,\n",
    "                'head': int(head) if head != '_' else None,\n",
    "                'deprel': deprel,\n",
    "                'deps': deps,\n",
    "                'misc': misc\n",
    "            }\n",
    "            tokens.append(token)\n",
    "        if tokens:\n",
    "            tokens.sort(key=lambda t: t['id'])\n",
    "            parsed.append(tokens)\n",
    "    return parsed\n",
    "\n",
    "sents = parseconllu()\n",
    "print(f'Parsed {len(sents)} sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bb54b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb instances (VERB tokens): 55805\n",
      "Total obj instances: 13912\n",
      "Objects immediately before verb: 10066 (72.35%)\n",
      "Objects within 3 before verb: 12937 (92.99%)\n",
      "Objects after verb: 0 (0.00%)\n",
      "Saved distance distribution to data/project_outputs\\object_verb_distance_distribution.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "OUT_DIR = 'data/project_outputs'\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def parse_conllu(path):\n",
    "    \"\"\"\n",
    "    Return list of sentences; each sentence is a list of token dicts:\n",
    "    {id:int, form:str, lemma:str, upos:str, head:int or None, deprel:str}\n",
    "    Ignores multiword lines (1-2) and empty-node decimal ids (3.1).\n",
    "    \"\"\"\n",
    "    sents = []\n",
    "    tokens = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    sents.append(tokens)\n",
    "                    tokens = []\n",
    "                continue\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            cols = line.split(\"\\t\")\n",
    "            if len(cols) != 10:\n",
    "                continue\n",
    "            id_field = cols[0]\n",
    "            # skip multiword / empty nodes\n",
    "            if \"-\" in id_field or \".\" in id_field:\n",
    "                continue\n",
    "            try:\n",
    "                tid = int(id_field)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            form = cols[1]\n",
    "            lemma = cols[2]\n",
    "            upos = cols[3]\n",
    "            head = cols[6]\n",
    "            deprel = cols[7]\n",
    "            try:\n",
    "                head_int = int(head) if head != \"_\" else None\n",
    "            except ValueError:\n",
    "                head_int = None\n",
    "            tok = {\"id\": tid, \"form\": form, \"lemma\": lemma, \"upos\": upos, \"head\": head_int, \"deprel\": deprel}\n",
    "            tokens.append(tok)\n",
    "    # final\n",
    "    if tokens:\n",
    "        sents.append(tokens)\n",
    "    return sents\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "verb_instances = 0\n",
    "obj_instances = 0\n",
    "obj_immediate = 0\n",
    "obj_within3 = 0\n",
    "obj_to_right = 0\n",
    "dist_counter = Counter()\n",
    "\n",
    "examples = defaultdict(list) \n",
    "\n",
    "for sent_idx, tokens in enumerate(sents):\n",
    "    id_to_tok = {t['id']: t for t in tokens}\n",
    "    dependents = defaultdict(list)\n",
    "    for t in tokens:\n",
    "        if t['head'] is not None and t['head'] in id_to_tok:\n",
    "            dependents[t['head']].append(t)\n",
    "    id_to_index = {t['id']: i for i, t in enumerate(tokens)}\n",
    "\n",
    "    for i, tok in enumerate(tokens):\n",
    "        if tok['upos'] == 'VERB':\n",
    "            verb_instances += 1\n",
    "            verb_id = tok['id']\n",
    "            deps = dependents.get(verb_id, [])\n",
    "            for dep in deps:\n",
    "                if dep['deprel'] == 'obj':\n",
    "                    obj_instances += 1\n",
    "                    obj_idx = id_to_index.get(dep['id'])\n",
    "                    verb_idx = i\n",
    "                    if obj_idx is None:\n",
    "                        continue\n",
    "                    diff = verb_idx - obj_idx  # positive if object is to verb's left\n",
    "                    dist_counter[diff] += 1\n",
    "                    if diff == 1:\n",
    "                        obj_immediate += 1\n",
    "                    if 1 <= diff <= 3:\n",
    "                        obj_within3 += 1\n",
    "                    if diff < 0:\n",
    "                        obj_to_right += 1\n",
    "\n",
    "                    \n",
    "                    lemma = tok['lemma'] if tok['lemma'] != '_' else tok['form']\n",
    "                    if len(examples[lemma]) < 5:\n",
    "                        sent_form = ' '.join([t['form'] for t in tokens])\n",
    "                        examples[lemma].append((dep['form'], tok['form'], sent_form))\n",
    "\n",
    "if obj_instances > 0:\n",
    "    pct_immediate = obj_immediate / obj_instances * 100\n",
    "    pct_within3 = obj_within3 / obj_instances * 100\n",
    "    pct_after = obj_to_right / obj_instances * 100\n",
    "else:\n",
    "    pct_immediate = pct_within3 = 0.0\n",
    "\n",
    "print('Verb instances (VERB tokens):', verb_instances)\n",
    "print('Total obj instances:', obj_instances)\n",
    "print(f'Objects immediately before verb: {obj_immediate} ({pct_immediate:.2f}%)')\n",
    "print(f'Objects within 3 before verb: {obj_within3} ({pct_within3:.2f}%)')\n",
    "print(f'Objects after verb: {obj_to_right} ({pct_after:.2f}%)')\n",
    "\n",
    "dist_items = sorted(dist_counter.items())\n",
    "df_dist = pd.DataFrame(dist_items, columns=['verb_minus_obj_index', 'count'])\n",
    "df_dist.to_csv(os.path.join(OUT_DIR, 'object_verb_distance_distribution.csv'), index=False)\n",
    "print('Saved distance distribution to', os.path.join(OUT_DIR, 'object_verb_distance_distribution.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94afcdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 23010 sentences from ko_kaist-ud-train.conllu\n",
      "Top verbs (sample): ['이', '하', '있', '되', '않', '보', '이러하', '대하', '주', '알', '보이', '생각', '가지', '말하', '못하', '오', '만들', '살', '받', '이루']\n",
      "Mid verbs (sample): ['물리치', '민감', '내려오', '제창', '갈', '문지르', '벗기', '자르', '되돌리', '담그', '금하', '대신', '낫', '운동', '공개', '소멸', '상호', '이바지', '낭비', '거절']\n",
      "Attempting to load vector model...\n",
      "Inspecting text vector file ko.tsv ...\n",
      "No headers so parsing text file line-by-line and building KeyedVectors (this may take time on your computers).\n",
      "Constructed KeyedVectors from text file with 603232 words and dim 5.\n",
      "Computing centroids and nearest neighbors...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "CONLLU_PATH = \"ko_kaist-ud-train.conllu\"   #Just a reminder, all files are directly just in the same folder as this code.\n",
    "OUT_DIR = \"data/project_outputs_task2\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "VERB_UPOS = (\"VERB\", \"AUX\")   # token UPOS considered verbs\n",
    "COPULA_CANONICAL = \"이\"       # canonical token for copula (이)\n",
    "\n",
    "def parse_conllu(path): #Nothing much here, same as previous parse functions, just has a path argument now\n",
    "    sents = []\n",
    "    tokens = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    sents.append(tokens)\n",
    "                    tokens = []\n",
    "                continue\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            cols = line.split(\"\\t\")\n",
    "            if len(cols) != 10:\n",
    "                continue\n",
    "            id_field = cols[0]\n",
    "            if \"-\" in id_field or \".\" in id_field:\n",
    "                \n",
    "                continue\n",
    "            try:\n",
    "                tid = int(id_field)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            form = cols[1]\n",
    "            lemma = cols[2]\n",
    "            upos = cols[3]\n",
    "            head = cols[6]\n",
    "            deprel = cols[7]\n",
    "            try:\n",
    "                head_int = int(head) if head != \"_\" else None\n",
    "            except ValueError:\n",
    "                head_int = None\n",
    "            tok = {\n",
    "                \"id\": tid,\n",
    "                \"form\": form,\n",
    "                \"lemma\": lemma,\n",
    "                \"upos\": upos,\n",
    "                \"head\": head_int,\n",
    "                \"deprel\": deprel\n",
    "            }\n",
    "            tokens.append(tok)\n",
    "    if tokens:\n",
    "        sents.append(tokens)\n",
    "    return sents\n",
    "\n",
    "import re\n",
    "\n",
    "def normalize_korean_verb(lemma_or_form):\n",
    "\n",
    "    if not lemma_or_form or lemma_or_form == \"_\":\n",
    "        return lemma_or_form\n",
    "\n",
    "    \n",
    "    s = lemma_or_form.replace(\"＋\", \"+\").replace(\"‧\", \"+\").strip()\n",
    "\n",
    "    #if segmented with '+', split into morphemes\n",
    "    if \"+\" in s:\n",
    "        parts = [p for p in s.split(\"+\") if p]  # drop empty parts\n",
    "        #if any part equals the copula morpheme '이', treat as copula\n",
    "        if any(p == \"이\" for p in parts):\n",
    "            return COPULA_CANONICAL\n",
    "        return parts[0]\n",
    "\n",
    "    #if unsegmented but ends with '다', strip terminal '다' \n",
    "    if len(s) > 1 and s.endswith(\"다\"):\n",
    "        return s[:-1]\n",
    "\n",
    "    return s\n",
    "\n",
    "# ensures counts/locations are collapsed to normalized keys\n",
    "def aggregate_normalized_counts(counter, locations):\n",
    "\n",
    "    norm_counter = Counter()\n",
    "    norm_locations = defaultdict(list)\n",
    "    if locations:\n",
    "        for raw_key, locs in locations.items():\n",
    "            norm = normalize_korean_verb(raw_key)\n",
    "            norm_counter[norm] += len(locs)\n",
    "            norm_locations[norm].extend(locs)\n",
    "    else:\n",
    "        for raw_key, cnt in counter.items():\n",
    "            norm = normalize_korean_verb(raw_key)\n",
    "            norm_counter[norm] += cnt\n",
    "    return norm_counter, norm_locations\n",
    "\n",
    "\n",
    "\n",
    "def verb_frequencies(sents, verb_upos=VERB_UPOS):\n",
    "    counter = Counter()\n",
    "    locations = defaultdict(list)  # canonical_verb -> list of (sent_idx, token_idx, original_token)\n",
    "    for si, tokens in enumerate(sents):\n",
    "        for i, tok in enumerate(tokens):\n",
    "            if tok[\"upos\"] in verb_upos:\n",
    "                lemma = tok[\"lemma\"] if tok[\"lemma\"] != \"_\" else tok[\"form\"]\n",
    "                norm = normalize_korean_verb(lemma)\n",
    "                counter[norm] += 1\n",
    "                locations[norm].append((si, i, tok))\n",
    "    return counter, locations\n",
    "\n",
    "\n",
    "def select_verbs_by_quantiles(counter, top_pct=0.20, next_pct=0.20, top_k_each=20):\n",
    "    items = counter.most_common()\n",
    "    types = [v for v, _ in items]\n",
    "    n_types = len(types)\n",
    "    top_n_types = max(1, int(n_types * top_pct))\n",
    "    next_n_types = max(1, int(n_types * next_pct))\n",
    "    top_type_set = set(types[:top_n_types])\n",
    "    next_type_set = set(types[top_n_types: top_n_types + next_n_types])\n",
    "    top_candidates = [v for v, _ in items if v in top_type_set]\n",
    "    next_candidates = [v for v, _ in items if v in next_type_set]\n",
    "    return top_candidates[:top_k_each], next_candidates[:top_k_each]\n",
    "\n",
    "\n",
    "def extract_verb_sets(sents, verb_locations, verbs, \n",
    "                      subj_deprels=(\"nsubj\", \"nsubj:pass\", \"csubj\"), \n",
    "                      obj_deprels_prefix=(\"obj\",), \n",
    "                      modifier_deprels_prefixes=(\"advmod\", \"amod\", \"nmod\", \"obl\", \"advcl\", \"compound\")):\n",
    "    results = {}\n",
    "    for verb in verbs:\n",
    "        subj_c = Counter()\n",
    "        obj_c = Counter()\n",
    "        mod_c = Counter()\n",
    "        before_c = Counter()\n",
    "        after_c = Counter()\n",
    "        occ = 0\n",
    "        locs = verb_locations.get(verb, [])\n",
    "        for si, vi, original_tok in locs:\n",
    "            sent = sents[si]\n",
    "            if vi < 0 or vi >= len(sent):\n",
    "                continue\n",
    "            occ += 1\n",
    "            id_to_tok = {t['id']: t for t in sent}\n",
    "            dependents = defaultdict(list)\n",
    "            for t in sent:\n",
    "                h = t['head']\n",
    "                if h is not None and h in id_to_tok:\n",
    "                    dependents[h].append(t)\n",
    "            v_deps = dependents.get(original_tok['id'], [])\n",
    "            for dep in v_deps:\n",
    "                deprel = dep['deprel']\n",
    "                if deprel in subj_deprels or deprel.startswith(\"nsubj\"):\n",
    "                    head_form = dep['lemma'] if dep['lemma'] != \"_\" else dep['form']\n",
    "                    subj_c[head_form] += 1\n",
    "                if any(deprel == p or deprel.startswith(p) for p in obj_deprels_prefix):\n",
    "                    head_form = dep['lemma'] if dep['lemma'] != \"_\" else dep['form']\n",
    "                    obj_c[head_form] += 1\n",
    "                if any(deprel == p or deprel.startswith(p) for p in modifier_deprels_prefixes):\n",
    "                    head_form = dep['lemma'] if dep['lemma'] != \"_\" else dep['form']\n",
    "                    mod_c[head_form] += 1\n",
    "            if vi - 1 >= 0:\n",
    "                before_c[sent[vi-1]['form']] += 1\n",
    "            if vi + 1 < len(sent):\n",
    "                after_c[sent[vi+1]['form']] += 1\n",
    "        results[verb] = {\n",
    "            \"subject\": subj_c,\n",
    "            \"object\": obj_c,\n",
    "            \"modifier\": mod_c,\n",
    "            \"before\": before_c,\n",
    "            \"after\": after_c,\n",
    "            \"occurrences\": occ\n",
    "        }\n",
    "    return results\n",
    "\n",
    "def load_korean_vector_model(bin_path=None, tsv_path=None, verbose=True):\n",
    "    \n",
    "    \n",
    "    from gensim.models import KeyedVectors, Word2Vec\n",
    "    attempts = []\n",
    "    \n",
    "    def _log(msg):\n",
    "        if verbose:\n",
    "            print(msg)\n",
    "    \"\"\"\n",
    "    if bin_path and os.path.exists(bin_path):\n",
    "        try:\n",
    "            _log(f\"Trying Word2Vec.load on {bin_path} ...\")\n",
    "            model = Word2Vec.load(bin_path)\n",
    "            if hasattr(model, 'wv'):\n",
    "                _log(\"Loaded Word2Vec; returning model.wv\")\n",
    "                return model.wv\n",
    "            if isinstance(model, KeyedVectors) or hasattr(model, 'key_to_index'):\n",
    "                _log(\"Loaded object looks like KeyedVectors; returning it directly.\")\n",
    "                return model\n",
    "            if hasattr(model, 'vectors') and hasattr(model, 'index_to_key'):\n",
    "                kv = KeyedVectors(vector_size=model.vector_size if hasattr(model,'vector_size') else model.vectors.shape[1])\n",
    "                kv.add_vectors(model.index_to_key, model.vectors)\n",
    "                _log(\"Converted loaded object to KeyedVectors and returning.\")\n",
    "                return kv\n",
    "            attempts.append((\"Word2Vec.load_unexpected_type\", f\"Loaded type {type(model)} lacked expected attributes\"))\n",
    "        except Exception as e:\n",
    "            attempts.append((\"Word2Vec.load\", str(e)))\n",
    "\n",
    "    if bin_path and os.path.exists(bin_path):\n",
    "        try:\n",
    "            _log(f\"Trying KeyedVectors.load on {bin_path} ...\")\n",
    "            kv = KeyedVectors.load(bin_path, mmap='r')\n",
    "            _log(\"Loaded KeyedVectors via KeyedVectors.load()\")\n",
    "            return kv\n",
    "        except Exception as e:\n",
    "            attempts.append((\"KeyedVectors.load\", str(e)))\n",
    "\n",
    "    if bin_path and os.path.exists(bin_path):\n",
    "        try:\n",
    "            _log(f\"Trying gensim.models.fasttext.load_facebook_model on {bin_path} ...\")\n",
    "            from gensim.models.fasttext import load_facebook_model\n",
    "            ft = load_facebook_model(bin_path)\n",
    "            _log(\"Loaded fastText (facebook) model; returning ft.wv\")\n",
    "            return ft.wv\n",
    "        except Exception as e:\n",
    "            attempts.append((\"fasttext_fb\", str(e)))\n",
    "\n",
    "    \n",
    "    if bin_path and os.path.exists(bin_path):\n",
    "        try:\n",
    "            _log(\"Trying python-fasttext (fasttext) to load .bin (supervised or unsupervised)...\")\n",
    "            import fasttext\n",
    "            ftmodel = fasttext.load_model(bin_path)  \n",
    "            words = ftmodel.get_words()\n",
    "            \n",
    "            vectors = []\n",
    "            found_words = []\n",
    "            for w in words:\n",
    "                vec = ftmodel.get_word_vector(w)\n",
    "                if vec is None:\n",
    "                    continue\n",
    "                found_words.append(w)\n",
    "                vectors.append(np.array(vec, dtype=np.float32))\n",
    "            if not vectors:\n",
    "                raise RuntimeError(\"fasttext loaded but no word vectors returned.\")\n",
    "            vecs_arr = np.vstack(vectors)\n",
    "            kv = KeyedVectors(vector_size=vecs_arr.shape[1])\n",
    "            kv.add_vectors(found_words, vecs_arr)\n",
    "            _log(\"Constructed KeyedVectors from python-fasttext model; returning.\")\n",
    "            return kv\n",
    "        except Exception as e:\n",
    "            attempts.append((\"fasttext_py\", str(e)))\n",
    "\n",
    "    \n",
    "    if bin_path and os.path.exists(bin_path):\n",
    "        try:\n",
    "            _log(f\"Trying KeyedVectors.load_word2vec_format(binary=True) on {bin_path} ...\")\n",
    "            kv = KeyedVectors.load_word2vec_format(bin_path, binary=True, unicode_errors='ignore')\n",
    "            _log(\"Loaded KeyedVectors from word2vec binary.\")\n",
    "            return kv\n",
    "        except Exception as e:\n",
    "            attempts.append((\"w2v_binary\", str(e)))\"\"\"\n",
    "\n",
    "    # Tried multiple ways to load the word2vec model but it was acting super weird and the stackoverflow page got very confusing for me\n",
    "    # (https://stackoverflow.com/questions/70458726/cant-load-the-pre-trained-word2vec-of-korean-language)\n",
    "    # Now I am just manually loading the whole model, let me know if you think this is the correct way\n",
    "    if tsv_path and os.path.exists(tsv_path):\n",
    "        try:\n",
    "            _log(f\"Inspecting text vector file {tsv_path} ...\")\n",
    "            with open(tsv_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                \n",
    "                sample = []\n",
    "                max_lines = 5000  \n",
    "                for i, line in enumerate(f):\n",
    "                    if not line.strip():\n",
    "                        continue\n",
    "                    sample.append(line.rstrip('\\n'))\n",
    "                    if i >= max_lines:\n",
    "                        break\n",
    "            if not sample:\n",
    "                raise RuntimeError(\"Text vector file appears empty or unreadable.\")\n",
    "            \n",
    "            first_tokens = sample[0].strip().split()\n",
    "            header_like = False\n",
    "            if len(first_tokens) >= 2 and first_tokens[0].isdigit() and first_tokens[1].isdigit():\n",
    "                header_like = True\n",
    "\n",
    "            \n",
    "\n",
    "            _log(\"No headers so parsing text file line-by-line and building KeyedVectors (this may take time on your computers).\")\n",
    "            words = []\n",
    "            vecs = []\n",
    "            dim = None\n",
    "            with open(tsv_path, 'r', encoding='utf-8', errors='ignore') as fh:\n",
    "                for line in fh:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    parts = line.split()\n",
    "                    if len(parts) < 2:\n",
    "                        # malformed; skip\n",
    "                        continue\n",
    "                    word = parts[0]\n",
    "                    num_tokens = parts[1:]\n",
    "                    # try convert to floats\n",
    "                    try:\n",
    "                        vec = np.array([float(x) for x in num_tokens], dtype=np.float32)\n",
    "                    except Exception:\n",
    "                        # kip line with non-numeric tokens\n",
    "                        continue\n",
    "                    if dim is None:\n",
    "                        dim = vec.shape[0]\n",
    "                    else:\n",
    "                        # if vector dims mismatch, skip\n",
    "                        if vec.shape[0] != dim:\n",
    "                            continue\n",
    "                    words.append(word)\n",
    "                    vecs.append(vec)\n",
    "            if not words:\n",
    "                raise RuntimeError(\"No valid word/vector lines parsed from file.\")\n",
    "            arr = np.vstack(vecs)\n",
    "            kv = KeyedVectors(vector_size=arr.shape[1])\n",
    "            kv.add_vectors(words, arr)\n",
    "            _log(f\"Constructed KeyedVectors from text file with {len(words)} words and dim {arr.shape[1]}.\")\n",
    "            return kv\n",
    "        except Exception as e:\n",
    "            attempts.append((\"w2v_text_manual\", str(e)))\n",
    "\n",
    "\n",
    "def centroid_of_words(model, words):\n",
    "    vecs = []\n",
    "    missed = []\n",
    "    for w in words:\n",
    "        if w is None:\n",
    "            continue\n",
    "        key = w\n",
    "        # for Korean we might want to try forms and lemmas as separate keys present assumption is that keys are surface tokens\n",
    "        if key in model:\n",
    "            vecs.append(model[key])\n",
    "        else:\n",
    "            missed.append(w)\n",
    "    if not vecs:\n",
    "        return None, 0, missed\n",
    "    arr = np.vstack(vecs)\n",
    "    return np.mean(arr, axis=0), arr.shape[0], missed\n",
    "\n",
    "def topk_neighbors_from_centroid(model, centroid_vec, k=10):\n",
    "    if centroid_vec is None:\n",
    "        return []\n",
    "    return model.similar_by_vector(centroid_vec, topn=k)\n",
    "\n",
    "\n",
    "def build_task2_analysis(conllu_path=CONLLU_PATH, model_bin=None, model_tsv=None, k_neighbors=10, top_k_each=5):\n",
    "    sents = parse_conllu(conllu_path)\n",
    "    print(f\"Loaded {len(sents)} sentences from {conllu_path}\")\n",
    "    raw_counter, raw_locations = verb_frequencies(sents)\n",
    "\n",
    "    verb_counter, verb_locations = aggregate_normalized_counts(raw_counter, raw_locations)\n",
    "\n",
    "    vf_df = pd.DataFrame(verb_counter.most_common(), columns=[\"verb_norm\", \"freq\"])\n",
    "    \n",
    "    vf_df.to_csv(os.path.join(OUT_DIR, \"verb_frequencies_normalized.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "    top_verbs, mid_verbs = select_verbs_by_quantiles(verb_counter, top_pct=0.20, next_pct=0.20, top_k_each=top_k_each)\n",
    "    print(\"Top verbs (sample):\", top_verbs)\n",
    "    print(\"Mid verbs (sample):\", mid_verbs)\n",
    "    verbs_to_analyze = list(top_verbs) + list(mid_verbs)\n",
    "    sets = extract_verb_sets(sents, verb_locations, verbs_to_analyze)\n",
    "    summary = {}\n",
    "    for v in verbs_to_analyze:\n",
    "        entry = sets[v]\n",
    "        summary[v] = {\n",
    "            \"occurrences\": entry[\"occurrences\"],\n",
    "            \"top_subjects\": entry[\"subject\"].most_common(50),\n",
    "            \"top_objects\": entry[\"object\"].most_common(50),\n",
    "            \"top_modifiers\": entry[\"modifier\"].most_common(50),\n",
    "            \"top_before\": entry[\"before\"].most_common(50),\n",
    "            \"top_after\": entry[\"after\"].most_common(50)\n",
    "        }\n",
    "    with open(os.path.join(OUT_DIR, \"verb_sets_summary_normalized.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    model = None\n",
    "    neighbors_summary = {}\n",
    "    if (model_bin and os.path.exists(model_bin)) or (model_tsv and os.path.exists(model_tsv)):\n",
    "        print(\"Attempting to load vector model...\")\n",
    "        model = load_korean_vector_model(bin_path=model_bin, tsv_path=model_tsv)\n",
    "        print(\"Computing centroids and nearest neighbors...\")\n",
    "        for v in verbs_to_analyze:\n",
    "            neighbors_summary[v] = {}\n",
    "            for set_name in (\"top_subjects\", \"top_objects\", \"top_modifiers\", \"top_before\", \"top_after\"):\n",
    "                words = [w for w, cnt in summary[v][set_name][:200]]\n",
    "                centroid, n_in_vocab, missed = centroid_of_words(model, words)\n",
    "                knn = topk_neighbors_from_centroid(model, centroid, k=k_neighbors) if centroid is not None else []\n",
    "                neighbors_summary[v][set_name] = {\n",
    "                    \"centroid_n_in_vocab\": int(n_in_vocab),\n",
    "                    \"missed_count\": len(missed),\n",
    "                    \"missed_examples\": missed[:30],\n",
    "                    \"knn\": [(w, float(sim)) for w, sim in knn]\n",
    "                }\n",
    "        with open(os.path.join(OUT_DIR, \"verb_neighbors_summary_normalized.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(neighbors_summary, f, ensure_ascii=False, indent=2)\n",
    "    else:\n",
    "        print(\"No model files provided or found; skipping centroid/knn stage.\")\n",
    "\n",
    "    return {\n",
    "        \"sentences\": len(sents),\n",
    "        \"verb_freq_df\": vf_df,\n",
    "        \"top_verbs\": top_verbs,\n",
    "        \"mid_verbs\": mid_verbs,\n",
    "        \"sets\": sets,\n",
    "        \"summary\": summary,\n",
    "        \"neighbors_summary\": neighbors_summary if model else None\n",
    "    }\n",
    "\n",
    "\n",
    "res = build_task2_analysis(\n",
    "    conllu_path=\"ko_kaist-ud-train.conllu\",\n",
    "    model_bin=\"ko.bin\",     \n",
    "    model_tsv=\"ko.tsv\",     \n",
    "    k_neighbors=20,\n",
    "    top_k_each=20\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5913ee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_after_adjacent = 0    \n",
    "adj_within3_after = 0    \n",
    "adj_syntactic_amod = 0   \n",
    "\n",
    "for i, tok in enumerate(tokens):\n",
    "    if tok['upos'] == 'VERB':\n",
    "        verb_idx = i\n",
    "        verb_id = tok['id']\n",
    "        deps = dependents.get(verb_id, [])\n",
    "        for dep in deps:\n",
    "            if dep['deprel'] == 'obj':\n",
    "                obj_idx = id_to_index.get(dep['id'])\n",
    "                \n",
    "                for k in range(1, 4):  \n",
    "                    check_idx = obj_idx + k\n",
    "                    if check_idx < len(tokens):\n",
    "                        nxt = tokens[check_idx]\n",
    "                        if nxt['upos'] == 'ADJ':\n",
    "                            if k == 1:\n",
    "                                adj_after_adjacent += 1\n",
    "                            adj_within3_after += 1\n",
    "                obj_id = dep['id']\n",
    "                obj_dependents = dependents.get(obj_id, [])\n",
    "                for od in obj_dependents:\n",
    "                    if od['deprel'] == 'amod' or od['upos'] == 'ADJ':\n",
    "                        adj_syntactic_amod += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "135847b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>obj_count</th>\n",
       "      <th>obj_immediate_count</th>\n",
       "      <th>pct_immediate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>하+는</td>\n",
       "      <td>122</td>\n",
       "      <td>59</td>\n",
       "      <td>48.360656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>가지+고</td>\n",
       "      <td>113</td>\n",
       "      <td>98</td>\n",
       "      <td>86.725664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>위하+ㄴ</td>\n",
       "      <td>98</td>\n",
       "      <td>90</td>\n",
       "      <td>91.836735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>하+ㄴ</td>\n",
       "      <td>89</td>\n",
       "      <td>38</td>\n",
       "      <td>42.696629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>알+ㄹ</td>\n",
       "      <td>84</td>\n",
       "      <td>49</td>\n",
       "      <td>58.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>하+었+다</td>\n",
       "      <td>82</td>\n",
       "      <td>44</td>\n",
       "      <td>53.658537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>가지+ㄴ</td>\n",
       "      <td>77</td>\n",
       "      <td>68</td>\n",
       "      <td>88.311688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>하+ㄹ</td>\n",
       "      <td>68</td>\n",
       "      <td>37</td>\n",
       "      <td>54.411765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>하+ㄴ다</td>\n",
       "      <td>61</td>\n",
       "      <td>27</td>\n",
       "      <td>44.262295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>지니+고</td>\n",
       "      <td>58</td>\n",
       "      <td>48</td>\n",
       "      <td>82.758621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>알+고</td>\n",
       "      <td>56</td>\n",
       "      <td>29</td>\n",
       "      <td>51.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>갖+고</td>\n",
       "      <td>55</td>\n",
       "      <td>45</td>\n",
       "      <td>81.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>보+ㄹ</td>\n",
       "      <td>54</td>\n",
       "      <td>29</td>\n",
       "      <td>53.703704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>하+고</td>\n",
       "      <td>52</td>\n",
       "      <td>36</td>\n",
       "      <td>69.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>갖+는</td>\n",
       "      <td>46</td>\n",
       "      <td>40</td>\n",
       "      <td>86.956522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>비롯+하+ㄴ</td>\n",
       "      <td>44</td>\n",
       "      <td>36</td>\n",
       "      <td>81.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>지니+ㄴ</td>\n",
       "      <td>43</td>\n",
       "      <td>39</td>\n",
       "      <td>90.697674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>의미+하+ㄴ다</td>\n",
       "      <td>42</td>\n",
       "      <td>35</td>\n",
       "      <td>83.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>갖+게</td>\n",
       "      <td>41</td>\n",
       "      <td>39</td>\n",
       "      <td>95.121951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>알+게</td>\n",
       "      <td>40</td>\n",
       "      <td>28</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lemma  obj_count  obj_immediate_count  pct_immediate\n",
       "93        하+는        122                   59      48.360656\n",
       "388      가지+고        113                   98      86.725664\n",
       "284      위하+ㄴ         98                   90      91.836735\n",
       "88        하+ㄴ         89                   38      42.696629\n",
       "48        알+ㄹ         84                   49      58.333333\n",
       "386     하+었+다         82                   44      53.658537\n",
       "7        가지+ㄴ         77                   68      88.311688\n",
       "693       하+ㄹ         68                   37      54.411765\n",
       "79       하+ㄴ다         61                   27      44.262295\n",
       "46       지니+고         58                   48      82.758621\n",
       "31        알+고         56                   29      51.785714\n",
       "125       갖+고         55                   45      81.818182\n",
       "158       보+ㄹ         54                   29      53.703704\n",
       "60        하+고         52                   36      69.230769\n",
       "241       갖+는         46                   40      86.956522\n",
       "9      비롯+하+ㄴ         44                   36      81.818182\n",
       "289      지니+ㄴ         43                   39      90.697674\n",
       "1461  의미+하+ㄴ다         42                   35      83.333333\n",
       "305       갖+게         41                   39      95.121951\n",
       "290       알+게         40                   28      70.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved per-verb obj stats to data/project_outputs\\verbs_with_obj_stats.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "verb_obj_counts = defaultdict(int)\n",
    "verb_obj_immediate = defaultdict(int)\n",
    "\n",
    "for sent_idx, tokens in enumerate(sents):\n",
    "    id_to_tok = {t['id']: t for t in tokens}\n",
    "    dependents = defaultdict(list)\n",
    "    for t in tokens:\n",
    "        if t['head'] is not None and t['head'] in id_to_tok:\n",
    "            dependents[t['head']].append(t)\n",
    "    id_to_index = {t['id']: i for i, t in enumerate(tokens)}\n",
    "\n",
    "    for i, tok in enumerate(tokens):\n",
    "        if tok['upos'] == 'VERB':\n",
    "            verb_id = tok['id']\n",
    "            lemma = tok['lemma'] if tok['lemma'] != '_' else tok['form']\n",
    "            deps = dependents.get(verb_id, [])\n",
    "            for dep in deps:\n",
    "                if dep['deprel'] == 'obj':\n",
    "                    verb_obj_counts[lemma] += 1\n",
    "                    obj_idx = id_to_index.get(dep['id'])\n",
    "                    if obj_idx is not None and i - obj_idx == 1:\n",
    "                        verb_obj_immediate[lemma] += 1\n",
    "\n",
    "rows = []\n",
    "for lemma, cnt in verb_obj_counts.items():\n",
    "    rows.append({\n",
    "        'lemma': lemma,\n",
    "        'obj_count': cnt,\n",
    "        'obj_immediate_count': verb_obj_immediate.get(lemma, 0),\n",
    "        'pct_immediate': verb_obj_immediate.get(lemma, 0) / cnt * 100 if cnt>0 else 0\n",
    "    })\n",
    "df_verb_obj = pd.DataFrame(rows).sort_values('obj_count', ascending=False)\n",
    "\n",
    "display(df_verb_obj.head(20))\n",
    "csv_path = os.path.join(OUT_DIR, 'verbs_with_obj_stats.csv')\n",
    "df_verb_obj.to_csv(csv_path, index=False)\n",
    "print('Saved per-verb obj stats to', csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14ce7012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot to data/project_outputs\\object_verb_distance_hist.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(os.path.join(OUT_DIR, 'object_verb_distance_distribution.csv'))\n",
    "\n",
    "mask = (df['verb_minus_obj_index'] >= -10) & (df['verb_minus_obj_index'] <= 10)\n",
    "df_vis = df[mask]\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(df_vis['verb_minus_obj_index'], df_vis['count'])\n",
    "plt.xlabel('verb_index - object_index (positive → object before verb)')\n",
    "plt.ylabel('count')\n",
    "plt.title('Object position relative to verb (distance distribution)')\n",
    "plt.grid(axis='y', linestyle=':', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, 'object_verb_distance_hist.png'))\n",
    "print('Saved plot to', os.path.join(OUT_DIR, 'object_verb_distance_hist.png'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2c3c3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEMMA: 부리+지\n",
      "  obj= 탐욕을 verb= 부리지\n",
      "    뭇 짐승들은 일단 배만 부르면 더 이상의 탐욕을 부리지 않는다 .\n",
      "  obj= 억지를 verb= 부리지\n",
      "    네가 맞으면 억지를 부리지 않아도 될 것이고 , 틀리면 아무리 억지를 써도 결국 잘못된 것이기 때문이다 .\n",
      "\n",
      "LEMMA: 본받+으려\n",
      "  obj= 인간형을 verb= 본받으려\n",
      "    이렇게 되면 이러한 인간형을 다투어 본받으려 할 것이 틀림없다 .\n",
      "\n",
      "LEMMA: 넓히+어\n",
      "  obj= 시각을 verb= 넓혀\n",
      "    이제 우리는 개인의 도덕적 차원을 넘어서 우리 사회구성원 전체의 구조적인 의식면으로 시각을 넓혀 볼 때가 되었다 .\n",
      "\n",
      "LEMMA: 젓+어+가+ㄹ\n",
      "  obj= 노를 verb= 저어갈\n",
      "    이것이 구체적으로 눈 앞에 그려져야 우리 모두가 신바람이 나서 힘껏 노를 저어갈 수 있겠기 때문이다 .\n",
      "\n",
      "LEMMA: 잃+ㄴ\n",
      "  obj= 믿음을 verb= 잃은\n",
      "    믿음을 잃은 것이다 .\n",
      "  obj= 생기를 verb= 잃은\n",
      "    거무스레하고 칙칙한 피부 , 거칠어진 피부 , 생기를 잃은 피부에 효과적이다 .\n",
      "  obj= 기운을 verb= 잃은\n",
      "    길에서 잠시 인터뷰한 십여 명의 라마승들의 말과 인상으로 볼 때 그들은 이미 신앙 종교적 지도자로서 기운을 잃은 것 같았다 .\n",
      "  obj= 품위를 verb= 잃은\n",
      "    그리하여 정권을 유지하는 일 , 돈을 버는 일 , 출세를 하는 일 , 심지어는 자녀의 교육에 이르기까지 품위를 잃은 경쟁을 일삼아 왔다 .\n",
      "  obj= 목표를 verb= 잃은\n",
      "    좀 더 솔직히 말한다면 경쟁을 위한 경쟁 , 목표를 잃은 경쟁을 해온 것이다 .\n",
      "\n",
      "LEMMA: 바꾸+어야\n",
      "  obj= 시각을 verb= 바꿔야\n",
      "    그러기 위해서는 모든 시각을 근시로부터 원시로 바꿔야 한다 .\n",
      "  obj= 설계를 verb= 바꾸어야\n",
      "    무엇보다도 조사연구의 설계를 총체적으로 바꾸어야 한다 .\n",
      "  obj= 당헌을 verb= 바꾸어야\n",
      "    지구당원들이 스스로 입후보자를 선출하는 방향으로 각 정당은 당헌을 바꾸어야 한다 .\n",
      "\n",
      "LEMMA: 묶+어+놓+ㄴ\n",
      "  obj= 신분을 verb= 묶어놓은\n",
      "    권력까지 넘보지 못하게 아예 제도적으로 신분을 최하위에다 묶어놓은 것이다 .\n",
      "\n",
      "LEMMA: 가지+ㄴ\n",
      "  obj= 권력을 verb= 가진\n",
      "    한편 무서운 권력을 가진 사람에게 돈까지 갖게 한다면 어떻게 될까 .\n",
      "  obj= 이름을 verb= 가진\n",
      "    이상한 이름을 가진 아이는 평생 놀림거리가 되고 , 깊은 좌절감에 빠져 , 좋은 기회들마저 포기하게 된다 .\n",
      "  obj= 음기를 verb= 가진\n",
      "    관악산 음기설이란 관악산이 음기를 가진 산이어서 남자들은 기를 빼앗기고 , 여자들은 기를 얻는다는 것이겠다 .\n",
      "  obj= 셋을 verb= 가진\n",
      "    당신은 좋은 아이 셋을 가진 복된 여자다 .\n",
      "  obj= 힘을 verb= 가진\n",
      "    우리는 과학이라면 현대의 기계문명을 연상하리만큼 우리의 일상생활을 보다 편리하고 효과적이게 하는 힘을 가진 것으로 생각한다 .\n",
      "\n",
      "LEMMA: 꿰뚫+어+보+ㄴ\n",
      "  obj= 이치와 verb= 꿰뚫어본\n",
      "    지당한 발상이요 사물의 이치와 본질을 정확히 꿰뚫어본 시각이다 .\n",
      "\n",
      "LEMMA: 비롯+하+ㄴ\n",
      "  obj= 법을 verb= 비롯한\n",
      "    법을 비롯한 각종 물리적 힘만으로는 사람의 마음을 움직일 수 없다 .\n",
      "  obj= 바다표범이나 verb= 비롯한\n",
      "    거기에다 바다표범이나 고래를 비롯한 바다에서 잡히는 온갖 물고기를 먹는다 .\n",
      "  obj= 양고기를 verb= 비롯한\n",
      "    주식은 양고기를 비롯한 육류로서 여름철에는 젖을 굳혀 만든 유제품을 먹는다 .\n",
      "  obj= 셰바르드나제를 verb= 비롯한\n",
      "    5만여명의 시민들이 결집한 의사당광장에는 셰바르드나제를 비롯한 개혁파 지도부가 참석해 뿌치세력의 불법성을 준열히 규탄하면서 민주 , 개혁을 옹호했다 .\n",
      "  obj= 영국이라든가 verb= 비롯한\n",
      "    그러나 혁명은 독일에서는 실패했고 , 물론 헝가리에서는 일시적인 성공을 거두긴 했으나 영국이라든가 프랑스를 비롯한 세계의 여타 국가에서도 구체화되지 못했다 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for lemma, exs in list(examples.items())[:10]:\n",
    "    print('LEMMA:', lemma)\n",
    "    for obj_form, verb_form, sent in exs:\n",
    "        print('  obj=', obj_form, 'verb=', verb_form)\n",
    "        print('   ', sent)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1946b07f-aa17-4b31-81db-c928176b4fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sent_id = M2TA_064-s1\n",
      "# text = 하기야 짐승도 잘 가르치기만 하면 어느 정도는 순치될 수 있다.\n",
      "# translit = .ha.gi.ya .jim.seung.do .jal .ga.reu.chi.gi.man .ha.myeon .eo.neu .jeong.do.neun .sun.chi.doel .su .iss.da.\n",
      "1\t하기야\t하기야\tCCONJ\tmaj\t_\t8\tcc\t_\tTranslit=.ha.gi.ya|LTranslit=.ha.gi.ya\n",
      "2\t짐승도\t짐승+도\tADV\tncn+jxc\t_\t8\tadvcl\t_\tTranslit=.jim.seung.do|LTranslit=.jim.seung+.do\n",
      "3\t잘\t잘\tADV\tmag\t_\t4\tadvmod\t_\tTranslit=.jal|LTranslit=.jal\n",
      "4\t가르치기만\t가르치+기+만\tADV\tpvg+etn+jxc\t_\t5\tadvcl\t_\tTranslit=.ga.reu.chi.gi.man|LTranslit=.ga.reu.chi+.gi+.man\n",
      "5\t하면\t하+면\tSCONJ\tpvg+ecs\t_\t8\tccomp\t_\tTranslit=.ha.myeon|LTranslit=.ha+.myeon\n",
      "6\t어느\t어느\tDET\tmmd\t_\t7\tdet\t_\tTranslit=.eo.neu|LTranslit=.eo.neu\n",
      "7\t정도는\t정도+는\tNOUN\tncn+jxt\t_\t8\tdislocated\t_\tTranslit=.jeong.do.neun|LTranslit=.jeong.do+.neun\n",
      "8\t순치될\t순치+되+ㄹ\tVERB\tncpa+xsv+etm\t_\t0\troot\t_\tTranslit=.sun.chi.doel|LTranslit=.sun.chi+.doe+ㄹ\n",
      "9\t수\t수\tNOUN\tnbn\t_\t8\tobl\t_\tOrigDeprel=aux|Translit=.su|LTranslit=.su\n",
      "10\t있다\t있+다\tADJ\tpaa+ef\t_\t9\tfixed\t_\tSpaceAfter=No|Translit=.iss.da|LTranslit=.iss+.da\n",
      "11\t.\t.\tPUNCT\tsf\t_\t9\tpunct\t_\tTranslit=.|LTranslit=.\n",
      "\n",
      "# sent_id = M2TA_064-s2\n",
      "# text = 사람이 스스로 만물의 영장이라 하고 우쭐대는 까닭이 여기에 있다.\n",
      "# translit = .sa.ram.i .seu.seu.ro .man.mul.yi .yeong.jang.i.ra .ha.go .u.jjul.dae.neun .gga.darg.i .yeo.gi.e .iss.da.\n",
      "1\t사람이\t사람+이\tNOUN\tncn+jcs\t_\t6\tnsubj\t_\tTranslit=.sa.ram.i|LTranslit=.sa.ram+.i\n",
      "2\t스스로\t스스로\tADV\tmag\t_\t5\tadvmod\t_\tTranslit=.seu.seu.ro|LTranslit=.seu.seu.ro\n",
      "3\t만물의\t만물+의\tNOUN\tncn+jcm\t_\t4\tnmod\t_\tTranslit=.man.mul.yi|LTranslit=.man.mul+.yi\n",
      "4\t영장이라\t영장+이+라\tSCONJ\tncn+jp+ecs\t_\t5\tccomp\t_\tTranslit=.yeong.jang.i.ra|LTranslit=.yeong.jang+.i+.ra\n",
      "5\t하고\t하+고\tSCONJ\tpvg+ecs\t_\t6\tccomp\t_\tTranslit=.ha.go|LTranslit=.ha+.go\n",
      "6\t우쭐대는\t우쭐대+는\tVERB\tpvg+etm\t_\t7\tacl\t_\tTranslit=.u.jjul.dae.neun|LTranslit=.u.jjul.dae+.neun\n",
      "7\t까닭이\t까닭+이\tNOUN\tncn+jcs\t_\t9\tnsubj\t_\tTranslit=.gga.darg.i|LTranslit=.gga.darg+.i\n",
      "8\t여기에\t여기+에\tADV\tnpd+jca\t_\t9\tobl\t_\tTranslit=.yeo.gi.e|LTranslit=.yeo.gi+.e\n",
      "9\t있다\t있+다\tADJ\tpaa+ef\t_\t0\troot\t_\tSpaceAfter=No|Translit=.iss.da|LTranslit=.iss+.da\n",
      "10\t.\t.\tPUNCT\tsf\t_\t9\tpunct\t_\tTranslit=.|LTranslit=.\n",
      "\n",
      "# sent_id = M2TA_064-s3\n",
      "# text = 뭇 짐승들은 일단 배만 부르면 더 이상의 탐욕을 부리지 않는다.\n",
      "# translit = .mus .jim.seung.deul.eun .il.dan .bae.man .bu.reu.myeon .deo .i.sang.yi .tam.yog.eul .bu.ri.ji .anh.neun.da.\n",
      "1\t뭇\t뭇\tADJ\tmma\t_\t2\tamod\t_\tTranslit=.mus|LTranslit=.mus\n",
      "2\t짐승들은\t짐승+들+은\tNOUN\tncn+xsn+jxt\t_\t9\tdislocated\t_\tTranslit=.jim.seung.deul.eun|LTranslit=.jim.seung+.deul+.eun\n",
      "3\t일단\t일단\tADV\tmag\t_\t5\tadvmod\t_\tTranslit=.il.dan|LTranslit=.il.dan\n",
      "4\t배만\t배+만\tADV\tncn+jxc\t_\t5\tadvcl\t_\tTranslit=.bae.man|LTranslit=.bae+.man\n",
      "5\t부르면\t부르+면\tSCONJ\tpaa+ecs\t_\t9\txcomp\t_\tTranslit=.bu.reu.myeon|LTranslit=.bu.reu+.myeon\n",
      "6\t더\t더\tADV\tmag\t_\t7\tadvmod\t_\tTranslit=.deo|LTranslit=.deo\n",
      "7\t이상의\t이상+의\tNOUN\tncn+jcm\t_\t8\tnmod\t_\tTranslit=.i.sang.yi|LTranslit=.i.sang+.yi\n",
      "8\t탐욕을\t탐욕+을\tNOUN\tncn+jco\t_\t9\tobj\t_\tTranslit=.tam.yog.eul|LTranslit=.tam.yog+.eul\n",
      "9\t부리지\t부리+지\tVERB\tpvg+ecx\t_\t0\troot\t_\tTranslit=.bu.ri.ji|LTranslit=.bu.ri+.ji\n",
      "10\t않는다\t않\tAUX\tpx+ef\t_\t9\taux\t_\tOrigLemma=않+는다|SpaceAfter=No|Translit=.anh.neun.da|LTranslit=.anh\n",
      "11\t.\t.\tPUNCT\tsf\t_\t10\tpunct\t_\tTranslit=.|LTranslit=.\n",
      "\n",
      "# sent_id = M2TA_064-s4\n",
      "# text = 우거진 쑥밭에서 쑥만 뽑아낸다고 곡식이 저절로 자라지 않는 것과 같다.\n",
      "# translit = .u.geo.jin .ssug.bat.e.seo .ssug.man .bbob.a.naen.da.go .gog.sig.i .jeo.jeol.ro .ja.ra.ji .anh.neun .geos.gwa .gat.da.\n",
      "1\t우거진\t우거지+ㄴ\tVERB\tpvg+etm\t_\t2\tacl\t_\tTranslit=.u.geo.jin|LTranslit=.u.geo.ji+ㄴ\n",
      "2\t쑥밭에서\t쑥밭+에서\tADV\tncn+jca\t_\t4\tadvcl\t_\tTranslit=.ssug.bat.e.seo|LTranslit=.ssug.bat+.e.seo\n",
      "3\t쑥만\t쑥+만\tADV\tncn+jxc\t_\t4\tadvcl\t_\tTranslit=.ssug.man|LTranslit=.ssug+.man\n",
      "4\t뽑아낸다고\t뽑+아+내+ㄴ다고\tSCONJ\tpvg+ecx+px+ecs\t_\t7\tccomp\t_\tTranslit=.bbob.a.naen.da.go|LTranslit=.bbob+.a+.nae+ㄴ.da.go\n",
      "5\t곡식이\t곡식+이\tNOUN\tncn+jcs\t_\t7\tnsubj\t_\tTranslit=.gog.sig.i|LTranslit=.gog.sig+.i\n",
      "6\t저절로\t저절로\tADV\tmag\t_\t7\tadvmod\t_\tTranslit=.jeo.jeol.ro|LTranslit=.jeo.jeol.ro\n",
      "7\t자라지\t자라+지\tVERB\tpvg+ecx\t_\t9\tacl\t_\tTranslit=.ja.ra.ji|LTranslit=.ja.ra+.ji\n",
      "8\t않는\t않\tAUX\tpx+etm\t_\t7\taux\t_\tOrigLemma=않+는|Translit=.anh.neun|LTranslit=.anh\n",
      "9\t것과\t것+과\tCCONJ\tnbn+jct\t_\t0\troot\t_\tTranslit=.geos.gwa|LTranslit=.geos+.gwa\n",
      "10\t같다\t같+다\tADJ\tpaa+ef\t_\t9\tconj\t_\tSpaceAfter=No|Translit=.gat.da|LTranslit=.ga\n"
     ]
    }
   ],
   "source": [
    "print(data[:4000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc952160-39df-4b1f-9527-54240f9da389",
   "metadata": {},
   "source": [
    "As you can see, this is a tabular format. There is a line for each word in the sentence, and the information for that word is given in tab-delimited cells, for example:\n",
    "\n",
    "```2\tAppreciation\tappreciation\tNOUN\tNN\tNumber=Sing\t0\troot\t0:root\tEntity=1)|MSeg=Appreciat-ion```\n",
    "\n",
    "This is the CoNLL-U format, which originated with a shared task at the Conference on Natural Language Learning (CoNLL). \n",
    "\n",
    "There is a Python package, conllu, that is made for reading CoNLL-U data. Once we have read the GUM corpus into a string, we can parse it with conllu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef8f2e94-c22a-4fca-9442-ff4620567801",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = conllu.parse(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f45e066-23f5-4fdf-827e-b65ff1a5adaf",
   "metadata": {},
   "source": [
    "The content of `sentences` is a sequence of TokenList objects. Here is the one for the 10th sentence of the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d2333cc-4aba-4d54-ae98-76a3fc1b57d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenList<भोजपुरी, खातिर, एगो, बड़हन, आन्दोलन, चलवला, के, जरूरत, :, सदानन्द, शाही, विश्व, भोजपुरी, सम्मेलन, के, बलिया, इकाई, अउर, पाती, सांस्कृतिक, मंच, के, एगो, बड़हन, आयोजन, पिछला, अतवारा, का, दिने, बलिया, के, टाउन, हाल, बापू, भवन, में, भइल, ।, metadata={sent_id: \"f1-11\", text: \"भोजपुरी खातिर एगो बड़हन आन्दोलन चलवला के जरूरत : सदानन्द शाही विश्व भोजपुरी सम्मेलन के बलिया इकाई अउर पाती सांस्कृतिक मंच के एगो बड़हन आयोजन पिछला अतवारा का दिने बलिया के टाउन हाल बापू भवन में भइल ।\", translit: \"bhojapurī khātira ego baṛahana āndolana calavalā ke jarūrata : sadānanda śāhī viśva bhojapurī sammelana ke baliyā ikāī aura pātī sāṁskr̥tika maṁca ke ego baṛahana āyojana pichalā atavārā kā dine baliyā ke ṭāuna hāla bāpū bhavana meṁ bhaila .\"}>\n"
     ]
    }
   ],
   "source": [
    "print(sentences[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6836027d-685d-4613-b4b1-89ae92254c4a",
   "metadata": {},
   "source": [
    "We can access the entries on the TokenList through a for-loop, or using an index. Here is the first token of sentence 10. As you can see, it is a Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bba93605-0b32-4f1f-b35f-34d2421814cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'form': 'भोजपुरी',\n",
       " 'lemma': 'भोजपुरी',\n",
       " 'upos': 'PROPN',\n",
       " 'xpos': 'N_NNP',\n",
       " 'feats': {'Case': 'Nom', 'Gender': 'Fem', 'Number': 'Sing', 'Person': '3'},\n",
       " 'head': 5,\n",
       " 'deprel': 'compound',\n",
       " 'deps': None,\n",
       " 'misc': {'Translit': 'bhojapurī', 'LTranslit': 'bhojapurī'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence10 = sentences[10]\n",
    "firstword = sentence10[0]\n",
    "firstword"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f591f3d7-7f12-47eb-b7e0-ae2dadf1b4cc",
   "metadata": {},
   "source": [
    "You can access the entries in that dictionary by their keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52dd8ff8-ca4e-4f16-9231-865453e85431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'भोजपुरी'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstword[\"lemma\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c2bfd8-91dd-4a60-a737-ca819e741975",
   "metadata": {},
   "source": [
    "To better understand this big dictionary, it helps to view it as an attribute-value matrix. Here is the first word of the 10th sentence of the UD_English-GUM corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4fba85e-117a-40bc-b2ba-1f09995b47b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "firstword = {'id': 1,\n",
    "  'form': 'Thus',\n",
    "  'lemma': 'thus',\n",
    "  'upos': 'ADV',\n",
    "  'xpos': 'RB',\n",
    "  'feats': None,\n",
    "  'head': 16,\n",
    "  'deprel': 'advmod',\n",
    "  'deps': None,\n",
    "  'misc': {'SpaceAfter': 'No'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a98da69-4ce7-4805-8bb1-f69609ba10a7",
   "metadata": {},
   "source": [
    "This is the following attribute-value matrix (AVM):\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{ll}\n",
    "\\text{id:} & 1\\\\\n",
    "\\text{form:} & 'Thus'\\\\\n",
    "\\text{lemma:} & 'thus'\\\\\n",
    "\\text{upos:} &  'ADV'\\\\\n",
    "\\text{xpos:} & 'RB'\\\\\n",
    "\\text{feats:} &  None\\\\\n",
    "\\text{head:} & 16\\\\\n",
    "\\text{deprel:}  & advmod\\\\\n",
    "\\text{deps:}  & None\\\\\n",
    "\\text{misc:} & \\left[\\begin{array}{ll}\n",
    "\\text{SpaceAfter:} & 'No'\n",
    "\\end{array}\\right]\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "As you saw above, you can access an entry in this attribute-value matrix through its dictionary key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56e51ee5-055c-4d1c-8176-751bab20de51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thus'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstword[\"lemma\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f84425-20f4-4249-a478-5c258ffc9a6a",
   "metadata": {},
   "source": [
    "One of the values in the AVM is itself an AVM. To access the value that tells you whether there is a space after the word, you need to specify the whole path of keys. `firstword[\"misc\"]` accesses a dictionary, namely `{'SpaceAfter': 'No'}`, which again has keys, in particular `SpaceAfter`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4578946-d0d3-49ef-90e0-50f0f092919f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstword[\"misc\"][\"SpaceAfter\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0425615d-45b3-45fb-aa3f-48af620c03d1",
   "metadata": {},
   "source": [
    "The Universal Dependencies representation of a whole sentence is a list of tokens, that is, a list of dictionaries (=AVMs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2de31cd0-1665-4d18-b683-d3448fe71b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence10 = [{'id': 1,\n",
    "  'form': 'Thus',\n",
    "  'lemma': 'thus',\n",
    "  'upos': 'ADV',\n",
    "  'xpos': 'RB',\n",
    "  'feats': None,\n",
    "  'head': 16,\n",
    "  'deprel': 'advmod',\n",
    "  'deps': None,\n",
    "  'misc': {'SpaceAfter': 'No'}},\n",
    " {'id': 2,\n",
    "  'form': ',',\n",
    "  'lemma': ',',\n",
    "  'upos': 'PUNCT',\n",
    "  'xpos': ',',\n",
    "  'feats': None,\n",
    "  'head': 1,\n",
    "  'deprel': 'punct',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 3,\n",
    "  'form': 'the',\n",
    "  'lemma': 'the',\n",
    "  'upos': 'DET',\n",
    "  'xpos': 'DT',\n",
    "  'feats': {'Definite': 'Def', 'PronType': 'Art'},\n",
    "  'head': 4,\n",
    "  'deprel': 'det',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 4,\n",
    "  'form': 'time',\n",
    "  'lemma': 'time',\n",
    "  'upos': 'NOUN',\n",
    "  'xpos': 'NN',\n",
    "  'feats': {'Number': 'Sing'},\n",
    "  'head': 16,\n",
    "  'deprel': 'nsubj',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 5,\n",
    "  'form': 'it',\n",
    "  'lemma': 'it',\n",
    "  'upos': 'PRON',\n",
    "  'xpos': 'PRP',\n",
    "  'feats': {'Case': 'Nom',\n",
    "   'Gender': 'Neut',\n",
    "   'Number': 'Sing',\n",
    "   'Person': '3',\n",
    "   'PronType': 'Prs'},\n",
    "  'head': 6,\n",
    "  'deprel': 'nsubj',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 6,\n",
    "  'form': 'takes',\n",
    "  'lemma': 'take',\n",
    "  'upos': 'VERB',\n",
    "  'xpos': 'VBZ',\n",
    "  'feats': {'Mood': 'Ind',\n",
    "   'Number': 'Sing',\n",
    "   'Person': '3',\n",
    "   'Tense': 'Pres',\n",
    "   'VerbForm': 'Fin'},\n",
    "  'head': 4,\n",
    "  'deprel': 'acl:relcl',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 7,\n",
    "  'form': 'and',\n",
    "  'lemma': 'and',\n",
    "  'upos': 'CCONJ',\n",
    "  'xpos': 'CC',\n",
    "  'feats': None,\n",
    "  'head': 9,\n",
    "  'deprel': 'cc',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 8,\n",
    "  'form': 'the',\n",
    "  'lemma': 'the',\n",
    "  'upos': 'DET',\n",
    "  'xpos': 'DT',\n",
    "  'feats': {'Definite': 'Def', 'PronType': 'Art'},\n",
    "  'head': 9,\n",
    "  'deprel': 'det',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 9,\n",
    "  'form': 'ways',\n",
    "  'lemma': 'way',\n",
    "  'upos': 'NOUN',\n",
    "  'xpos': 'NNS',\n",
    "  'feats': {'Number': 'Plur'},\n",
    "  'head': 4,\n",
    "  'deprel': 'conj',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 10,\n",
    "  'form': 'of',\n",
    "  'lemma': 'of',\n",
    "  'upos': 'SCONJ',\n",
    "  'xpos': 'IN',\n",
    "  'feats': None,\n",
    "  'head': 12,\n",
    "  'deprel': 'mark',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 11,\n",
    "  'form': 'visually',\n",
    "  'lemma': 'visually',\n",
    "  'upos': 'ADV',\n",
    "  'xpos': 'RB',\n",
    "  'feats': None,\n",
    "  'head': 12,\n",
    "  'deprel': 'advmod',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 12,\n",
    "  'form': 'exploring',\n",
    "  'lemma': 'explore',\n",
    "  'upos': 'VERB',\n",
    "  'xpos': 'VBG',\n",
    "  'feats': {'VerbForm': 'Ger'},\n",
    "  'head': 9,\n",
    "  'deprel': 'acl',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 13,\n",
    "  'form': 'an',\n",
    "  'lemma': 'a',\n",
    "  'upos': 'DET',\n",
    "  'xpos': 'DT',\n",
    "  'feats': {'Definite': 'Ind', 'PronType': 'Art'},\n",
    "  'head': 14,\n",
    "  'deprel': 'det',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 14,\n",
    "  'form': 'artwork',\n",
    "  'lemma': 'artwork',\n",
    "  'upos': 'NOUN',\n",
    "  'xpos': 'NN',\n",
    "  'feats': {'Number': 'Sing'},\n",
    "  'head': 12,\n",
    "  'deprel': 'obj',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 15,\n",
    "  'form': 'can',\n",
    "  'lemma': 'can',\n",
    "  'upos': 'AUX',\n",
    "  'xpos': 'MD',\n",
    "  'feats': {'VerbForm': 'Fin'},\n",
    "  'head': 16,\n",
    "  'deprel': 'aux',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 16,\n",
    "  'form': 'inform',\n",
    "  'lemma': 'inform',\n",
    "  'upos': 'VERB',\n",
    "  'xpos': 'VB',\n",
    "  'feats': {'VerbForm': 'Inf'},\n",
    "  'head': 0,\n",
    "  'deprel': 'root',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 17,\n",
    "  'form': 'about',\n",
    "  'lemma': 'about',\n",
    "  'upos': 'ADP',\n",
    "  'xpos': 'IN',\n",
    "  'feats': None,\n",
    "  'head': 19,\n",
    "  'deprel': 'case',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 18,\n",
    "  'form': 'its',\n",
    "  'lemma': 'its',\n",
    "  'upos': 'PRON',\n",
    "  'xpos': 'PRP$',\n",
    "  'feats': {'Gender': 'Neut',\n",
    "   'Number': 'Sing',\n",
    "   'Person': '3',\n",
    "   'Poss': 'Yes',\n",
    "   'PronType': 'Prs'},\n",
    "  'head': 19,\n",
    "  'deprel': 'nmod:poss',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 19,\n",
    "  'form': 'relevance',\n",
    "  'lemma': 'relevance',\n",
    "  'upos': 'NOUN',\n",
    "  'xpos': 'NN',\n",
    "  'feats': {'Number': 'Sing'},\n",
    "  'head': 16,\n",
    "  'deprel': 'obl',\n",
    "  'deps': None,\n",
    "  'misc': {'SpaceAfter': 'No'}},\n",
    " {'id': 20,\n",
    "  'form': ',',\n",
    "  'lemma': ',',\n",
    "  'upos': 'PUNCT',\n",
    "  'xpos': ',',\n",
    "  'feats': None,\n",
    "  'head': 21,\n",
    "  'deprel': 'punct',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 21,\n",
    "  'form': 'interestingness',\n",
    "  'lemma': 'interestingness',\n",
    "  'upos': 'NOUN',\n",
    "  'xpos': 'NN',\n",
    "  'feats': {'Number': 'Sing'},\n",
    "  'head': 19,\n",
    "  'deprel': 'conj',\n",
    "  'deps': None,\n",
    "  'misc': {'SpaceAfter': 'No'}},\n",
    " {'id': 22,\n",
    "  'form': ',',\n",
    "  'lemma': ',',\n",
    "  'upos': 'PUNCT',\n",
    "  'xpos': ',',\n",
    "  'feats': None,\n",
    "  'head': 27,\n",
    "  'deprel': 'punct',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 23,\n",
    "  'form': 'and',\n",
    "  'lemma': 'and',\n",
    "  'upos': 'CCONJ',\n",
    "  'xpos': 'CC',\n",
    "  'feats': None,\n",
    "  'head': 27,\n",
    "  'deprel': 'cc',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 24,\n",
    "  'form': 'even',\n",
    "  'lemma': 'even',\n",
    "  'upos': 'ADV',\n",
    "  'xpos': 'RB',\n",
    "  'feats': None,\n",
    "  'head': 27,\n",
    "  'deprel': 'advmod',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 25,\n",
    "  'form': 'its',\n",
    "  'lemma': 'its',\n",
    "  'upos': 'PRON',\n",
    "  'xpos': 'PRP$',\n",
    "  'feats': {'Gender': 'Neut',\n",
    "   'Number': 'Sing',\n",
    "   'Person': '3',\n",
    "   'Poss': 'Yes',\n",
    "   'PronType': 'Prs'},\n",
    "  'head': 27,\n",
    "  'deprel': 'nmod:poss',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 26,\n",
    "  'form': 'aesthetic',\n",
    "  'lemma': 'aesthetic',\n",
    "  'upos': 'ADJ',\n",
    "  'xpos': 'JJ',\n",
    "  'feats': {'Degree': 'Pos'},\n",
    "  'head': 27,\n",
    "  'deprel': 'amod',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 27,\n",
    "  'form': 'appeal',\n",
    "  'lemma': 'appeal',\n",
    "  'upos': 'NOUN',\n",
    "  'xpos': 'NN',\n",
    "  'feats': {'Number': 'Sing'},\n",
    "  'head': 19,\n",
    "  'deprel': 'conj',\n",
    "  'deps': None,\n",
    "  'misc': {'SpaceAfter': 'No'}},\n",
    " {'id': 28,\n",
    "  'form': '.',\n",
    "  'lemma': '.',\n",
    "  'upos': 'PUNCT',\n",
    "  'xpos': '.',\n",
    "  'feats': None,\n",
    "  'head': 16,\n",
    "  'deprel': 'punct',\n",
    "  'deps': None,\n",
    "  'misc': None}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d492bac8-0a69-41a0-adb1-2ef59ec67350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Thus ADV 16 advmod\n",
      "2 , PUNCT 1 punct\n",
      "3 the DET 4 det\n",
      "4 time NOUN 16 nsubj\n",
      "5 it PRON 6 nsubj\n",
      "6 takes VERB 4 acl:relcl\n",
      "7 and CCONJ 9 cc\n",
      "8 the DET 9 det\n",
      "9 ways NOUN 4 conj\n",
      "10 of SCONJ 12 mark\n",
      "11 visually ADV 12 advmod\n",
      "12 exploring VERB 9 acl\n",
      "13 an DET 14 det\n",
      "14 artwork NOUN 12 obj\n",
      "15 can AUX 16 aux\n",
      "16 inform VERB 0 root\n",
      "17 about ADP 19 case\n",
      "18 its PRON 19 nmod:poss\n",
      "19 relevance NOUN 16 obl\n",
      "20 , PUNCT 21 punct\n",
      "21 interestingness NOUN 19 conj\n",
      "22 , PUNCT 27 punct\n",
      "23 and CCONJ 27 cc\n",
      "24 even ADV 27 advmod\n",
      "25 its PRON 27 nmod:poss\n",
      "26 aesthetic ADJ 27 amod\n",
      "27 appeal NOUN 19 conj\n",
      "28 . PUNCT 16 punct\n"
     ]
    }
   ],
   "source": [
    "# now we can iterate through the AVMs for this sentence, and \n",
    "# print informati0n for each one\n",
    "for token in sentence10:\n",
    "    print(token[\"id\"], token[\"form\"], token[\"upos\"], \n",
    "          token[\"head\"], token[\"deprel\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfb7c79-8b4e-43e5-a09b-5e2baede941a",
   "metadata": {},
   "source": [
    "Now say we want to determine how often we have subject-verb-object (SVO) versus SOV versus VSO etc. in a Universal Dependencies corpus. To do that, we would like to have an AVM for a word that includes all its dependents. For the verb \"inform\" in the sentence above, we would like the AVM to list that \"time\" (word 4) is the nsubj of \"inform\", and \"relevance\" (word 19) is its obl:\n",
    "\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{ll}\n",
    "\\text{form:} & inform\\\\\n",
    "\\text{id:} & 16\\\\\n",
    "\\text{upos:} & VERB\\\\\n",
    "\\text{dep:} & \\[ \\left[\\begin{array}{ll}\n",
    "\\text{id:} & 4\\\\\n",
    "\\text{deprel:} & nsubj\\end{array}\\right], \n",
    "\\left[\\begin{array}{ll}\n",
    "\\text{id:} & 19\\\\\n",
    "\\text{deprel:} & obl\\end{array}\\right]\\]\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "As a Python data structure, this AVM is rather complex: It is a dictionary, but under the key \"dep\" the value is a list of dictionaries. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8811f49f-7ff4-4b54-9589-3b026a6fe6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inform_avm_with_deps = { \"form\" : \"inform\",\n",
    "                        \"id\" : 16,\n",
    "                        \"upos\" : \"VERB\",\n",
    "                        \"dep\" : [ {\"id\" : 4, \"deprel\" : \"nsubj\"}, \n",
    "                                  {\"id\" : 19, \"deprel\" : \"obl\"}]\n",
    "                       }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b79e50-3d38-4a8f-8e11-76f89e33d83c",
   "metadata": {},
   "source": [
    "Here is how we make a version of sentence 10 that has such an AVM for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "807e8456-009c-4331-a75c-2dc41bf90d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'form': 'Thus',\n",
       "  'id': 1,\n",
       "  'upos': 'ADV',\n",
       "  'dep': [{'id': 2, 'deprel': 'punct'}]},\n",
       " {'form': ',', 'id': 2, 'upos': 'PUNCT', 'dep': []},\n",
       " {'form': 'the', 'id': 3, 'upos': 'DET', 'dep': []},\n",
       " {'form': 'time',\n",
       "  'id': 4,\n",
       "  'upos': 'NOUN',\n",
       "  'dep': [{'id': 3, 'deprel': 'det'},\n",
       "   {'id': 6, 'deprel': 'acl:relcl'},\n",
       "   {'id': 9, 'deprel': 'conj'}]},\n",
       " {'form': 'it', 'id': 5, 'upos': 'PRON', 'dep': []},\n",
       " {'form': 'takes',\n",
       "  'id': 6,\n",
       "  'upos': 'VERB',\n",
       "  'dep': [{'id': 5, 'deprel': 'nsubj'}]},\n",
       " {'form': 'and', 'id': 7, 'upos': 'CCONJ', 'dep': []},\n",
       " {'form': 'the', 'id': 8, 'upos': 'DET', 'dep': []},\n",
       " {'form': 'ways',\n",
       "  'id': 9,\n",
       "  'upos': 'NOUN',\n",
       "  'dep': [{'id': 7, 'deprel': 'cc'},\n",
       "   {'id': 8, 'deprel': 'det'},\n",
       "   {'id': 12, 'deprel': 'acl'}]},\n",
       " {'form': 'of', 'id': 10, 'upos': 'SCONJ', 'dep': []},\n",
       " {'form': 'visually', 'id': 11, 'upos': 'ADV', 'dep': []},\n",
       " {'form': 'exploring',\n",
       "  'id': 12,\n",
       "  'upos': 'VERB',\n",
       "  'dep': [{'id': 10, 'deprel': 'mark'},\n",
       "   {'id': 11, 'deprel': 'advmod'},\n",
       "   {'id': 14, 'deprel': 'obj'}]},\n",
       " {'form': 'an', 'id': 13, 'upos': 'DET', 'dep': []},\n",
       " {'form': 'artwork',\n",
       "  'id': 14,\n",
       "  'upos': 'NOUN',\n",
       "  'dep': [{'id': 13, 'deprel': 'det'}]},\n",
       " {'form': 'can', 'id': 15, 'upos': 'AUX', 'dep': []},\n",
       " {'form': 'inform',\n",
       "  'id': 16,\n",
       "  'upos': 'VERB',\n",
       "  'dep': [{'id': 1, 'deprel': 'advmod'},\n",
       "   {'id': 4, 'deprel': 'nsubj'},\n",
       "   {'id': 15, 'deprel': 'aux'},\n",
       "   {'id': 19, 'deprel': 'obl'},\n",
       "   {'id': 28, 'deprel': 'punct'}]},\n",
       " {'form': 'about', 'id': 17, 'upos': 'ADP', 'dep': []},\n",
       " {'form': 'its', 'id': 18, 'upos': 'PRON', 'dep': []},\n",
       " {'form': 'relevance',\n",
       "  'id': 19,\n",
       "  'upos': 'NOUN',\n",
       "  'dep': [{'id': 17, 'deprel': 'case'},\n",
       "   {'id': 18, 'deprel': 'nmod:poss'},\n",
       "   {'id': 21, 'deprel': 'conj'},\n",
       "   {'id': 27, 'deprel': 'conj'}]},\n",
       " {'form': ',', 'id': 20, 'upos': 'PUNCT', 'dep': []},\n",
       " {'form': 'interestingness',\n",
       "  'id': 21,\n",
       "  'upos': 'NOUN',\n",
       "  'dep': [{'id': 20, 'deprel': 'punct'}]},\n",
       " {'form': ',', 'id': 22, 'upos': 'PUNCT', 'dep': []},\n",
       " {'form': 'and', 'id': 23, 'upos': 'CCONJ', 'dep': []},\n",
       " {'form': 'even', 'id': 24, 'upos': 'ADV', 'dep': []},\n",
       " {'form': 'its', 'id': 25, 'upos': 'PRON', 'dep': []},\n",
       " {'form': 'aesthetic', 'id': 26, 'upos': 'ADJ', 'dep': []},\n",
       " {'form': 'appeal',\n",
       "  'id': 27,\n",
       "  'upos': 'NOUN',\n",
       "  'dep': [{'id': 22, 'deprel': 'punct'},\n",
       "   {'id': 23, 'deprel': 'cc'},\n",
       "   {'id': 24, 'deprel': 'advmod'},\n",
       "   {'id': 25, 'deprel': 'nmod:poss'},\n",
       "   {'id': 26, 'deprel': 'amod'}]},\n",
       " {'form': '.',\n",
       "  'id': 28,\n",
       "  'upos': 'PUNCT',\n",
       "  'dep': [{'id': 16, 'deprel': 'root'}]}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reformat_sentence(sentence):\n",
    "    # first we initialize each AVM to have an empty dependencies list\n",
    "    sentence_reformatted = [ ]\n",
    "    for token in sentence:\n",
    "        sentence_reformatted.append( { \"form\" : token[\"form\"], \n",
    "                                    \"id\" : token[\"id\"],\n",
    "                                    \"upos\" : token[\"upos\"],\n",
    "                                    \"dep\" : [ ]\n",
    "                                  } )\n",
    "\n",
    "    # now we add dependencies\n",
    "    for token in sentence:\n",
    "        # looking up the head of this token. index is that head minus one.\n",
    "        myhead_ix = token[\"head\"] - 1\n",
    "        # print(token[\"form\"], token[\"id\"], token[\"head\"], sentence10_reformat[myhead_ix][\"form\"])\n",
    "        # adding this token to the head's dependencies\n",
    "        sentence_reformatted[ myhead_ix ][\"dep\"].append({ \"id\" : token[\"id\"],\n",
    "                                                       \"deprel\" : token[\"deprel\"]})\n",
    "\n",
    "    return sentence_reformatted\n",
    "    \n",
    "reformat_sentence(sentence10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07568beb-0ea6-45aa-872b-9edcf2122ae1",
   "metadata": {},
   "source": [
    "Based on this data structure, we can determine whether the subject is before the verb: If so, its ID is lower than that of the verb. We can also determine whether the subject is before the object: If so, its ID is lower than that of the the object.\n",
    "\n",
    "We can also see how far away from the verb the subject is, by computing the difference between the IDs of the verb and its subject. In the same way, we can determine how far away from the verb the direct object is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04aad0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 23010 sentences from ko_kaist-ud-train.conllu\n",
      "Top verbs (sample): ['있', '것+이+다', '않', '하', '이러하+ㄴ', '대하+ㄴ', '되+었+다', '되+ㄴ다', '하+ㄹ', '하+는', '하+ㄴ다', '때문+이+다', '것+이+ㅂ니다', '이', '되+ㄹ', '보+ㄹ', '하+었+다', '되+는', '되+ㄴ', '중요+하+ㄴ']\n",
      "Mid verbs (sample): ['딸리+ㄴ', '외몽골+이+라', '내려오+았+다', '후+이+었+다', '조사+하+ㄴ', '맞추+기', '감추+고', '확인+하+ㄴ', '궁금+하+ㄴ', '보+다', '닮+아', '보호+이+라는', '설명+하+어야', '찾+았+다는', '감추+지', '만나+았+다', '이해+하+었+다', '겪+지', '전개+되+ㄹ', '상정+하+ㄹ']\n",
      "No Word2Vec model provided; skipping centroid and neighbor computation.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "CONLLU_PATH = \"ko_kaist-ud-train.conllu\"  #Just a reminder, all files are directly just in the same folder as this code.\n",
    "OUT_DIR = \"data/project_outputs_task2\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def verb_frequencies(sents, verb_upos=(\"VERB\", \"AUX\")):\n",
    "    \n",
    "    counter = Counter()\n",
    "    locations = defaultdict(list)\n",
    "    for si, tokens in enumerate(sents):\n",
    "        for i, tok in enumerate(tokens):\n",
    "            if tok[\"upos\"] in verb_upos:\n",
    "                lemma = tok.get(\"lemma\", \"_\") if tok.get(\"lemma\", \"_\") != \"_\" else tok.get(\"form\", \"\")\n",
    "                norm = normalize_korean_verb(lemma)\n",
    "                counter[norm] += 1\n",
    "                # store extra info so you still have original lemma/form if needed\n",
    "                locations[norm].append((si, i, lemma, tok.get(\"form\", \"\")))\n",
    "    return counter, locations\n",
    "\n",
    "def select_verbs_by_quantiles(counter, top_pct=0.20, next_pct=0.20, top_k_each=20):\n",
    "    \n",
    "    total_verbs = sum(counter.values())\n",
    "    #sorting verbs by frequency descending\n",
    "    items = counter.most_common()\n",
    "    # just finding top x%\n",
    "    types = [v for v, _ in items]\n",
    "    n_types = len(types)\n",
    "    top_n_types = max(1, int(n_types * top_pct))\n",
    "    next_n_types = max(1, int(n_types * next_pct))\n",
    "    top_type_set = set(types[:top_n_types])\n",
    "    next_type_set = set(types[top_n_types: top_n_types + next_n_types])\n",
    "    top_candidates = [v for v, _ in items if v in top_type_set]\n",
    "    next_candidates = [v for v, _ in items if v in next_type_set]\n",
    "    # pick the top_k_each most frequent within each\n",
    "    return top_candidates[:top_k_each], next_candidates[:top_k_each]\n",
    "\n",
    "\n",
    "\n",
    "def extract_verb_sets(sents, verb_locations, verbs, \n",
    "                      subj_deprels=(\"nsubj\", \"nsubj:pass\", \"csubj\"), \n",
    "                      obj_deprels_prefix=(\"obj\",), \n",
    "                      modifier_deprels_prefixes=(\"advmod\", \"amod\", \"nmod\", \"obl\", \"advcl\", \"compound\")):\n",
    "    \n",
    "    results = {}\n",
    "    for verb in verbs:\n",
    "        subj_c = Counter()\n",
    "        obj_c = Counter()\n",
    "        mod_c = Counter()\n",
    "        before_c = Counter()\n",
    "        after_c = Counter()\n",
    "        occ = 0\n",
    "        locs = verb_locations.get(verb, [])\n",
    "        for si, vi in locs:\n",
    "            sent = sents[si]\n",
    "            if vi < 0 or vi >= len(sent):\n",
    "                continue\n",
    "            verb_token = sent[vi]\n",
    "            occ += 1\n",
    "            #build id->token and dependents map\n",
    "            id_to_tok = {t['id']: t for t in sent}\n",
    "            dependents = defaultdict(list)\n",
    "            for t in sent:\n",
    "                h = t['head']\n",
    "                if h is not None and h in id_to_tok:\n",
    "                    dependents[h].append(t)\n",
    "            #dependents of this verb\n",
    "            v_deps = dependents.get(verb_token['id'], [])\n",
    "            for dep in v_deps:\n",
    "                deprel = dep['deprel']\n",
    "                #subject tests\n",
    "                if deprel in subj_deprels or deprel.startswith(\"nsubj\"):\n",
    "                    head_form = dep['lemma'] if dep['lemma'] != \"_\" else dep['form']\n",
    "                    subj_c[head_form] += 1\n",
    "                # object tests\n",
    "                if any(deprel == p or deprel.startswith(p) for p in obj_deprels_prefix):\n",
    "                    head_form = dep['lemma'] if dep['lemma'] != \"_\" else dep['form']\n",
    "                    obj_c[head_form] += 1\n",
    "                # modifiers\n",
    "                if any(deprel == p or deprel.startswith(p) for p in modifier_deprels_prefixes):\n",
    "                    head_form = dep['lemma'] if dep['lemma'] != \"_\" else dep['form']\n",
    "                    mod_c[head_form] += 1\n",
    "            #surface neighbors\n",
    "            if vi - 1 >= 0:\n",
    "                before_c[sent[vi-1]['form']] += 1\n",
    "            if vi + 1 < len(sent):\n",
    "                after_c[sent[vi+1]['form']] += 1\n",
    "        results[verb] = {\n",
    "            \"subject\": subj_c,\n",
    "            \"object\": obj_c,\n",
    "            \"modifier\": mod_c,\n",
    "            \"before\": before_c,\n",
    "            \"after\": after_c,\n",
    "            \"occurrences\": occ\n",
    "        }\n",
    "    return results\n",
    "\n",
    "\n",
    "def load_word2vec_model(local_path=None, binary=None, gensim_name=None):\n",
    "    \"\"\"\n",
    "    Honestly the word2vec thing was acting really weirdly but I think I figured it out.\n",
    "    \"\"\"\n",
    "    from gensim.models import KeyedVectors\n",
    "    if local_path:\n",
    "        # try automatic format detection by file extension\n",
    "        if local_path.endswith(\".kv\") or local_path.endswith(\".kv.gz\") or local_path.endswith(\".model\"):\n",
    "            return KeyedVectors.load(local_path, mmap='r')\n",
    "        else:\n",
    "            # assume word2vec text or binary format\n",
    "            return KeyedVectors.load_word2vec_format(local_path, binary=binary if binary is not None else False, unicode_errors='ignore')\n",
    "    else:\n",
    "        if gensim_name:\n",
    "            import gensim.downloader as api\n",
    "            return api.load(gensim_name)\n",
    "        raise ValueError(\"No model path or gensim_name provided. Please provide a local path to your Word2Vec/KeyedVectors model.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def centroid_of_words(model, words, id_to_wordform=None):\n",
    "    \"\"\"\n",
    "    Given a gensim model and an iterable of words (forms/lemmas), return centroid (numpy array)\n",
    "    Skips OOV words. Returns (centroid_vector, n_in_vocab, missed_list)\n",
    "    \"\"\"\n",
    "    vecs = []\n",
    "    missed = []\n",
    "    for w in words:\n",
    "        if w is None:\n",
    "            continue\n",
    "        # optionally transform or normalize tokens for Korean (lowercasing not always desired)\n",
    "        key = w\n",
    "        if key in model:\n",
    "            vecs.append(model[key])\n",
    "        else:\n",
    "            missed.append(w)\n",
    "    if not vecs:\n",
    "        return None, 0, missed\n",
    "    arr = np.vstack(vecs)\n",
    "    return np.mean(arr, axis=0), arr.shape[0], missed\n",
    "\n",
    "\n",
    "\n",
    "def topk_neighbors_from_centroid(model, centroid_vec, k=10):\n",
    "    \"\"\"\n",
    "    Return top-k nearest neighbor (word, similarity) pairs using model.similar_by_vector\n",
    "    \"\"\"\n",
    "    if centroid_vec is None:\n",
    "        return []\n",
    "    return model.similar_by_vector(centroid_vec, topn=k)\n",
    "\n",
    "\n",
    "def build_task2_analysis(conllu_path=CONLLU_PATH, model_local_path=None, model_name=None, k_neighbors=10, top_k_each=5):\n",
    "    \"\"\"\n",
    "    Main orchestration function\n",
    "     1:parses conllu\n",
    "     2:computes verb frequencies\n",
    "     3:selects verbs\n",
    "     4:extracts subject/object/modifier/before/after sets\n",
    "     5:computes centroids + nearest neighbors\n",
    "    6:Returns a dict with everything and writes outputs to OUT_DIR.\n",
    "    \"\"\"\n",
    "    sents = parse_conllu(conllu_path)\n",
    "    print(f\"Loaded {len(sents)} sentences from {conllu_path}\")\n",
    "    verb_counter, verb_locations = verb_frequencies(sents)\n",
    "    # save verb frequency df\n",
    "    vf_df = pd.DataFrame(verb_counter.most_common(), columns=[\"verb_lemma\", \"freq\"])\n",
    "    vf_df.to_csv(os.path.join(OUT_DIR, \"verb_frequencies.csv\"), index=False)\n",
    "    #select verbs\n",
    "    top_verbs, mid_verbs = select_verbs_by_quantiles(verb_counter, top_pct=0.20, next_pct=0.20, top_k_each=top_k_each)\n",
    "    print(\"Top verbs (sample):\", top_verbs)\n",
    "    print(\"Mid verbs (sample):\", mid_verbs)\n",
    "    # extract sets\n",
    "    verbs_to_analyze = list(top_verbs) + list(mid_verbs)\n",
    "    sets = extract_verb_sets(sents, verb_locations, verbs_to_analyze)\n",
    "    # prepare simple summaries and save\n",
    "    summary = {}\n",
    "    for v in verbs_to_analyze:\n",
    "        entry = sets[v]\n",
    "        # convert counters to most common lists\n",
    "        entry_summary = {\n",
    "            \"occurrences\": entry[\"occurrences\"],\n",
    "            \"top_subjects\": entry[\"subject\"].most_common(30),\n",
    "            \"top_objects\": entry[\"object\"].most_common(30),\n",
    "            \"top_modifiers\": entry[\"modifier\"].most_common(30),\n",
    "            \"top_before\": entry[\"before\"].most_common(30),\n",
    "            \"top_after\": entry[\"after\"].most_common(30)\n",
    "        }\n",
    "        summary[v] = entry_summary\n",
    "\n",
    "    # save preliminary JSON\n",
    "    with open(os.path.join(OUT_DIR, \"verb_sets_summary.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # If model requested, load and compute centroids + neighbors\n",
    "    model = None\n",
    "    neighbors_summary = {}\n",
    "    if model_local_path or model_name:\n",
    "        print(\"Loading model...\")\n",
    "        model = load_word2vec_model(local_path=model_local_path, gensim_name=model_name)\n",
    "        for v in verbs_to_analyze:\n",
    "            neighbors_summary[v] = {}\n",
    "            for set_name in (\"top_subjects\", \"top_objects\", \"top_modifiers\", \"top_before\", \"top_after\"):\n",
    "                # take the top N words from the set (we'll use top 100 or fewer)\n",
    "                words = [w for w, cnt in summary[v][set_name][:200]]\n",
    "                centroid, n_in_vocab, missed = centroid_of_words(model, words)\n",
    "                knn = topk_neighbors_from_centroid(model, centroid, k=k_neighbors) if centroid is not None else []\n",
    "                neighbors_summary[v][set_name] = {\n",
    "                    \"centroid_n_in_vocab\": int(n_in_vocab),\n",
    "                    \"missed_count\": len(missed),\n",
    "                    \"missed_examples\": missed[:30],\n",
    "                    \"knn\": [(w, float(sim)) for w, sim in knn]\n",
    "                }\n",
    "        # save neighbors summary\n",
    "        with open(os.path.join(OUT_DIR, \"verb_neighbors_summary.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(neighbors_summary, f, ensure_ascii=False, indent=2)\n",
    "    else:\n",
    "        print(\"No Word2Vec model provided; skipping centroid and neighbor computation.\")\n",
    "\n",
    "    # Return high-level outputs\n",
    "    return {\n",
    "        \"sentences\": len(sents),\n",
    "        \"verb_frequencies_df\": vf_df,\n",
    "        \"top_verbs\": top_verbs,\n",
    "        \"mid_verbs\": mid_verbs,\n",
    "        \"sets\": sets,\n",
    "        \"summary\": summary,\n",
    "        \"neighbors_summary\": neighbors_summary if model is not None else None\n",
    "    }\n",
    "\n",
    "\n",
    "res = build_task2_analysis(\n",
    "    conllu_path=\"ko_kaist-ud-train.conllu\",\n",
    "    model_local_path=None,\n",
    "    model_name=None,  \n",
    "    k_neighbors=20,\n",
    "    top_k_each=20\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
